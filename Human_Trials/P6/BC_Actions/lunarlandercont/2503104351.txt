Policy train: iteration: 500, policy_loss: 0.458828
Policy train: iteration: 1000, policy_loss: 0.354556
Policy train: iteration: 1500, policy_loss: 0.349396
Policy train: iteration: 2000, policy_loss: 0.392211
Policy train: iteration: 2500, policy_loss: 0.323140
Policy train: iteration: 3000, policy_loss: 0.332254
Policy train: iteration: 3500, policy_loss: 0.285477
Policy train: iteration: 4000, policy_loss: 0.319106
Policy train: iteration: 4500, policy_loss: 0.299877
Policy train: iteration: 5000, policy_loss: 0.267715
Policy train: iteration: 5500, policy_loss: 0.249755
Policy train: iteration: 6000, policy_loss: 0.253635
Policy train: iteration: 6500, policy_loss: 0.274261
Policy train: iteration: 7000, policy_loss: 0.264014
Policy train: iteration: 7500, policy_loss: 0.256395
Policy train: iteration: 8000, policy_loss: 0.252364
Policy train: iteration: 8500, policy_loss: 0.262059
Policy train: iteration: 9000, policy_loss: 0.242550

Background Trial: 1, reward: -10.407039267902846
Background Trial: 2, reward: -177.40853009102057
Background Trial: 3, reward: -35.95844294909229
Background Trial: 4, reward: -20.71844010886275
Background Trial: 5, reward: -240.1279215231477
Background Trial: 6, reward: -469.7824549910258
Background Trial: 7, reward: -50.76730750096796
Background Trial: 8, reward: -228.9750353673435
Background Trial: 9, reward: -41.405491345608255
Iteration: 1, average_reward: -141.72785146055242

Policy train: iteration: 500, policy_loss: 0.243199
Policy train: iteration: 1000, policy_loss: 0.205493
Policy train: iteration: 1500, policy_loss: 0.271572
Policy train: iteration: 2000, policy_loss: 0.227844
Policy train: iteration: 2500, policy_loss: 0.244138
Policy train: iteration: 3000, policy_loss: 0.220939
Policy train: iteration: 3500, policy_loss: 0.226755
Policy train: iteration: 4000, policy_loss: 0.224863
Policy train: iteration: 4500, policy_loss: 0.206993
Policy train: iteration: 5000, policy_loss: 0.224470
Policy train: iteration: 5500, policy_loss: 0.213211
Policy train: iteration: 6000, policy_loss: 0.203887
Policy train: iteration: 6500, policy_loss: 0.184847
Policy train: iteration: 7000, policy_loss: 0.230210
Policy train: iteration: 7500, policy_loss: 0.218613
Policy train: iteration: 8000, policy_loss: 0.184941
Policy train: iteration: 8500, policy_loss: 0.181205
Policy train: iteration: 9000, policy_loss: 0.180846

Background Trial: 1, reward: -64.88404584277181
Background Trial: 2, reward: -45.13903466703907
Background Trial: 3, reward: -5.384790732578665
Background Trial: 4, reward: -64.98850489931256
Background Trial: 5, reward: -26.387475953625753
Background Trial: 6, reward: -21.757537203349344
Background Trial: 7, reward: -408.03429884327363
Background Trial: 8, reward: 26.54508662565128
Background Trial: 9, reward: -28.60544613847057
Iteration: 2, average_reward: -70.95956085053002

Policy train: iteration: 500, policy_loss: 0.217169
Policy train: iteration: 1000, policy_loss: 0.180768
Policy train: iteration: 1500, policy_loss: 0.190064
Policy train: iteration: 2000, policy_loss: 0.222554
Policy train: iteration: 2500, policy_loss: 0.189496
Policy train: iteration: 3000, policy_loss: 0.177464
Policy train: iteration: 3500, policy_loss: 0.195524
Policy train: iteration: 4000, policy_loss: 0.248577
Policy train: iteration: 4500, policy_loss: 0.187499
Policy train: iteration: 5000, policy_loss: 0.221111
Policy train: iteration: 5500, policy_loss: 0.178128
Policy train: iteration: 6000, policy_loss: 0.174510
Policy train: iteration: 6500, policy_loss: 0.164078
Policy train: iteration: 7000, policy_loss: 0.214644
Policy train: iteration: 7500, policy_loss: 0.196189
Policy train: iteration: 8000, policy_loss: 0.178662
Policy train: iteration: 8500, policy_loss: 0.179358
Policy train: iteration: 9000, policy_loss: 0.189194

Background Trial: 1, reward: -436.7755457313149
Background Trial: 2, reward: -100.49960126222288
Background Trial: 3, reward: -310.3619682902855
Background Trial: 4, reward: -498.8964862575884
Background Trial: 5, reward: -283.4464081953663
Background Trial: 6, reward: -588.2880460251133
Background Trial: 7, reward: 8.44444606041047
Background Trial: 8, reward: -5.325373835529689
Background Trial: 9, reward: -248.42888007506318
Iteration: 3, average_reward: -273.7308737346749

Policy train: iteration: 500, policy_loss: 0.186700
Policy train: iteration: 1000, policy_loss: 0.156147
Policy train: iteration: 1500, policy_loss: 0.167303
Policy train: iteration: 2000, policy_loss: 0.205164
Policy train: iteration: 2500, policy_loss: 0.191390
Policy train: iteration: 3000, policy_loss: 0.178740
Policy train: iteration: 3500, policy_loss: 0.223576
Policy train: iteration: 4000, policy_loss: 0.182882
Policy train: iteration: 4500, policy_loss: 0.164245
Policy train: iteration: 5000, policy_loss: 0.191084
Policy train: iteration: 5500, policy_loss: 0.185700
Policy train: iteration: 6000, policy_loss: 0.184580
Policy train: iteration: 6500, policy_loss: 0.166499
Policy train: iteration: 7000, policy_loss: 0.204182
Policy train: iteration: 7500, policy_loss: 0.180727
Policy train: iteration: 8000, policy_loss: 0.218242
Policy train: iteration: 8500, policy_loss: 0.156446
Policy train: iteration: 9000, policy_loss: 0.145732

Background Trial: 1, reward: -376.7687721602692
Background Trial: 2, reward: -129.3752637236688
Background Trial: 3, reward: -383.97722604555696
Background Trial: 4, reward: -257.22557469425215
Background Trial: 5, reward: -338.21532108692827
Background Trial: 6, reward: -357.4970021869559
Background Trial: 7, reward: -22.581617338578354
Background Trial: 8, reward: -22.22683180258595
Background Trial: 9, reward: 217.7086839048469
Iteration: 4, average_reward: -185.57321390377209

Policy train: iteration: 500, policy_loss: 0.171907
Policy train: iteration: 1000, policy_loss: 0.152245
Policy train: iteration: 1500, policy_loss: 0.145046
Policy train: iteration: 2000, policy_loss: 0.148362
Policy train: iteration: 2500, policy_loss: 0.148851
Policy train: iteration: 3000, policy_loss: 0.192361
Policy train: iteration: 3500, policy_loss: 0.124900
Policy train: iteration: 4000, policy_loss: 0.169897
Policy train: iteration: 4500, policy_loss: 0.151815
Policy train: iteration: 5000, policy_loss: 0.161815
Policy train: iteration: 5500, policy_loss: 0.202344
Policy train: iteration: 6000, policy_loss: 0.147615
Policy train: iteration: 6500, policy_loss: 0.164902
Policy train: iteration: 7000, policy_loss: 0.156579
Policy train: iteration: 7500, policy_loss: 0.154419
Policy train: iteration: 8000, policy_loss: 0.176297
Policy train: iteration: 8500, policy_loss: 0.149574
Policy train: iteration: 9000, policy_loss: 0.134509

Background Trial: 1, reward: -77.1659573618972
Background Trial: 2, reward: -170.32253869097713
Background Trial: 3, reward: -107.6971549310241
Background Trial: 4, reward: -77.21044559849707
Background Trial: 5, reward: -482.1251383691977
Background Trial: 6, reward: -254.46117101819695
Background Trial: 7, reward: -170.516122431734
Background Trial: 8, reward: -145.025944095846
Background Trial: 9, reward: -258.8374919780003
Iteration: 5, average_reward: -193.7068849417078

Policy train: iteration: 500, policy_loss: 0.134199
Policy train: iteration: 1000, policy_loss: 0.132858
Policy train: iteration: 1500, policy_loss: 0.154849
Policy train: iteration: 2000, policy_loss: 0.147257
Policy train: iteration: 2500, policy_loss: 0.146076
Policy train: iteration: 3000, policy_loss: 0.146476
Policy train: iteration: 3500, policy_loss: 0.172566
Policy train: iteration: 4000, policy_loss: 0.104957
Policy train: iteration: 4500, policy_loss: 0.149607
Policy train: iteration: 5000, policy_loss: 0.170454
Policy train: iteration: 5500, policy_loss: 0.158014
Policy train: iteration: 6000, policy_loss: 0.168470
Policy train: iteration: 6500, policy_loss: 0.148402
Policy train: iteration: 7000, policy_loss: 0.104123
Policy train: iteration: 7500, policy_loss: 0.162090
Policy train: iteration: 8000, policy_loss: 0.116806
Policy train: iteration: 8500, policy_loss: 0.143792
Policy train: iteration: 9000, policy_loss: 0.147093

Background Trial: 1, reward: -279.1927551715505
Background Trial: 2, reward: -230.39018757644965
Background Trial: 3, reward: -287.9187405024296
Background Trial: 4, reward: -285.7385698541275
Background Trial: 5, reward: -117.8498726868976
Background Trial: 6, reward: -385.9329450526451
Background Trial: 7, reward: -71.60200460078931
Background Trial: 8, reward: -83.60352366967805
Background Trial: 9, reward: -157.9449198673055
Iteration: 6, average_reward: -211.13039099798587

Policy train: iteration: 500, policy_loss: 0.118693
Policy train: iteration: 1000, policy_loss: 0.112911
Policy train: iteration: 1500, policy_loss: 0.145581
Policy train: iteration: 2000, policy_loss: 0.140763
Policy train: iteration: 2500, policy_loss: 0.112918
Policy train: iteration: 3000, policy_loss: 0.159856
Policy train: iteration: 3500, policy_loss: 0.143943
Policy train: iteration: 4000, policy_loss: 0.101571
Policy train: iteration: 4500, policy_loss: 0.156117
Policy train: iteration: 5000, policy_loss: 0.147685
Policy train: iteration: 5500, policy_loss: 0.105512
Policy train: iteration: 6000, policy_loss: 0.129451
Policy train: iteration: 6500, policy_loss: 0.132251
Policy train: iteration: 7000, policy_loss: 0.162586
Policy train: iteration: 7500, policy_loss: 0.129349
Policy train: iteration: 8000, policy_loss: 0.121905
Policy train: iteration: 8500, policy_loss: 0.225592
Policy train: iteration: 9000, policy_loss: 0.153463

Background Trial: 1, reward: -65.44969051670446
Background Trial: 2, reward: -494.93026665862186
Background Trial: 3, reward: -579.3713646275469
Background Trial: 4, reward: -386.0219138639306
Background Trial: 5, reward: -735.3074372194412
Background Trial: 6, reward: -303.91914509664196
Background Trial: 7, reward: 11.789118571869878
Background Trial: 8, reward: -314.83000439979
Background Trial: 9, reward: 26.108905250935564
Iteration: 7, average_reward: -315.77019983998576

Policy train: iteration: 500, policy_loss: 0.109330
Policy train: iteration: 1000, policy_loss: 0.143033
Policy train: iteration: 1500, policy_loss: 0.129363
Policy train: iteration: 2000, policy_loss: 0.184940
Policy train: iteration: 2500, policy_loss: 0.112013
Policy train: iteration: 3000, policy_loss: 0.122076
Policy train: iteration: 3500, policy_loss: 0.121315
Policy train: iteration: 4000, policy_loss: 0.124236
Policy train: iteration: 4500, policy_loss: 0.129613
Policy train: iteration: 5000, policy_loss: 0.133706
Policy train: iteration: 5500, policy_loss: 0.149311
Policy train: iteration: 6000, policy_loss: 0.139853
Policy train: iteration: 6500, policy_loss: 0.125725
Policy train: iteration: 7000, policy_loss: 0.132734
Policy train: iteration: 7500, policy_loss: 0.183732
Policy train: iteration: 8000, policy_loss: 0.123530
Policy train: iteration: 8500, policy_loss: 0.125734
Policy train: iteration: 9000, policy_loss: 0.167055

Background Trial: 1, reward: -1010.1676078511853
Background Trial: 2, reward: -97.8558011623634
Background Trial: 3, reward: -35.42555344877111
Background Trial: 4, reward: -320.47877574859825
Background Trial: 5, reward: -551.7785103120037
Background Trial: 6, reward: -14.830794442459379
Background Trial: 7, reward: -40.85579229305458
Background Trial: 8, reward: -73.21473937486417
Background Trial: 9, reward: -93.22912107985968
Iteration: 8, average_reward: -248.64852174590663

Policy train: iteration: 500, policy_loss: 0.110995
Policy train: iteration: 1000, policy_loss: 0.107584
Policy train: iteration: 1500, policy_loss: 0.146987
Policy train: iteration: 2000, policy_loss: 0.092416
Policy train: iteration: 2500, policy_loss: 0.112763
Policy train: iteration: 3000, policy_loss: 0.143902
Policy train: iteration: 3500, policy_loss: 0.132880
Policy train: iteration: 4000, policy_loss: 0.166809
Policy train: iteration: 4500, policy_loss: 0.148941
Policy train: iteration: 5000, policy_loss: 0.142961
Policy train: iteration: 5500, policy_loss: 0.125136
Policy train: iteration: 6000, policy_loss: 0.120663
Policy train: iteration: 6500, policy_loss: 0.144345
Policy train: iteration: 7000, policy_loss: 0.120696
Policy train: iteration: 7500, policy_loss: 0.144870
Policy train: iteration: 8000, policy_loss: 0.131346
Policy train: iteration: 8500, policy_loss: 0.135833
Policy train: iteration: 9000, policy_loss: 0.127865

Background Trial: 1, reward: -297.5202750427392
Background Trial: 2, reward: -86.47062905899506
Background Trial: 3, reward: -262.47320764634173
Background Trial: 4, reward: -69.39057383815108
Background Trial: 5, reward: -835.9025012026304
Background Trial: 6, reward: -64.86913456724494
Background Trial: 7, reward: -300.5417490399251
Background Trial: 8, reward: -369.30477535941924
Background Trial: 9, reward: -95.40078477426488
Iteration: 9, average_reward: -264.6526256144124

Policy train: iteration: 500, policy_loss: 0.100255
Policy train: iteration: 1000, policy_loss: 0.140116
Policy train: iteration: 1500, policy_loss: 0.122146
Policy train: iteration: 2000, policy_loss: 0.141581
Policy train: iteration: 2500, policy_loss: 0.129152
Policy train: iteration: 3000, policy_loss: 0.114626
Policy train: iteration: 3500, policy_loss: 0.114287
Policy train: iteration: 4000, policy_loss: 0.117807
Policy train: iteration: 4500, policy_loss: 0.141135
Policy train: iteration: 5000, policy_loss: 0.124281
Policy train: iteration: 5500, policy_loss: 0.129055
Policy train: iteration: 6000, policy_loss: 0.128483
Policy train: iteration: 6500, policy_loss: 0.132971
Policy train: iteration: 7000, policy_loss: 0.135900
Policy train: iteration: 7500, policy_loss: 0.120092
Policy train: iteration: 8000, policy_loss: 0.113149
Policy train: iteration: 8500, policy_loss: 0.150309
Policy train: iteration: 9000, policy_loss: 0.098753

Background Trial: 1, reward: -349.0342914869657
Background Trial: 2, reward: -363.70980879631713
Background Trial: 3, reward: -97.90989928696801
Background Trial: 4, reward: -25.149720268588695
Background Trial: 5, reward: -567.7806414873595
Background Trial: 6, reward: -429.21978403360174
Background Trial: 7, reward: -49.364296683074826
Background Trial: 8, reward: -110.57823411965654
Background Trial: 9, reward: -99.83917724184352
Iteration: 10, average_reward: -232.50953926715283

Policy train: iteration: 500, policy_loss: 0.119259
Policy train: iteration: 1000, policy_loss: 0.153105
Policy train: iteration: 1500, policy_loss: 0.134519
Policy train: iteration: 2000, policy_loss: 0.100015
Policy train: iteration: 2500, policy_loss: 0.108893
Policy train: iteration: 3000, policy_loss: 0.170476
Policy train: iteration: 3500, policy_loss: 0.139769
Policy train: iteration: 4000, policy_loss: 0.131730
Policy train: iteration: 4500, policy_loss: 0.105752
Policy train: iteration: 5000, policy_loss: 0.145675
Policy train: iteration: 5500, policy_loss: 0.110514
Policy train: iteration: 6000, policy_loss: 0.127155
Policy train: iteration: 6500, policy_loss: 0.125168
Policy train: iteration: 7000, policy_loss: 0.139794
Policy train: iteration: 7500, policy_loss: 0.140486
Policy train: iteration: 8000, policy_loss: 0.156167
Policy train: iteration: 8500, policy_loss: 0.126879
Policy train: iteration: 9000, policy_loss: 0.112346

Background Trial: 1, reward: -216.2737977211873
Background Trial: 2, reward: -74.43360945161606
Background Trial: 3, reward: -99.54966452122879
Background Trial: 4, reward: -218.47227323138708
Background Trial: 5, reward: -1017.2305849286727
Background Trial: 6, reward: -257.75388642513525
Background Trial: 7, reward: -50.17222342718319
Background Trial: 8, reward: -87.32622948324263
Background Trial: 9, reward: -70.72842562449895
Iteration: 11, average_reward: -232.4378549793502

Policy train: iteration: 500, policy_loss: 0.099441
Policy train: iteration: 1000, policy_loss: 0.134582
Policy train: iteration: 1500, policy_loss: 0.123190
Policy train: iteration: 2000, policy_loss: 0.134695
Policy train: iteration: 2500, policy_loss: 0.167616
Policy train: iteration: 3000, policy_loss: 0.096763
Policy train: iteration: 3500, policy_loss: 0.108828
Policy train: iteration: 4000, policy_loss: 0.174470
Policy train: iteration: 4500, policy_loss: 0.120713
Policy train: iteration: 5000, policy_loss: 0.111433
Policy train: iteration: 5500, policy_loss: 0.137253
Policy train: iteration: 6000, policy_loss: 0.096231
Policy train: iteration: 6500, policy_loss: 0.163024
Policy train: iteration: 7000, policy_loss: 0.125593
Policy train: iteration: 7500, policy_loss: 0.142175
Policy train: iteration: 8000, policy_loss: 0.103588
Policy train: iteration: 8500, policy_loss: 0.126356
Policy train: iteration: 9000, policy_loss: 0.108359

Background Trial: 1, reward: -305.90318231468854
Background Trial: 2, reward: -393.2457611555921
Background Trial: 3, reward: -24.224936558895934
Background Trial: 4, reward: -371.64160292428954
Background Trial: 5, reward: -288.7754577654579
Background Trial: 6, reward: -355.8644334872823
Background Trial: 7, reward: -574.3627844276853
Background Trial: 8, reward: -295.85421896975333
Background Trial: 9, reward: -434.3986573038396
Iteration: 12, average_reward: -338.25233721194274

Policy train: iteration: 500, policy_loss: 0.107309
Policy train: iteration: 1000, policy_loss: 0.113871
Policy train: iteration: 1500, policy_loss: 0.086010
Policy train: iteration: 2000, policy_loss: 0.135392
Policy train: iteration: 2500, policy_loss: 0.134286
Policy train: iteration: 3000, policy_loss: 0.122439
Policy train: iteration: 3500, policy_loss: 0.097890
Policy train: iteration: 4000, policy_loss: 0.133228
Policy train: iteration: 4500, policy_loss: 0.107816
Policy train: iteration: 5000, policy_loss: 0.112513
Policy train: iteration: 5500, policy_loss: 0.103293
Policy train: iteration: 6000, policy_loss: 0.124104
Policy train: iteration: 6500, policy_loss: 0.133565
Policy train: iteration: 7000, policy_loss: 0.118686
Policy train: iteration: 7500, policy_loss: 0.128785
Policy train: iteration: 8000, policy_loss: 0.120763
Policy train: iteration: 8500, policy_loss: 0.134320
Policy train: iteration: 9000, policy_loss: 0.096273

Background Trial: 1, reward: -327.1777203790065
Background Trial: 2, reward: -277.67402243999686
Background Trial: 3, reward: -71.90752233391072
Background Trial: 4, reward: -386.0755241379474
Background Trial: 5, reward: -310.79713672797266
Background Trial: 6, reward: -65.69723353932912
Background Trial: 7, reward: -213.67043892629027
Background Trial: 8, reward: -442.2544540301392
Background Trial: 9, reward: -94.58351358401617
Iteration: 13, average_reward: -243.31528512206762

Policy train: iteration: 500, policy_loss: 0.117589
Policy train: iteration: 1000, policy_loss: 0.128279
Policy train: iteration: 1500, policy_loss: 0.080277
Policy train: iteration: 2000, policy_loss: 0.098071
Policy train: iteration: 2500, policy_loss: 0.139457
Policy train: iteration: 3000, policy_loss: 0.106285
Policy train: iteration: 3500, policy_loss: 0.105231
Policy train: iteration: 4000, policy_loss: 0.103376
Policy train: iteration: 4500, policy_loss: 0.123426
Policy train: iteration: 5000, policy_loss: 0.107082
Policy train: iteration: 5500, policy_loss: 0.076868
Policy train: iteration: 6000, policy_loss: 0.132304
Policy train: iteration: 6500, policy_loss: 0.109490
Policy train: iteration: 7000, policy_loss: 0.123146
Policy train: iteration: 7500, policy_loss: 0.124404
Policy train: iteration: 8000, policy_loss: 0.126786
Policy train: iteration: 8500, policy_loss: 0.109552
Policy train: iteration: 9000, policy_loss: 0.108163

Background Trial: 1, reward: -38.260970431553474
Background Trial: 2, reward: -478.3816096002049
Background Trial: 3, reward: -174.69064608887535
Background Trial: 4, reward: -10.18145665422685
Background Trial: 5, reward: -462.071060675691
Background Trial: 6, reward: -325.4776165537507
Background Trial: 7, reward: -53.24585901734514
Background Trial: 8, reward: -304.7600787582859
Background Trial: 9, reward: -259.2477974962838
Iteration: 14, average_reward: -234.03523280846855

Policy train: iteration: 500, policy_loss: 0.108967
Policy train: iteration: 1000, policy_loss: 0.128154
Policy train: iteration: 1500, policy_loss: 0.149548
Policy train: iteration: 2000, policy_loss: 0.105301
Policy train: iteration: 2500, policy_loss: 0.116237
Policy train: iteration: 3000, policy_loss: 0.123708
Policy train: iteration: 3500, policy_loss: 0.114275
Policy train: iteration: 4000, policy_loss: 0.133072
Policy train: iteration: 4500, policy_loss: 0.110838
Policy train: iteration: 5000, policy_loss: 0.103884
Policy train: iteration: 5500, policy_loss: 0.140860
Policy train: iteration: 6000, policy_loss: 0.096946
Policy train: iteration: 6500, policy_loss: 0.114328
Policy train: iteration: 7000, policy_loss: 0.137531
Policy train: iteration: 7500, policy_loss: 0.131062
Policy train: iteration: 8000, policy_loss: 0.116042
Policy train: iteration: 8500, policy_loss: 0.087207
Policy train: iteration: 9000, policy_loss: 0.092262

Background Trial: 1, reward: -560.9744788405771
Background Trial: 2, reward: -56.89026764579816
Background Trial: 3, reward: 200.63131282182465
Background Trial: 4, reward: -570.563895687101
Background Trial: 5, reward: -230.72830313536397
Background Trial: 6, reward: -77.61012006701651
Background Trial: 7, reward: -727.62562530806
Background Trial: 8, reward: -314.2678230477954
Background Trial: 9, reward: -289.82750530833476
Iteration: 15, average_reward: -291.9840784686914

Policy train: iteration: 500, policy_loss: 0.157694
Policy train: iteration: 1000, policy_loss: 0.101057
Policy train: iteration: 1500, policy_loss: 0.099310
Policy train: iteration: 2000, policy_loss: 0.095503
Policy train: iteration: 2500, policy_loss: 0.105702
Policy train: iteration: 3000, policy_loss: 0.098162
Policy train: iteration: 3500, policy_loss: 0.107216
Policy train: iteration: 4000, policy_loss: 0.157365
Policy train: iteration: 4500, policy_loss: 0.128677
Policy train: iteration: 5000, policy_loss: 0.118164
Policy train: iteration: 5500, policy_loss: 0.133671
Policy train: iteration: 6000, policy_loss: 0.125848
Policy train: iteration: 6500, policy_loss: 0.165313
Policy train: iteration: 7000, policy_loss: 0.134199
Policy train: iteration: 7500, policy_loss: 0.101805
Policy train: iteration: 8000, policy_loss: 0.128777
Policy train: iteration: 8500, policy_loss: 0.160649
Policy train: iteration: 9000, policy_loss: 0.093651

Background Trial: 1, reward: -80.89423277554519
Background Trial: 2, reward: -303.8938433099974
Background Trial: 3, reward: -101.33672214102124
Background Trial: 4, reward: -77.61524761355551
Background Trial: 5, reward: -1238.0249155600723
Background Trial: 6, reward: -138.5214638944402
Background Trial: 7, reward: -354.0086412651679
Background Trial: 8, reward: -103.64760555382135
Background Trial: 9, reward: -93.82779092173105
Iteration: 16, average_reward: -276.86338478170575

Policy train: iteration: 500, policy_loss: 0.098034
Policy train: iteration: 1000, policy_loss: 0.081706
Policy train: iteration: 1500, policy_loss: 0.106257
Policy train: iteration: 2000, policy_loss: 0.116063
Policy train: iteration: 2500, policy_loss: 0.130032
Policy train: iteration: 3000, policy_loss: 0.104783
Policy train: iteration: 3500, policy_loss: 0.105568
Policy train: iteration: 4000, policy_loss: 0.107091
Policy train: iteration: 4500, policy_loss: 0.095956
Policy train: iteration: 5000, policy_loss: 0.124308
Policy train: iteration: 5500, policy_loss: 0.105399
Policy train: iteration: 6000, policy_loss: 0.094620
Policy train: iteration: 6500, policy_loss: 0.112451
Policy train: iteration: 7000, policy_loss: 0.146377
Policy train: iteration: 7500, policy_loss: 0.130258
Policy train: iteration: 8000, policy_loss: 0.099003
Policy train: iteration: 8500, policy_loss: 0.109450
Policy train: iteration: 9000, policy_loss: 0.091256

Background Trial: 1, reward: -52.86241606632778
Background Trial: 2, reward: -207.26213637563788
Background Trial: 3, reward: -31.847939621328962
Background Trial: 4, reward: -35.597683006866674
Background Trial: 5, reward: -368.4434174479544
Background Trial: 6, reward: -346.20973206274533
Background Trial: 7, reward: -381.1564422279836
Background Trial: 8, reward: -755.8932616194102
Background Trial: 9, reward: -701.9118258824722
Iteration: 17, average_reward: -320.1316504789697

Policy train: iteration: 500, policy_loss: 0.104120
Policy train: iteration: 1000, policy_loss: 0.137493
Policy train: iteration: 1500, policy_loss: 0.165103
Policy train: iteration: 2000, policy_loss: 0.094463
Policy train: iteration: 2500, policy_loss: 0.091785
Policy train: iteration: 3000, policy_loss: 0.111342
Policy train: iteration: 3500, policy_loss: 0.104246
Policy train: iteration: 4000, policy_loss: 0.107444
Policy train: iteration: 4500, policy_loss: 0.124761
Policy train: iteration: 5000, policy_loss: 0.128061
Policy train: iteration: 5500, policy_loss: 0.091739
Policy train: iteration: 6000, policy_loss: 0.132828
Policy train: iteration: 6500, policy_loss: 0.114673
Policy train: iteration: 7000, policy_loss: 0.111054
Policy train: iteration: 7500, policy_loss: 0.125594
Policy train: iteration: 8000, policy_loss: 0.118248
Policy train: iteration: 8500, policy_loss: 0.088059
Policy train: iteration: 9000, policy_loss: 0.132150

Background Trial: 1, reward: -220.79213066516778
Background Trial: 2, reward: -225.78352417266882
Background Trial: 3, reward: 240.07438478491093
Background Trial: 4, reward: -282.60672332031277
Background Trial: 5, reward: -351.4523222068573
Background Trial: 6, reward: -346.75782609441626
Background Trial: 7, reward: -65.35590587241842
Background Trial: 8, reward: -524.2681038573976
Background Trial: 9, reward: -372.6052763634029
Iteration: 18, average_reward: -238.83860308530342

Policy train: iteration: 500, policy_loss: 0.126311
Policy train: iteration: 1000, policy_loss: 0.091233
Policy train: iteration: 1500, policy_loss: 0.102319
Policy train: iteration: 2000, policy_loss: 0.066804
Policy train: iteration: 2500, policy_loss: 0.190180
Policy train: iteration: 3000, policy_loss: 0.117991
Policy train: iteration: 3500, policy_loss: 0.137564
Policy train: iteration: 4000, policy_loss: 0.121743
Policy train: iteration: 4500, policy_loss: 0.156824
Policy train: iteration: 5000, policy_loss: 0.122222
Policy train: iteration: 5500, policy_loss: 0.110007
Policy train: iteration: 6000, policy_loss: 0.109788
Policy train: iteration: 6500, policy_loss: 0.113348
Policy train: iteration: 7000, policy_loss: 0.105968
Policy train: iteration: 7500, policy_loss: 0.089355
Policy train: iteration: 8000, policy_loss: 0.101141
Policy train: iteration: 8500, policy_loss: 0.090603
Policy train: iteration: 9000, policy_loss: 0.116615

Background Trial: 1, reward: -441.4023938299775
Background Trial: 2, reward: -80.22271900107428
Background Trial: 3, reward: -305.41209976959146
Background Trial: 4, reward: -616.3549284092555
Background Trial: 5, reward: -87.42460621153124
Background Trial: 6, reward: 16.416744786435544
Background Trial: 7, reward: -246.36783785233249
Background Trial: 8, reward: -354.6743685351355
Background Trial: 9, reward: -424.0937149204406
Iteration: 19, average_reward: -282.17065819365587

Policy train: iteration: 500, policy_loss: 0.113986
Policy train: iteration: 1000, policy_loss: 0.101602
Policy train: iteration: 1500, policy_loss: 0.135846
Policy train: iteration: 2000, policy_loss: 0.146986
Policy train: iteration: 2500, policy_loss: 0.083253
Policy train: iteration: 3000, policy_loss: 0.123030
Policy train: iteration: 3500, policy_loss: 0.090434
Policy train: iteration: 4000, policy_loss: 0.111986
Policy train: iteration: 4500, policy_loss: 0.093999
Policy train: iteration: 5000, policy_loss: 0.123208
Policy train: iteration: 5500, policy_loss: 0.111958
Policy train: iteration: 6000, policy_loss: 0.087222
Policy train: iteration: 6500, policy_loss: 0.108169
Policy train: iteration: 7000, policy_loss: 0.097681
Policy train: iteration: 7500, policy_loss: 0.112285
Policy train: iteration: 8000, policy_loss: 0.081921
Policy train: iteration: 8500, policy_loss: 0.091264
Policy train: iteration: 9000, policy_loss: 0.086307

Background Trial: 1, reward: -352.6343881803898
Background Trial: 2, reward: -454.6734935123259
Background Trial: 3, reward: -59.352696731693705
Background Trial: 4, reward: -263.2876784963063
Background Trial: 5, reward: -463.27294054206146
Background Trial: 6, reward: -307.4038076409685
Background Trial: 7, reward: 0.1836879447241273
Background Trial: 8, reward: -357.9769027674863
Background Trial: 9, reward: -230.04582687118386
Iteration: 20, average_reward: -276.4960051997435

Policy train: iteration: 500, policy_loss: 0.117438
Policy train: iteration: 1000, policy_loss: 0.085633
Policy train: iteration: 1500, policy_loss: 0.106644
Policy train: iteration: 2000, policy_loss: 0.092285
Policy train: iteration: 2500, policy_loss: 0.102517
Policy train: iteration: 3000, policy_loss: 0.077450
Policy train: iteration: 3500, policy_loss: 0.074648
Policy train: iteration: 4000, policy_loss: 0.105359
Policy train: iteration: 4500, policy_loss: 0.123627
Policy train: iteration: 5000, policy_loss: 0.115705
Policy train: iteration: 5500, policy_loss: 0.099497
Policy train: iteration: 6000, policy_loss: 0.138203
Policy train: iteration: 6500, policy_loss: 0.134357
Policy train: iteration: 7000, policy_loss: 0.147305
Policy train: iteration: 7500, policy_loss: 0.163819
Policy train: iteration: 8000, policy_loss: 0.110446
Policy train: iteration: 8500, policy_loss: 0.139547
Policy train: iteration: 9000, policy_loss: 0.101710

Background Trial: 1, reward: -363.88454325383475
Background Trial: 2, reward: -515.0588871099352
Background Trial: 3, reward: -169.58061110813503
Background Trial: 4, reward: -39.991982022294266
Background Trial: 5, reward: -73.6954223582483
Background Trial: 6, reward: -72.35535336206262
Background Trial: 7, reward: -427.13938535183235
Background Trial: 8, reward: -395.49147810474415
Background Trial: 9, reward: -358.3348621936597
Iteration: 21, average_reward: -268.3925027627496

Policy train: iteration: 500, policy_loss: 0.116711
Policy train: iteration: 1000, policy_loss: 0.097913
Policy train: iteration: 1500, policy_loss: 0.098266
Policy train: iteration: 2000, policy_loss: 0.063675
Policy train: iteration: 2500, policy_loss: 0.091883
Policy train: iteration: 3000, policy_loss: 0.103351
Policy train: iteration: 3500, policy_loss: 0.126548
Policy train: iteration: 4000, policy_loss: 0.109367
Policy train: iteration: 4500, policy_loss: 0.116289
Policy train: iteration: 5000, policy_loss: 0.155558
Policy train: iteration: 5500, policy_loss: 0.067316
Policy train: iteration: 6000, policy_loss: 0.098930
Policy train: iteration: 6500, policy_loss: 0.104319
Policy train: iteration: 7000, policy_loss: 0.103280
Policy train: iteration: 7500, policy_loss: 0.095125
Policy train: iteration: 8000, policy_loss: 0.087587
Policy train: iteration: 8500, policy_loss: 0.123239
Policy train: iteration: 9000, policy_loss: 0.122560

Background Trial: 1, reward: -59.96336403030486
Background Trial: 2, reward: -349.77062533063327
Background Trial: 3, reward: -33.221410207668555
Background Trial: 4, reward: -315.8570641615962
Background Trial: 5, reward: -213.7428808976317
Background Trial: 6, reward: -203.41407825774638
Background Trial: 7, reward: -329.8427269423245
Background Trial: 8, reward: -170.1615932411292
Background Trial: 9, reward: -490.84015075976953
Iteration: 22, average_reward: -240.7570993143116

Policy train: iteration: 500, policy_loss: 0.114184
Policy train: iteration: 1000, policy_loss: 0.107413
Policy train: iteration: 1500, policy_loss: 0.109716
Policy train: iteration: 2000, policy_loss: 0.081591
Policy train: iteration: 2500, policy_loss: 0.113754
Policy train: iteration: 3000, policy_loss: 0.126398
Policy train: iteration: 3500, policy_loss: 0.107536
Policy train: iteration: 4000, policy_loss: 0.099036
Policy train: iteration: 4500, policy_loss: 0.130115
Policy train: iteration: 5000, policy_loss: 0.096200
Policy train: iteration: 5500, policy_loss: 0.089473
Policy train: iteration: 6000, policy_loss: 0.128400
Policy train: iteration: 6500, policy_loss: 0.106067
Policy train: iteration: 7000, policy_loss: 0.080140
Policy train: iteration: 7500, policy_loss: 0.098478
Policy train: iteration: 8000, policy_loss: 0.111777
Policy train: iteration: 8500, policy_loss: 0.131923
Policy train: iteration: 9000, policy_loss: 0.097750

Background Trial: 1, reward: -51.600845092681766
Background Trial: 2, reward: -400.36272506071424
Background Trial: 3, reward: -459.3909149672376
Background Trial: 4, reward: -385.47750437014724
Background Trial: 5, reward: -91.07308294808693
Background Trial: 6, reward: -251.1815360600586
Background Trial: 7, reward: -510.9662652262726
Background Trial: 8, reward: -14.829126711743072
Background Trial: 9, reward: -129.01353604557056
Iteration: 23, average_reward: -254.87728183139026

Policy train: iteration: 500, policy_loss: 0.133226
Policy train: iteration: 1000, policy_loss: 0.104404
Policy train: iteration: 1500, policy_loss: 0.085036
Policy train: iteration: 2000, policy_loss: 0.109226
Policy train: iteration: 2500, policy_loss: 0.098945
Policy train: iteration: 3000, policy_loss: 0.109504
Policy train: iteration: 3500, policy_loss: 0.110592
Policy train: iteration: 4000, policy_loss: 0.117687
Policy train: iteration: 4500, policy_loss: 0.081902
Policy train: iteration: 5000, policy_loss: 0.114228
Policy train: iteration: 5500, policy_loss: 0.104765
Policy train: iteration: 6000, policy_loss: 0.118400
Policy train: iteration: 6500, policy_loss: 0.094761
Policy train: iteration: 7000, policy_loss: 0.106198
Policy train: iteration: 7500, policy_loss: 0.125356
Policy train: iteration: 8000, policy_loss: 0.084152
Policy train: iteration: 8500, policy_loss: 0.083049
Policy train: iteration: 9000, policy_loss: 0.121034

Background Trial: 1, reward: -625.532278318519
Background Trial: 2, reward: -407.58532280823783
Background Trial: 3, reward: -342.2364337173735
Background Trial: 4, reward: -18.622686570206838
Background Trial: 5, reward: -810.0958362312138
Background Trial: 6, reward: -323.1095153443159
Background Trial: 7, reward: -28.299980972038014
Background Trial: 8, reward: -61.12657649187831
Background Trial: 9, reward: -185.13898617187974
Iteration: 24, average_reward: -311.30529073618476

Policy train: iteration: 500, policy_loss: 0.088661
Policy train: iteration: 1000, policy_loss: 0.105982
Policy train: iteration: 1500, policy_loss: 0.081332
Policy train: iteration: 2000, policy_loss: 0.098864
Policy train: iteration: 2500, policy_loss: 0.136582
Policy train: iteration: 3000, policy_loss: 0.104818
Policy train: iteration: 3500, policy_loss: 0.116084
Policy train: iteration: 4000, policy_loss: 0.084519
Policy train: iteration: 4500, policy_loss: 0.086599
Policy train: iteration: 5000, policy_loss: 0.085396
Policy train: iteration: 5500, policy_loss: 0.107357
Policy train: iteration: 6000, policy_loss: 0.075750
Policy train: iteration: 6500, policy_loss: 0.101285
Policy train: iteration: 7000, policy_loss: 0.077075
Policy train: iteration: 7500, policy_loss: 0.119066
Policy train: iteration: 8000, policy_loss: 0.083687
Policy train: iteration: 8500, policy_loss: 0.084196
Policy train: iteration: 9000, policy_loss: 0.088746

Background Trial: 1, reward: -268.598927343098
Background Trial: 2, reward: -108.88772829460471
Background Trial: 3, reward: -239.90548703144393
Background Trial: 4, reward: -0.5497575643035333
Background Trial: 5, reward: -90.9261538896479
Background Trial: 6, reward: -16.180232255581572
Background Trial: 7, reward: -503.41743097208007
Background Trial: 8, reward: -140.30169582756093
Background Trial: 9, reward: -449.21393958815685
Iteration: 25, average_reward: -201.99792808516418

Policy train: iteration: 500, policy_loss: 0.094945
Policy train: iteration: 1000, policy_loss: 0.056016
Policy train: iteration: 1500, policy_loss: 0.082810
Policy train: iteration: 2000, policy_loss: 0.073790
Policy train: iteration: 2500, policy_loss: 0.127071
Policy train: iteration: 3000, policy_loss: 0.088906
Policy train: iteration: 3500, policy_loss: 0.090060
Policy train: iteration: 4000, policy_loss: 0.099136
Policy train: iteration: 4500, policy_loss: 0.160338
Policy train: iteration: 5000, policy_loss: 0.080730
Policy train: iteration: 5500, policy_loss: 0.106463
Policy train: iteration: 6000, policy_loss: 0.122223
Policy train: iteration: 6500, policy_loss: 0.116791
Policy train: iteration: 7000, policy_loss: 0.116924
Policy train: iteration: 7500, policy_loss: 0.123029
Policy train: iteration: 8000, policy_loss: 0.096388
Policy train: iteration: 8500, policy_loss: 0.087692
Policy train: iteration: 9000, policy_loss: 0.093838

Background Trial: 1, reward: -317.9342875615051
Background Trial: 2, reward: -606.3786463194153
Background Trial: 3, reward: -107.05838382628973
Background Trial: 4, reward: -17.457671336285742
Background Trial: 5, reward: -166.95497984506505
Background Trial: 6, reward: -272.93904571714245
Background Trial: 7, reward: -281.450626878226
Background Trial: 8, reward: -354.21884127205436
Background Trial: 9, reward: -516.1352394648319
Iteration: 26, average_reward: -293.39196913564615

Policy train: iteration: 500, policy_loss: 0.082485
Policy train: iteration: 1000, policy_loss: 0.103285
Policy train: iteration: 1500, policy_loss: 0.093233
Policy train: iteration: 2000, policy_loss: 0.090859
Policy train: iteration: 2500, policy_loss: 0.097271
Policy train: iteration: 3000, policy_loss: 0.089421
Policy train: iteration: 3500, policy_loss: 0.099500
Policy train: iteration: 4000, policy_loss: 0.106242
Policy train: iteration: 4500, policy_loss: 0.082774
Policy train: iteration: 5000, policy_loss: 0.130250
Policy train: iteration: 5500, policy_loss: 0.098265
Policy train: iteration: 6000, policy_loss: 0.098118
Policy train: iteration: 6500, policy_loss: 0.106724
Policy train: iteration: 7000, policy_loss: 0.078528
Policy train: iteration: 7500, policy_loss: 0.086472
Policy train: iteration: 8000, policy_loss: 0.094289
Policy train: iteration: 8500, policy_loss: 0.087675
Policy train: iteration: 9000, policy_loss: 0.119307

Background Trial: 1, reward: -206.0661215042322
Background Trial: 2, reward: -504.1386835535514
Background Trial: 3, reward: -104.57566588549044
Background Trial: 4, reward: -713.8953257879751
Background Trial: 5, reward: -300.7229117564769
Background Trial: 6, reward: -315.65193594489597
Background Trial: 7, reward: -278.45271060453206
Background Trial: 8, reward: -314.74860901557304
Background Trial: 9, reward: -113.88732426622235
Iteration: 27, average_reward: -316.9043653687722

Policy train: iteration: 500, policy_loss: 0.078413
Policy train: iteration: 1000, policy_loss: 0.081134
Policy train: iteration: 1500, policy_loss: 0.099449
Policy train: iteration: 2000, policy_loss: 0.103621
Policy train: iteration: 2500, policy_loss: 0.101760
Policy train: iteration: 3000, policy_loss: 0.105546
Policy train: iteration: 3500, policy_loss: 0.091195
Policy train: iteration: 4000, policy_loss: 0.104815
Policy train: iteration: 4500, policy_loss: 0.093411
Policy train: iteration: 5000, policy_loss: 0.088160
Policy train: iteration: 5500, policy_loss: 0.103072
Policy train: iteration: 6000, policy_loss: 0.098950
Policy train: iteration: 6500, policy_loss: 0.119479
Policy train: iteration: 7000, policy_loss: 0.076274
Policy train: iteration: 7500, policy_loss: 0.080814
Policy train: iteration: 8000, policy_loss: 0.103731
Policy train: iteration: 8500, policy_loss: 0.118017
Policy train: iteration: 9000, policy_loss: 0.102991

Background Trial: 1, reward: -10.442919624245292
Background Trial: 2, reward: -35.27386226259786
Background Trial: 3, reward: 59.38366560074428
Background Trial: 4, reward: -447.945314457248
Background Trial: 5, reward: -328.8865765225777
Background Trial: 6, reward: -171.6540632555704
Background Trial: 7, reward: -320.9785015413366
Background Trial: 8, reward: -80.29248256916053
Background Trial: 9, reward: -509.36127002668354
Iteration: 28, average_reward: -205.0501471842973

Policy train: iteration: 500, policy_loss: 0.099103
Policy train: iteration: 1000, policy_loss: 0.093988
Policy train: iteration: 1500, policy_loss: 0.091721
Policy train: iteration: 2000, policy_loss: 0.079615
Policy train: iteration: 2500, policy_loss: 0.125298
Policy train: iteration: 3000, policy_loss: 0.084391
Policy train: iteration: 3500, policy_loss: 0.141120
Policy train: iteration: 4000, policy_loss: 0.095784
Policy train: iteration: 4500, policy_loss: 0.093609
Policy train: iteration: 5000, policy_loss: 0.064546
Policy train: iteration: 5500, policy_loss: 0.080083
Policy train: iteration: 6000, policy_loss: 0.078173
Policy train: iteration: 6500, policy_loss: 0.129025
Policy train: iteration: 7000, policy_loss: 0.089096
Policy train: iteration: 7500, policy_loss: 0.087104
Policy train: iteration: 8000, policy_loss: 0.101406
Policy train: iteration: 8500, policy_loss: 0.106176
Policy train: iteration: 9000, policy_loss: 0.098800

Background Trial: 1, reward: -55.13863593364557
Background Trial: 2, reward: -63.503218675612395
Background Trial: 3, reward: -72.68730548939789
Background Trial: 4, reward: -419.6232191174958
Background Trial: 5, reward: -295.1897650246922
Background Trial: 6, reward: -353.0296379322966
Background Trial: 7, reward: -371.33398180481225
Background Trial: 8, reward: -367.9384478293893
Background Trial: 9, reward: -109.27461349039915
Iteration: 29, average_reward: -234.1909805886379

Policy train: iteration: 500, policy_loss: 0.077478
Policy train: iteration: 1000, policy_loss: 0.115982
Policy train: iteration: 1500, policy_loss: 0.092065
Policy train: iteration: 2000, policy_loss: 0.095702
Policy train: iteration: 2500, policy_loss: 0.072763
Policy train: iteration: 3000, policy_loss: 0.095463
Policy train: iteration: 3500, policy_loss: 0.095182
Policy train: iteration: 4000, policy_loss: 0.130248
Policy train: iteration: 4500, policy_loss: 0.076250
Policy train: iteration: 5000, policy_loss: 0.075669
Policy train: iteration: 5500, policy_loss: 0.076559
Policy train: iteration: 6000, policy_loss: 0.072819
Policy train: iteration: 6500, policy_loss: 0.089126
Policy train: iteration: 7000, policy_loss: 0.076641
Policy train: iteration: 7500, policy_loss: 0.081364
Policy train: iteration: 8000, policy_loss: 0.092702
Policy train: iteration: 8500, policy_loss: 0.092386
Policy train: iteration: 9000, policy_loss: 0.119517

Background Trial: 1, reward: -65.40050114337596
Background Trial: 2, reward: -39.247278549695125
Background Trial: 3, reward: -71.12900989698579
Background Trial: 4, reward: -45.858087270897
Background Trial: 5, reward: -298.7801515504925
Background Trial: 6, reward: -479.6965239109139
Background Trial: 7, reward: -412.8427277157335
Background Trial: 8, reward: -669.6348658267889
Background Trial: 9, reward: -592.1149973664865
Iteration: 30, average_reward: -297.18934924792995

Policy train: iteration: 500, policy_loss: 0.091317
Policy train: iteration: 1000, policy_loss: 0.071769
Policy train: iteration: 1500, policy_loss: 0.108007
Policy train: iteration: 2000, policy_loss: 0.111662
Policy train: iteration: 2500, policy_loss: 0.080286
Policy train: iteration: 3000, policy_loss: 0.101347
Policy train: iteration: 3500, policy_loss: 0.078100
Policy train: iteration: 4000, policy_loss: 0.095386
Policy train: iteration: 4500, policy_loss: 0.087705
Policy train: iteration: 5000, policy_loss: 0.084386
Policy train: iteration: 5500, policy_loss: 0.085090
Policy train: iteration: 6000, policy_loss: 0.100817
Policy train: iteration: 6500, policy_loss: 0.114790
Policy train: iteration: 7000, policy_loss: 0.112205
Policy train: iteration: 7500, policy_loss: 0.077517
Policy train: iteration: 8000, policy_loss: 0.138227
Policy train: iteration: 8500, policy_loss: 0.107144
Policy train: iteration: 9000, policy_loss: 0.093767

Background Trial: 1, reward: -529.3486081315923
Background Trial: 2, reward: -181.67972695953463
Background Trial: 3, reward: -343.30998650104016
Background Trial: 4, reward: -356.8281813560769
Background Trial: 5, reward: -316.36386787160484
Background Trial: 6, reward: -247.12130314932875
Background Trial: 7, reward: -50.53542706060624
Background Trial: 8, reward: -307.93437790321616
Background Trial: 9, reward: -655.263902080341
Iteration: 31, average_reward: -332.0428201125935

Policy train: iteration: 500, policy_loss: 0.095473
Policy train: iteration: 1000, policy_loss: 0.131151
Policy train: iteration: 1500, policy_loss: 0.095307
Policy train: iteration: 2000, policy_loss: 0.095900
Policy train: iteration: 2500, policy_loss: 0.095443
Policy train: iteration: 3000, policy_loss: 0.077923
Policy train: iteration: 3500, policy_loss: 0.095638
Policy train: iteration: 4000, policy_loss: 0.093274
Policy train: iteration: 4500, policy_loss: 0.117394
Policy train: iteration: 5000, policy_loss: 0.088188
Policy train: iteration: 5500, policy_loss: 0.133414
Policy train: iteration: 6000, policy_loss: 0.126842
Policy train: iteration: 6500, policy_loss: 0.112011
Policy train: iteration: 7000, policy_loss: 0.118167
Policy train: iteration: 7500, policy_loss: 0.090446
Policy train: iteration: 8000, policy_loss: 0.133808
Policy train: iteration: 8500, policy_loss: 0.113478
Policy train: iteration: 9000, policy_loss: 0.089352

Background Trial: 1, reward: -58.506897313993605
Background Trial: 2, reward: -196.8569535568639
Background Trial: 3, reward: -252.7833692840869
Background Trial: 4, reward: -11.634505295627008
Background Trial: 5, reward: -667.9196013369761
Background Trial: 6, reward: -373.49981669495753
Background Trial: 7, reward: -18.90140422181335
Background Trial: 8, reward: -318.42234678097077
Background Trial: 9, reward: 28.23424044613384
Iteration: 32, average_reward: -207.81007267101734

Policy train: iteration: 500, policy_loss: 0.079339
Policy train: iteration: 1000, policy_loss: 0.085006
Policy train: iteration: 1500, policy_loss: 0.126899
Policy train: iteration: 2000, policy_loss: 0.103985
Policy train: iteration: 2500, policy_loss: 0.082250
Policy train: iteration: 3000, policy_loss: 0.168497
Policy train: iteration: 3500, policy_loss: 0.103421
Policy train: iteration: 4000, policy_loss: 0.101781
Policy train: iteration: 4500, policy_loss: 0.092211
Policy train: iteration: 5000, policy_loss: 0.139286
Policy train: iteration: 5500, policy_loss: 0.114402
Policy train: iteration: 6000, policy_loss: 0.104025
Policy train: iteration: 6500, policy_loss: 0.086783
Policy train: iteration: 7000, policy_loss: 0.112692
Policy train: iteration: 7500, policy_loss: 0.104164
Policy train: iteration: 8000, policy_loss: 0.098225
Policy train: iteration: 8500, policy_loss: 0.105537
Policy train: iteration: 9000, policy_loss: 0.096183

Background Trial: 1, reward: -179.17742790545572
Background Trial: 2, reward: -301.3814827030071
Background Trial: 3, reward: -367.2991246114975
Background Trial: 4, reward: -253.11086658780573
Background Trial: 5, reward: -74.61281949821078
Background Trial: 6, reward: -567.4997857629501
Background Trial: 7, reward: -500.23406147980216
Background Trial: 8, reward: -740.4200017559295
Background Trial: 9, reward: 22.412792989608576
Iteration: 33, average_reward: -329.0358641461167

Policy train: iteration: 500, policy_loss: 0.071484
Policy train: iteration: 1000, policy_loss: 0.123531
Policy train: iteration: 1500, policy_loss: 0.151160
Policy train: iteration: 2000, policy_loss: 0.091414
Policy train: iteration: 2500, policy_loss: 0.117538
Policy train: iteration: 3000, policy_loss: 0.113816
Policy train: iteration: 3500, policy_loss: 0.124723
Policy train: iteration: 4000, policy_loss: 0.088740
Policy train: iteration: 4500, policy_loss: 0.098332
Policy train: iteration: 5000, policy_loss: 0.161401
Policy train: iteration: 5500, policy_loss: 0.073944
Policy train: iteration: 6000, policy_loss: 0.090244
Policy train: iteration: 6500, policy_loss: 0.092392
Policy train: iteration: 7000, policy_loss: 0.076484
Policy train: iteration: 7500, policy_loss: 0.096477
Policy train: iteration: 8000, policy_loss: 0.098615
Policy train: iteration: 8500, policy_loss: 0.087390
Policy train: iteration: 9000, policy_loss: 0.113412

Background Trial: 1, reward: 24.76198098122218
Background Trial: 2, reward: -372.7896175419144
Background Trial: 3, reward: -74.07597174578312
Background Trial: 4, reward: -240.54243000500406
Background Trial: 5, reward: -500.6809193496777
Background Trial: 6, reward: -47.685634525365124
Background Trial: 7, reward: -557.1839502730284
Background Trial: 8, reward: -343.90603626599534
Background Trial: 9, reward: -290.9222933630742
Iteration: 34, average_reward: -267.00276356540223

Policy train: iteration: 500, policy_loss: 0.090588
Policy train: iteration: 1000, policy_loss: 0.065515
Policy train: iteration: 1500, policy_loss: 0.136206
Policy train: iteration: 2000, policy_loss: 0.108834
Policy train: iteration: 2500, policy_loss: 0.083898
Policy train: iteration: 3000, policy_loss: 0.088504
Policy train: iteration: 3500, policy_loss: 0.076441
Policy train: iteration: 4000, policy_loss: 0.102495
Policy train: iteration: 4500, policy_loss: 0.071206
Policy train: iteration: 5000, policy_loss: 0.098875
Policy train: iteration: 5500, policy_loss: 0.103505
Policy train: iteration: 6000, policy_loss: 0.093860
Policy train: iteration: 6500, policy_loss: 0.122510
Policy train: iteration: 7000, policy_loss: 0.077256
Policy train: iteration: 7500, policy_loss: 0.080538
Policy train: iteration: 8000, policy_loss: 0.135189
Policy train: iteration: 8500, policy_loss: 0.069432
Policy train: iteration: 9000, policy_loss: 0.107895

Background Trial: 1, reward: -377.40327673321286
Background Trial: 2, reward: -473.04493973349923
Background Trial: 3, reward: -639.8562130634327
Background Trial: 4, reward: -340.71040832085805
Background Trial: 5, reward: -61.654591549455766
Background Trial: 6, reward: -469.2324014584717
Background Trial: 7, reward: -487.44031842777486
Background Trial: 8, reward: -276.3281861857937
Background Trial: 9, reward: -398.1993110010131
Iteration: 35, average_reward: -391.5410718303902

Policy train: iteration: 500, policy_loss: 0.113382
Policy train: iteration: 1000, policy_loss: 0.086830
Policy train: iteration: 1500, policy_loss: 0.078015
Policy train: iteration: 2000, policy_loss: 0.077502
Policy train: iteration: 2500, policy_loss: 0.089675
Policy train: iteration: 3000, policy_loss: 0.082540
Policy train: iteration: 3500, policy_loss: 0.075642
Policy train: iteration: 4000, policy_loss: 0.090412
Policy train: iteration: 4500, policy_loss: 0.073265
Policy train: iteration: 5000, policy_loss: 0.086338
Policy train: iteration: 5500, policy_loss: 0.132146
Policy train: iteration: 6000, policy_loss: 0.100149
Policy train: iteration: 6500, policy_loss: 0.119002
Policy train: iteration: 7000, policy_loss: 0.106523
Policy train: iteration: 7500, policy_loss: 0.074965
Policy train: iteration: 8000, policy_loss: 0.109881
Policy train: iteration: 8500, policy_loss: 0.087796
Policy train: iteration: 9000, policy_loss: 0.080962

Background Trial: 1, reward: -425.5242713816791
Background Trial: 2, reward: -276.4235349527844
Background Trial: 3, reward: 31.120104924086746
Background Trial: 4, reward: -353.63646696418544
Background Trial: 5, reward: -267.5496719768539
Background Trial: 6, reward: -336.09126581439205
Background Trial: 7, reward: -292.2798944413107
Background Trial: 8, reward: -422.5778591676505
Background Trial: 9, reward: -209.696330846261
Iteration: 36, average_reward: -283.62879895789223

Policy train: iteration: 500, policy_loss: 0.129774
Policy train: iteration: 1000, policy_loss: 0.100654
Policy train: iteration: 1500, policy_loss: 0.107155
Policy train: iteration: 2000, policy_loss: 0.073083
Policy train: iteration: 2500, policy_loss: 0.102461
Policy train: iteration: 3000, policy_loss: 0.091447
Policy train: iteration: 3500, policy_loss: 0.122954
Policy train: iteration: 4000, policy_loss: 0.100184
Policy train: iteration: 4500, policy_loss: 0.071069
Policy train: iteration: 5000, policy_loss: 0.080237
Policy train: iteration: 5500, policy_loss: 0.086088
Policy train: iteration: 6000, policy_loss: 0.083435
Policy train: iteration: 6500, policy_loss: 0.056869
Policy train: iteration: 7000, policy_loss: 0.127565
Policy train: iteration: 7500, policy_loss: 0.065034
Policy train: iteration: 8000, policy_loss: 0.110164
Policy train: iteration: 8500, policy_loss: 0.067863
Policy train: iteration: 9000, policy_loss: 0.092722

Background Trial: 1, reward: -437.63887264303804
Background Trial: 2, reward: -495.38579928024143
Background Trial: 3, reward: -25.965234272515644
Background Trial: 4, reward: -390.9163850659546
Background Trial: 5, reward: -330.43616146926263
Background Trial: 6, reward: -42.00454551265427
Background Trial: 7, reward: -37.05725337656952
Background Trial: 8, reward: -67.56690285330617
Background Trial: 9, reward: -593.4594097716301
Iteration: 37, average_reward: -268.93672936057465

Policy train: iteration: 500, policy_loss: 0.180914
Policy train: iteration: 1000, policy_loss: 0.097789
Policy train: iteration: 1500, policy_loss: 0.066699
Policy train: iteration: 2000, policy_loss: 0.131520
Policy train: iteration: 2500, policy_loss: 0.094801
Policy train: iteration: 3000, policy_loss: 0.106334
Policy train: iteration: 3500, policy_loss: 0.094277
Policy train: iteration: 4000, policy_loss: 0.127169
Policy train: iteration: 4500, policy_loss: 0.100246
Policy train: iteration: 5000, policy_loss: 0.106030
Policy train: iteration: 5500, policy_loss: 0.082807
Policy train: iteration: 6000, policy_loss: 0.089317
Policy train: iteration: 6500, policy_loss: 0.111307
Policy train: iteration: 7000, policy_loss: 0.090450
Policy train: iteration: 7500, policy_loss: 0.059875
Policy train: iteration: 8000, policy_loss: 0.143216
Policy train: iteration: 8500, policy_loss: 0.083505
Policy train: iteration: 9000, policy_loss: 0.095501

Background Trial: 1, reward: -40.17247990920352
Background Trial: 2, reward: -318.03154338272435
Background Trial: 3, reward: -280.80415455749005
Background Trial: 4, reward: -469.12722309918223
Background Trial: 5, reward: -142.0902661571337
Background Trial: 6, reward: -286.64870561334544
Background Trial: 7, reward: -465.6837945902959
Background Trial: 8, reward: -24.70948305936443
Background Trial: 9, reward: -82.94222644184543
Iteration: 38, average_reward: -234.46776409006503

Policy train: iteration: 500, policy_loss: 0.131714
Policy train: iteration: 1000, policy_loss: 0.089623
Policy train: iteration: 1500, policy_loss: 0.136953
Policy train: iteration: 2000, policy_loss: 0.091387
Policy train: iteration: 2500, policy_loss: 0.149994
Policy train: iteration: 3000, policy_loss: 0.102828
Policy train: iteration: 3500, policy_loss: 0.081685
Policy train: iteration: 4000, policy_loss: 0.073046
Policy train: iteration: 4500, policy_loss: 0.113898
Policy train: iteration: 5000, policy_loss: 0.072638
Policy train: iteration: 5500, policy_loss: 0.083278
Policy train: iteration: 6000, policy_loss: 0.098922
Policy train: iteration: 6500, policy_loss: 0.112374
Policy train: iteration: 7000, policy_loss: 0.127819
Policy train: iteration: 7500, policy_loss: 0.078431
Policy train: iteration: 8000, policy_loss: 0.104149
Policy train: iteration: 8500, policy_loss: 0.091989
Policy train: iteration: 9000, policy_loss: 0.104819

Background Trial: 1, reward: -216.98243099669827
Background Trial: 2, reward: -206.9862320716274
Background Trial: 3, reward: -360.93022814638005
Background Trial: 4, reward: -991.701936409656
Background Trial: 5, reward: -348.33943985225204
Background Trial: 6, reward: -107.1276059936546
Background Trial: 7, reward: -400.19169286715396
Background Trial: 8, reward: -503.03400047964897
Background Trial: 9, reward: -25.900736279651127
Iteration: 39, average_reward: -351.24381145519135

Policy train: iteration: 500, policy_loss: 0.104234
Policy train: iteration: 1000, policy_loss: 0.115098
Policy train: iteration: 1500, policy_loss: 0.075907
Policy train: iteration: 2000, policy_loss: 0.119455
Policy train: iteration: 2500, policy_loss: 0.117408
Policy train: iteration: 3000, policy_loss: 0.085747
Policy train: iteration: 3500, policy_loss: 0.090963
Policy train: iteration: 4000, policy_loss: 0.082234
Policy train: iteration: 4500, policy_loss: 0.079993
Policy train: iteration: 5000, policy_loss: 0.078801
Policy train: iteration: 5500, policy_loss: 0.103462
Policy train: iteration: 6000, policy_loss: 0.090924
Policy train: iteration: 6500, policy_loss: 0.112490
Policy train: iteration: 7000, policy_loss: 0.081589
Policy train: iteration: 7500, policy_loss: 0.138345
Policy train: iteration: 8000, policy_loss: 0.105380
Policy train: iteration: 8500, policy_loss: 0.098693
Policy train: iteration: 9000, policy_loss: 0.085697

Background Trial: 1, reward: -409.9519488045459
Background Trial: 2, reward: 11.371270956201784
Background Trial: 3, reward: -227.48659321534194
Background Trial: 4, reward: -374.19204958443635
Background Trial: 5, reward: -508.9617751618006
Background Trial: 6, reward: -6.145410485283776
Background Trial: 7, reward: -234.8251934044858
Background Trial: 8, reward: -406.04231285901443
Background Trial: 9, reward: -201.82548092798027
Iteration: 40, average_reward: -262.0066103874097

Policy train: iteration: 500, policy_loss: 0.080421
Policy train: iteration: 1000, policy_loss: 0.056743
Policy train: iteration: 1500, policy_loss: 0.113402
Policy train: iteration: 2000, policy_loss: 0.087226
Policy train: iteration: 2500, policy_loss: 0.107949
Policy train: iteration: 3000, policy_loss: 0.096011
Policy train: iteration: 3500, policy_loss: 0.111461
Policy train: iteration: 4000, policy_loss: 0.105177
Policy train: iteration: 4500, policy_loss: 0.103473
Policy train: iteration: 5000, policy_loss: 0.115436
Policy train: iteration: 5500, policy_loss: 0.107601
Policy train: iteration: 6000, policy_loss: 0.086299
Policy train: iteration: 6500, policy_loss: 0.082836
Policy train: iteration: 7000, policy_loss: 0.088547
Policy train: iteration: 7500, policy_loss: 0.093823
Policy train: iteration: 8000, policy_loss: 0.080037
Policy train: iteration: 8500, policy_loss: 0.165227
Policy train: iteration: 9000, policy_loss: 0.091285

Background Trial: 1, reward: -276.0665509587722
Background Trial: 2, reward: -67.11772828140056
Background Trial: 3, reward: -175.64643738358063
Background Trial: 4, reward: -368.4935072402299
Background Trial: 5, reward: -295.6100295609938
Background Trial: 6, reward: 282.5668495594839
Background Trial: 7, reward: -297.49090717767604
Background Trial: 8, reward: -451.35080526569294
Background Trial: 9, reward: -572.0002606231125
Iteration: 41, average_reward: -246.80104188133055

Policy train: iteration: 500, policy_loss: 0.070482
Policy train: iteration: 1000, policy_loss: 0.073190
Policy train: iteration: 1500, policy_loss: 0.121227
Policy train: iteration: 2000, policy_loss: 0.093007
Policy train: iteration: 2500, policy_loss: 0.109313
Policy train: iteration: 3000, policy_loss: 0.076865
Policy train: iteration: 3500, policy_loss: 0.082997
Policy train: iteration: 4000, policy_loss: 0.088396
Policy train: iteration: 4500, policy_loss: 0.095423
Policy train: iteration: 5000, policy_loss: 0.085856
Policy train: iteration: 5500, policy_loss: 0.107729
Policy train: iteration: 6000, policy_loss: 0.080513
Policy train: iteration: 6500, policy_loss: 0.093089
Policy train: iteration: 7000, policy_loss: 0.077688
Policy train: iteration: 7500, policy_loss: 0.079547
Policy train: iteration: 8000, policy_loss: 0.064719
Policy train: iteration: 8500, policy_loss: 0.077257
Policy train: iteration: 9000, policy_loss: 0.117120

Background Trial: 1, reward: -411.4886595068318
Background Trial: 2, reward: -309.56761920840853
Background Trial: 3, reward: -323.7528041482933
Background Trial: 4, reward: -562.3574161370491
Background Trial: 5, reward: -250.6935763375784
Background Trial: 6, reward: -457.18937797401753
Background Trial: 7, reward: -331.5468181470943
Background Trial: 8, reward: -34.31755613226832
Background Trial: 9, reward: -74.89571425015149
Iteration: 42, average_reward: -306.2010602046325

Policy train: iteration: 500, policy_loss: 0.073514
Policy train: iteration: 1000, policy_loss: 0.056495
Policy train: iteration: 1500, policy_loss: 0.120866
Policy train: iteration: 2000, policy_loss: 0.099974
Policy train: iteration: 2500, policy_loss: 0.115495
Policy train: iteration: 3000, policy_loss: 0.078272
Policy train: iteration: 3500, policy_loss: 0.089432
Policy train: iteration: 4000, policy_loss: 0.086008
Policy train: iteration: 4500, policy_loss: 0.066315
Policy train: iteration: 5000, policy_loss: 0.088131
Policy train: iteration: 5500, policy_loss: 0.095665
Policy train: iteration: 6000, policy_loss: 0.110519
Policy train: iteration: 6500, policy_loss: 0.069688
Policy train: iteration: 7000, policy_loss: 0.087068
Policy train: iteration: 7500, policy_loss: 0.094807
Policy train: iteration: 8000, policy_loss: 0.080157
Policy train: iteration: 8500, policy_loss: 0.082952
Policy train: iteration: 9000, policy_loss: 0.112920

Background Trial: 1, reward: -117.20697840484918
Background Trial: 2, reward: -45.5390527584889
Background Trial: 3, reward: -72.25350463850825
Background Trial: 4, reward: -308.6023807438427
Background Trial: 5, reward: -635.2637940548759
Background Trial: 6, reward: -130.4186251084758
Background Trial: 7, reward: -621.1850030915609
Background Trial: 8, reward: -489.30948133572304
Background Trial: 9, reward: -570.9865057020714
Iteration: 43, average_reward: -332.3072584264885

Policy train: iteration: 500, policy_loss: 0.106148
Policy train: iteration: 1000, policy_loss: 0.109045
Policy train: iteration: 1500, policy_loss: 0.104479
Policy train: iteration: 2000, policy_loss: 0.127728
Policy train: iteration: 2500, policy_loss: 0.128560
Policy train: iteration: 3000, policy_loss: 0.101348
Policy train: iteration: 3500, policy_loss: 0.075686
Policy train: iteration: 4000, policy_loss: 0.066539
Policy train: iteration: 4500, policy_loss: 0.085255
Policy train: iteration: 5000, policy_loss: 0.088611
Policy train: iteration: 5500, policy_loss: 0.100440
Policy train: iteration: 6000, policy_loss: 0.088655
Policy train: iteration: 6500, policy_loss: 0.064263
Policy train: iteration: 7000, policy_loss: 0.059049
Policy train: iteration: 7500, policy_loss: 0.095378
Policy train: iteration: 8000, policy_loss: 0.108102
Policy train: iteration: 8500, policy_loss: 0.118351
Policy train: iteration: 9000, policy_loss: 0.111169

Background Trial: 1, reward: -97.83047296431815
Background Trial: 2, reward: -64.716940819531
Background Trial: 3, reward: -337.5689390234222
Background Trial: 4, reward: -268.6919179126084
Background Trial: 5, reward: -604.333633143019
Background Trial: 6, reward: -31.577084539663616
Background Trial: 7, reward: -326.92723322963604
Background Trial: 8, reward: -254.08178435824138
Background Trial: 9, reward: -391.8871233296266
Iteration: 44, average_reward: -264.1794588133407

Policy train: iteration: 500, policy_loss: 0.111630
Policy train: iteration: 1000, policy_loss: 0.152595
Policy train: iteration: 1500, policy_loss: 0.116899
Policy train: iteration: 2000, policy_loss: 0.048166
Policy train: iteration: 2500, policy_loss: 0.102593
Policy train: iteration: 3000, policy_loss: 0.162643
Policy train: iteration: 3500, policy_loss: 0.135887
Policy train: iteration: 4000, policy_loss: 0.089332
Policy train: iteration: 4500, policy_loss: 0.068727
Policy train: iteration: 5000, policy_loss: 0.069711
Policy train: iteration: 5500, policy_loss: 0.095626
Policy train: iteration: 6000, policy_loss: 0.108919
Policy train: iteration: 6500, policy_loss: 0.078690
Policy train: iteration: 7000, policy_loss: 0.082874
Policy train: iteration: 7500, policy_loss: 0.081501
Policy train: iteration: 8000, policy_loss: 0.068488
Policy train: iteration: 8500, policy_loss: 0.081020
Policy train: iteration: 9000, policy_loss: 0.110742

Background Trial: 1, reward: -718.4540007628601
Background Trial: 2, reward: -276.37486174093567
Background Trial: 3, reward: -98.25804300633824
Background Trial: 4, reward: -635.201795213378
Background Trial: 5, reward: -106.5651678412151
Background Trial: 6, reward: -471.33347484963576
Background Trial: 7, reward: -40.857936936562126
Background Trial: 8, reward: -385.34970068233804
Background Trial: 9, reward: -375.5255242621007
Iteration: 45, average_reward: -345.3245005883738

Policy train: iteration: 500, policy_loss: 0.078692
Policy train: iteration: 1000, policy_loss: 0.061437
Policy train: iteration: 1500, policy_loss: 0.097576
Policy train: iteration: 2000, policy_loss: 0.100325
Policy train: iteration: 2500, policy_loss: 0.138370
Policy train: iteration: 3000, policy_loss: 0.113598
Policy train: iteration: 3500, policy_loss: 0.086532
Policy train: iteration: 4000, policy_loss: 0.089083
Policy train: iteration: 4500, policy_loss: 0.103195
Policy train: iteration: 5000, policy_loss: 0.111595
Policy train: iteration: 5500, policy_loss: 0.078723
Policy train: iteration: 6000, policy_loss: 0.091251
Policy train: iteration: 6500, policy_loss: 0.085994
Policy train: iteration: 7000, policy_loss: 0.132956
Policy train: iteration: 7500, policy_loss: 0.083379
Policy train: iteration: 8000, policy_loss: 0.086589
Policy train: iteration: 8500, policy_loss: 0.113223
Policy train: iteration: 9000, policy_loss: 0.082301

Background Trial: 1, reward: -322.40878194931935
Background Trial: 2, reward: -349.4989320841223
Background Trial: 3, reward: -107.6434902109645
Background Trial: 4, reward: -707.9969268034473
Background Trial: 5, reward: -74.19561259392607
Background Trial: 6, reward: -59.104840293649765
Background Trial: 7, reward: -318.9105398325287
Background Trial: 8, reward: -368.8332440228755
Background Trial: 9, reward: -645.0462978783842
Iteration: 46, average_reward: -328.1820739632464

Policy train: iteration: 500, policy_loss: 0.067129
Policy train: iteration: 1000, policy_loss: 0.107054
Policy train: iteration: 1500, policy_loss: 0.093694
Policy train: iteration: 2000, policy_loss: 0.066453
Policy train: iteration: 2500, policy_loss: 0.095258
Policy train: iteration: 3000, policy_loss: 0.093364
Policy train: iteration: 3500, policy_loss: 0.100117
Policy train: iteration: 4000, policy_loss: 0.095627
Policy train: iteration: 4500, policy_loss: 0.120755
Policy train: iteration: 5000, policy_loss: 0.057704
Policy train: iteration: 5500, policy_loss: 0.072602
Policy train: iteration: 6000, policy_loss: 0.154809
Policy train: iteration: 6500, policy_loss: 0.114676
Policy train: iteration: 7000, policy_loss: 0.122464
Policy train: iteration: 7500, policy_loss: 0.084338
Policy train: iteration: 8000, policy_loss: 0.115950
Policy train: iteration: 8500, policy_loss: 0.100780
Policy train: iteration: 9000, policy_loss: 0.080608

Background Trial: 1, reward: -352.9139972658961
Background Trial: 2, reward: -49.38678921074471
Background Trial: 3, reward: -358.6315733845029
Background Trial: 4, reward: -66.73035938163243
Background Trial: 5, reward: -82.58277299451875
Background Trial: 6, reward: -613.3387957683755
Background Trial: 7, reward: -195.40868089370878
Background Trial: 8, reward: -269.8577783699601
Background Trial: 9, reward: -60.38613943418454
Iteration: 47, average_reward: -227.69298741150266

Policy train: iteration: 500, policy_loss: 0.083852
Policy train: iteration: 1000, policy_loss: 0.121691
Policy train: iteration: 1500, policy_loss: 0.103392
Policy train: iteration: 2000, policy_loss: 0.098657
Policy train: iteration: 2500, policy_loss: 0.122194
Policy train: iteration: 3000, policy_loss: 0.125771
Policy train: iteration: 3500, policy_loss: 0.101308
Policy train: iteration: 4000, policy_loss: 0.088707
Policy train: iteration: 4500, policy_loss: 0.088314
Policy train: iteration: 5000, policy_loss: 0.126468
Policy train: iteration: 5500, policy_loss: 0.108168
Policy train: iteration: 6000, policy_loss: 0.089527
Policy train: iteration: 6500, policy_loss: 0.080774
Policy train: iteration: 7000, policy_loss: 0.070619
Policy train: iteration: 7500, policy_loss: 0.083251
Policy train: iteration: 8000, policy_loss: 0.097345
Policy train: iteration: 8500, policy_loss: 0.115751
Policy train: iteration: 9000, policy_loss: 0.060931

Background Trial: 1, reward: -301.07708584880993
Background Trial: 2, reward: -211.96694397572452
Background Trial: 3, reward: -446.06010855835115
Background Trial: 4, reward: 23.420529456415508
Background Trial: 5, reward: -507.9381025131273
Background Trial: 6, reward: -103.78639294642099
Background Trial: 7, reward: 258.1412284536933
Background Trial: 8, reward: -380.24728498929676
Background Trial: 9, reward: -390.694379992201
Iteration: 48, average_reward: -228.91206010153584

Policy train: iteration: 500, policy_loss: 0.104877
Policy train: iteration: 1000, policy_loss: 0.092407
Policy train: iteration: 1500, policy_loss: 0.099511
Policy train: iteration: 2000, policy_loss: 0.097080
Policy train: iteration: 2500, policy_loss: 0.096610
Policy train: iteration: 3000, policy_loss: 0.085414
Policy train: iteration: 3500, policy_loss: 0.082034
Policy train: iteration: 4000, policy_loss: 0.106778
Policy train: iteration: 4500, policy_loss: 0.058496
Policy train: iteration: 5000, policy_loss: 0.058225
Policy train: iteration: 5500, policy_loss: 0.079131
Policy train: iteration: 6000, policy_loss: 0.093512
Policy train: iteration: 6500, policy_loss: 0.149488
Policy train: iteration: 7000, policy_loss: 0.085644
Policy train: iteration: 7500, policy_loss: 0.099971
Policy train: iteration: 8000, policy_loss: 0.095751
Policy train: iteration: 8500, policy_loss: 0.124498
Policy train: iteration: 9000, policy_loss: 0.107189

Background Trial: 1, reward: -273.6541782278184
Background Trial: 2, reward: -79.85895424154849
Background Trial: 3, reward: -410.34318762794675
Background Trial: 4, reward: -8.913357603883355
Background Trial: 5, reward: -357.08136045051987
Background Trial: 6, reward: -703.1076675111849
Background Trial: 7, reward: -43.33648288128791
Background Trial: 8, reward: -322.76092269510417
Background Trial: 9, reward: -28.678972805202307
Iteration: 49, average_reward: -247.52612044938846

Policy train: iteration: 500, policy_loss: 0.079918
Policy train: iteration: 1000, policy_loss: 0.074343
Policy train: iteration: 1500, policy_loss: 0.085275
Policy train: iteration: 2000, policy_loss: 0.083948
Policy train: iteration: 2500, policy_loss: 0.091471
Policy train: iteration: 3000, policy_loss: 0.073540
Policy train: iteration: 3500, policy_loss: 0.087261
Policy train: iteration: 4000, policy_loss: 0.109898
Policy train: iteration: 4500, policy_loss: 0.078455
Policy train: iteration: 5000, policy_loss: 0.119322
Policy train: iteration: 5500, policy_loss: 0.078161
Policy train: iteration: 6000, policy_loss: 0.098695
Policy train: iteration: 6500, policy_loss: 0.084154
Policy train: iteration: 7000, policy_loss: 0.079736
Policy train: iteration: 7500, policy_loss: 0.082274
Policy train: iteration: 8000, policy_loss: 0.100886
Policy train: iteration: 8500, policy_loss: 0.068221
Policy train: iteration: 9000, policy_loss: 0.100316

Background Trial: 1, reward: -287.5029190455738
Background Trial: 2, reward: -585.5210979302165
Background Trial: 3, reward: -468.5380151817852
Background Trial: 4, reward: -5.735985352792142
Background Trial: 5, reward: -455.0754894126992
Background Trial: 6, reward: -331.5579201007954
Background Trial: 7, reward: -238.39699846501838
Background Trial: 8, reward: -115.69099472320615
Background Trial: 9, reward: -42.58693837774109
Iteration: 50, average_reward: -281.17848428775864

Policy train: iteration: 500, policy_loss: 0.091147
Policy train: iteration: 1000, policy_loss: 0.082337
Policy train: iteration: 1500, policy_loss: 0.106250
Policy train: iteration: 2000, policy_loss: 0.065116
Policy train: iteration: 2500, policy_loss: 0.066204
Policy train: iteration: 3000, policy_loss: 0.113940
Policy train: iteration: 3500, policy_loss: 0.084222
Policy train: iteration: 4000, policy_loss: 0.083735
Policy train: iteration: 4500, policy_loss: 0.091595
Policy train: iteration: 5000, policy_loss: 0.114838
Policy train: iteration: 5500, policy_loss: 0.068322
Policy train: iteration: 6000, policy_loss: 0.059736
Policy train: iteration: 6500, policy_loss: 0.099710
Policy train: iteration: 7000, policy_loss: 0.103794
Policy train: iteration: 7500, policy_loss: 0.094843
Policy train: iteration: 8000, policy_loss: 0.072808
Policy train: iteration: 8500, policy_loss: 0.064318
Policy train: iteration: 9000, policy_loss: 0.058498

Background Trial: 1, reward: -246.18413223641187
Background Trial: 2, reward: -345.12066992435206
Background Trial: 3, reward: -12.076410163745095
Background Trial: 4, reward: -59.916636009535054
Background Trial: 5, reward: -328.0166604989399
Background Trial: 6, reward: -302.53002490234894
Background Trial: 7, reward: -97.61351351297874
Background Trial: 8, reward: -273.35139594685904
Background Trial: 9, reward: -80.5194466537186
Iteration: 51, average_reward: -193.92543220543217

Policy train: iteration: 500, policy_loss: 0.068464
Policy train: iteration: 1000, policy_loss: 0.096974
Policy train: iteration: 1500, policy_loss: 0.116892
Policy train: iteration: 2000, policy_loss: 0.083963
Policy train: iteration: 2500, policy_loss: 0.099135
Policy train: iteration: 3000, policy_loss: 0.095168
Policy train: iteration: 3500, policy_loss: 0.073584
Policy train: iteration: 4000, policy_loss: 0.091388
Policy train: iteration: 4500, policy_loss: 0.084198
Policy train: iteration: 5000, policy_loss: 0.121020
Policy train: iteration: 5500, policy_loss: 0.103033
Policy train: iteration: 6000, policy_loss: 0.099804
Policy train: iteration: 6500, policy_loss: 0.052735
Policy train: iteration: 7000, policy_loss: 0.081372
Policy train: iteration: 7500, policy_loss: 0.080083
Policy train: iteration: 8000, policy_loss: 0.092104
Policy train: iteration: 8500, policy_loss: 0.060377
Policy train: iteration: 9000, policy_loss: 0.104538

Background Trial: 1, reward: -127.56885257368954
Background Trial: 2, reward: -68.34356648178972
Background Trial: 3, reward: -52.06005447604393
Background Trial: 4, reward: -391.2611672549196
Background Trial: 5, reward: -259.9687255659913
Background Trial: 6, reward: -568.230008043331
Background Trial: 7, reward: -241.8110614435202
Background Trial: 8, reward: -189.9691397004891
Background Trial: 9, reward: -303.8433421139208
Iteration: 52, average_reward: -244.78399085041053

Policy train: iteration: 500, policy_loss: 0.080650
Policy train: iteration: 1000, policy_loss: 0.076694
Policy train: iteration: 1500, policy_loss: 0.075025
Policy train: iteration: 2000, policy_loss: 0.090674
Policy train: iteration: 2500, policy_loss: 0.078035
Policy train: iteration: 3000, policy_loss: 0.107516
Policy train: iteration: 3500, policy_loss: 0.085849
Policy train: iteration: 4000, policy_loss: 0.073397
Policy train: iteration: 4500, policy_loss: 0.061892
Policy train: iteration: 5000, policy_loss: 0.081610
Policy train: iteration: 5500, policy_loss: 0.084121
Policy train: iteration: 6000, policy_loss: 0.070829
Policy train: iteration: 6500, policy_loss: 0.079989
Policy train: iteration: 7000, policy_loss: 0.105328
Policy train: iteration: 7500, policy_loss: 0.085827
Policy train: iteration: 8000, policy_loss: 0.122314
Policy train: iteration: 8500, policy_loss: 0.098278
Policy train: iteration: 9000, policy_loss: 0.083925

Background Trial: 1, reward: -123.33441422738893
Background Trial: 2, reward: -386.2837103724767
Background Trial: 3, reward: -580.8381082217475
Background Trial: 4, reward: -369.1813605943044
Background Trial: 5, reward: -340.71019886362626
Background Trial: 6, reward: -25.346548600261485
Background Trial: 7, reward: -199.19018669019613
Background Trial: 8, reward: -64.17554148646369
Background Trial: 9, reward: -99.23433292800372
Iteration: 53, average_reward: -243.14382244271877

Policy train: iteration: 500, policy_loss: 0.101005
Policy train: iteration: 1000, policy_loss: 0.088878
Policy train: iteration: 1500, policy_loss: 0.074160
Policy train: iteration: 2000, policy_loss: 0.111868
Policy train: iteration: 2500, policy_loss: 0.085768
Policy train: iteration: 3000, policy_loss: 0.128369
Policy train: iteration: 3500, policy_loss: 0.109932
Policy train: iteration: 4000, policy_loss: 0.076198
Policy train: iteration: 4500, policy_loss: 0.089436
Policy train: iteration: 5000, policy_loss: 0.062115
Policy train: iteration: 5500, policy_loss: 0.107471
Policy train: iteration: 6000, policy_loss: 0.073668
Policy train: iteration: 6500, policy_loss: 0.106641
Policy train: iteration: 7000, policy_loss: 0.138388
Policy train: iteration: 7500, policy_loss: 0.081631
Policy train: iteration: 8000, policy_loss: 0.088852
Policy train: iteration: 8500, policy_loss: 0.124169
Policy train: iteration: 9000, policy_loss: 0.092875

Background Trial: 1, reward: -418.2991284643661
Background Trial: 2, reward: -684.7053143874948
Background Trial: 3, reward: -19.428994291154524
Background Trial: 4, reward: -677.3639786450559
Background Trial: 5, reward: -307.59001804543846
Background Trial: 6, reward: -292.8994093326761
Background Trial: 7, reward: -611.7612309320557
Background Trial: 8, reward: -375.64013427600145
Background Trial: 9, reward: -80.03429502632014
Iteration: 54, average_reward: -385.3025003778404

Policy train: iteration: 500, policy_loss: 0.111159
Policy train: iteration: 1000, policy_loss: 0.079961
Policy train: iteration: 1500, policy_loss: 0.051459
Policy train: iteration: 2000, policy_loss: 0.109953
Policy train: iteration: 2500, policy_loss: 0.077928
Policy train: iteration: 3000, policy_loss: 0.070584
Policy train: iteration: 3500, policy_loss: 0.087459
Policy train: iteration: 4000, policy_loss: 0.070625
Policy train: iteration: 4500, policy_loss: 0.108055
Policy train: iteration: 5000, policy_loss: 0.077169
Policy train: iteration: 5500, policy_loss: 0.099978
Policy train: iteration: 6000, policy_loss: 0.110583
Policy train: iteration: 6500, policy_loss: 0.084054
Policy train: iteration: 7000, policy_loss: 0.087740
Policy train: iteration: 7500, policy_loss: 0.114632
Policy train: iteration: 8000, policy_loss: 0.077999
Policy train: iteration: 8500, policy_loss: 0.075159
Policy train: iteration: 9000, policy_loss: 0.080797

Background Trial: 1, reward: -541.7912611735915
Background Trial: 2, reward: -286.3812124533452
Background Trial: 3, reward: -369.3005854486312
Background Trial: 4, reward: -2.5434520272554835
Background Trial: 5, reward: -53.09885717190038
Background Trial: 6, reward: -330.4367162562439
Background Trial: 7, reward: -435.6660952564063
Background Trial: 8, reward: -723.1548320425317
Background Trial: 9, reward: 9.638579314176724
Iteration: 55, average_reward: -303.6371591684143

Policy train: iteration: 500, policy_loss: 0.076117
Policy train: iteration: 1000, policy_loss: 0.077628
Policy train: iteration: 1500, policy_loss: 0.062122
Policy train: iteration: 2000, policy_loss: 0.087865
Policy train: iteration: 2500, policy_loss: 0.099471
Policy train: iteration: 3000, policy_loss: 0.103669
Policy train: iteration: 3500, policy_loss: 0.088068
Policy train: iteration: 4000, policy_loss: 0.084740
Policy train: iteration: 4500, policy_loss: 0.096459
Policy train: iteration: 5000, policy_loss: 0.085624
Policy train: iteration: 5500, policy_loss: 0.069524
Policy train: iteration: 6000, policy_loss: 0.084122
Policy train: iteration: 6500, policy_loss: 0.081318
Policy train: iteration: 7000, policy_loss: 0.073129
Policy train: iteration: 7500, policy_loss: 0.089320
Policy train: iteration: 8000, policy_loss: 0.092272
Policy train: iteration: 8500, policy_loss: 0.104302
Policy train: iteration: 9000, policy_loss: 0.101411

Background Trial: 1, reward: -87.77634548665525
Background Trial: 2, reward: -450.5725638434729
Background Trial: 3, reward: -405.75595065942514
Background Trial: 4, reward: -625.9036936439217
Background Trial: 5, reward: -348.05744782586
Background Trial: 6, reward: 35.0630102328561
Background Trial: 7, reward: -325.9118139252082
Background Trial: 8, reward: -142.00727628848801
Background Trial: 9, reward: -39.48276833005137
Iteration: 56, average_reward: -265.60053886335845

Policy train: iteration: 500, policy_loss: 0.080266
Policy train: iteration: 1000, policy_loss: 0.094725
Policy train: iteration: 1500, policy_loss: 0.062596
Policy train: iteration: 2000, policy_loss: 0.121006
Policy train: iteration: 2500, policy_loss: 0.099170
Policy train: iteration: 3000, policy_loss: 0.091036
Policy train: iteration: 3500, policy_loss: 0.067403
Policy train: iteration: 4000, policy_loss: 0.089177
Policy train: iteration: 4500, policy_loss: 0.123815
Policy train: iteration: 5000, policy_loss: 0.054109
Policy train: iteration: 5500, policy_loss: 0.095881
Policy train: iteration: 6000, policy_loss: 0.099368
Policy train: iteration: 6500, policy_loss: 0.069058
Policy train: iteration: 7000, policy_loss: 0.072471
Policy train: iteration: 7500, policy_loss: 0.102579
Policy train: iteration: 8000, policy_loss: 0.081646
Policy train: iteration: 8500, policy_loss: 0.056988
Policy train: iteration: 9000, policy_loss: 0.088329

Background Trial: 1, reward: -400.5239940703486
Background Trial: 2, reward: -319.5648519024304
Background Trial: 3, reward: -21.272258143092003
Background Trial: 4, reward: -477.4249988939336
Background Trial: 5, reward: -372.4191240221731
Background Trial: 6, reward: -379.3861959504061
Background Trial: 7, reward: -18.688992637911085
Background Trial: 8, reward: 31.305475077066802
Background Trial: 9, reward: -10.485757585550388
Iteration: 57, average_reward: -218.71785534764203

Policy train: iteration: 500, policy_loss: 0.100427
Policy train: iteration: 1000, policy_loss: 0.064760
Policy train: iteration: 1500, policy_loss: 0.118701
Policy train: iteration: 2000, policy_loss: 0.077237
Policy train: iteration: 2500, policy_loss: 0.084212
Policy train: iteration: 3000, policy_loss: 0.066307
Policy train: iteration: 3500, policy_loss: 0.067957
Policy train: iteration: 4000, policy_loss: 0.058004
Policy train: iteration: 4500, policy_loss: 0.093558
Policy train: iteration: 5000, policy_loss: 0.074534
Policy train: iteration: 5500, policy_loss: 0.120437
Policy train: iteration: 6000, policy_loss: 0.105451
Policy train: iteration: 6500, policy_loss: 0.123062
Policy train: iteration: 7000, policy_loss: 0.092557
Policy train: iteration: 7500, policy_loss: 0.109886
Policy train: iteration: 8000, policy_loss: 0.102318
Policy train: iteration: 8500, policy_loss: 0.056502
Policy train: iteration: 9000, policy_loss: 0.102229

Background Trial: 1, reward: -344.0270806665819
Background Trial: 2, reward: -340.3755642257365
Background Trial: 3, reward: -357.7983107718523
Background Trial: 4, reward: -39.78022046244247
Background Trial: 5, reward: -116.39877647490977
Background Trial: 6, reward: -76.32110764561418
Background Trial: 7, reward: -397.8570359222413
Background Trial: 8, reward: -66.98374865134039
Background Trial: 9, reward: -13.342366939964236
Iteration: 58, average_reward: -194.76491241785368

Policy train: iteration: 500, policy_loss: 0.109173
Policy train: iteration: 1000, policy_loss: 0.080940
Policy train: iteration: 1500, policy_loss: 0.096013
Policy train: iteration: 2000, policy_loss: 0.079743
Policy train: iteration: 2500, policy_loss: 0.080943
Policy train: iteration: 3000, policy_loss: 0.063699
Policy train: iteration: 3500, policy_loss: 0.096136
Policy train: iteration: 4000, policy_loss: 0.065406
Policy train: iteration: 4500, policy_loss: 0.070864
Policy train: iteration: 5000, policy_loss: 0.093590
Policy train: iteration: 5500, policy_loss: 0.071322
Policy train: iteration: 6000, policy_loss: 0.091395
Policy train: iteration: 6500, policy_loss: 0.087812
Policy train: iteration: 7000, policy_loss: 0.102704
Policy train: iteration: 7500, policy_loss: 0.063212
Policy train: iteration: 8000, policy_loss: 0.094317
Policy train: iteration: 8500, policy_loss: 0.079123
Policy train: iteration: 9000, policy_loss: 0.073925

Background Trial: 1, reward: -573.5365996131367
Background Trial: 2, reward: -284.04706816601345
Background Trial: 3, reward: -433.8348776874651
Background Trial: 4, reward: -419.3870990651467
Background Trial: 5, reward: -349.0764453016981
Background Trial: 6, reward: -147.67048971853842
Background Trial: 7, reward: -471.19802299068067
Background Trial: 8, reward: -34.75238377297862
Background Trial: 9, reward: -389.3394574038081
Iteration: 59, average_reward: -344.76027152438513

Policy train: iteration: 500, policy_loss: 0.066375
Policy train: iteration: 1000, policy_loss: 0.101783
Policy train: iteration: 1500, policy_loss: 0.062562
Policy train: iteration: 2000, policy_loss: 0.045008
Policy train: iteration: 2500, policy_loss: 0.080506
Policy train: iteration: 3000, policy_loss: 0.071865
Policy train: iteration: 3500, policy_loss: 0.077921
Policy train: iteration: 4000, policy_loss: 0.098091
Policy train: iteration: 4500, policy_loss: 0.082526
Policy train: iteration: 5000, policy_loss: 0.098515
Policy train: iteration: 5500, policy_loss: 0.082637
Policy train: iteration: 6000, policy_loss: 0.062773
Policy train: iteration: 6500, policy_loss: 0.076879
Policy train: iteration: 7000, policy_loss: 0.094694
Policy train: iteration: 7500, policy_loss: 0.067095
Policy train: iteration: 8000, policy_loss: 0.051334
Policy train: iteration: 8500, policy_loss: 0.141334
Policy train: iteration: 9000, policy_loss: 0.097985

Background Trial: 1, reward: -367.476831640463
Background Trial: 2, reward: -100.67837639183747
Background Trial: 3, reward: -390.8239038112336
Background Trial: 4, reward: -442.6811855199549
Background Trial: 5, reward: -403.57296420259456
Background Trial: 6, reward: -263.7543823291945
Background Trial: 7, reward: -331.14656019424586
Background Trial: 8, reward: -74.36568080764067
Background Trial: 9, reward: -80.62379126464481
Iteration: 60, average_reward: -272.79151957353434

Policy train: iteration: 500, policy_loss: 0.067346
Policy train: iteration: 1000, policy_loss: 0.095455
Policy train: iteration: 1500, policy_loss: 0.053572
Policy train: iteration: 2000, policy_loss: 0.100479
Policy train: iteration: 2500, policy_loss: 0.063635
Policy train: iteration: 3000, policy_loss: 0.082884
Policy train: iteration: 3500, policy_loss: 0.124267
Policy train: iteration: 4000, policy_loss: 0.072077
Policy train: iteration: 4500, policy_loss: 0.089381
Policy train: iteration: 5000, policy_loss: 0.080387
Policy train: iteration: 5500, policy_loss: 0.089089
Policy train: iteration: 6000, policy_loss: 0.131497
Policy train: iteration: 6500, policy_loss: 0.061407
Policy train: iteration: 7000, policy_loss: 0.135079
Policy train: iteration: 7500, policy_loss: 0.080248
Policy train: iteration: 8000, policy_loss: 0.091218
Policy train: iteration: 8500, policy_loss: 0.061978
Policy train: iteration: 9000, policy_loss: 0.107864

Background Trial: 1, reward: -823.2797577446286
Background Trial: 2, reward: -371.55893796977165
Background Trial: 3, reward: -7.596284989922353
Background Trial: 4, reward: -341.18720229081157
Background Trial: 5, reward: -394.25079300674645
Background Trial: 6, reward: -428.50498969126124
Background Trial: 7, reward: -337.43207015248237
Background Trial: 8, reward: 13.544588603000136
Background Trial: 9, reward: -344.6014909976047
Iteration: 61, average_reward: -337.2074375822477

Policy train: iteration: 500, policy_loss: 0.102667
Policy train: iteration: 1000, policy_loss: 0.100622
Policy train: iteration: 1500, policy_loss: 0.078186
Policy train: iteration: 2000, policy_loss: 0.087280
Policy train: iteration: 2500, policy_loss: 0.103728
Policy train: iteration: 3000, policy_loss: 0.114945
Policy train: iteration: 3500, policy_loss: 0.077637
Policy train: iteration: 4000, policy_loss: 0.093793
Policy train: iteration: 4500, policy_loss: 0.075595
Policy train: iteration: 5000, policy_loss: 0.106871
Policy train: iteration: 5500, policy_loss: 0.090839
Policy train: iteration: 6000, policy_loss: 0.086385
Policy train: iteration: 6500, policy_loss: 0.088413
Policy train: iteration: 7000, policy_loss: 0.135857
Policy train: iteration: 7500, policy_loss: 0.056718
Policy train: iteration: 8000, policy_loss: 0.093501
Policy train: iteration: 8500, policy_loss: 0.089150
Policy train: iteration: 9000, policy_loss: 0.086539

Background Trial: 1, reward: -307.24524619574515
Background Trial: 2, reward: -309.88680579576123
Background Trial: 3, reward: -100.00607753895798
Background Trial: 4, reward: -381.8879700303071
Background Trial: 5, reward: -371.7216975587296
Background Trial: 6, reward: -332.02002509366747
Background Trial: 7, reward: -301.96984089356283
Background Trial: 8, reward: -438.4502890579792
Background Trial: 9, reward: -485.67707489039464
Iteration: 62, average_reward: -336.5405585616784

Policy train: iteration: 500, policy_loss: 0.130985
Policy train: iteration: 1000, policy_loss: 0.093098
Policy train: iteration: 1500, policy_loss: 0.101434
Policy train: iteration: 2000, policy_loss: 0.065439
Policy train: iteration: 2500, policy_loss: 0.073016
Policy train: iteration: 3000, policy_loss: 0.076421
Policy train: iteration: 3500, policy_loss: 0.093637
Policy train: iteration: 4000, policy_loss: 0.110910
Policy train: iteration: 4500, policy_loss: 0.082000
Policy train: iteration: 5000, policy_loss: 0.075247
Policy train: iteration: 5500, policy_loss: 0.103244
Policy train: iteration: 6000, policy_loss: 0.130269
Policy train: iteration: 6500, policy_loss: 0.071451
Policy train: iteration: 7000, policy_loss: 0.050944
Policy train: iteration: 7500, policy_loss: 0.089523
Policy train: iteration: 8000, policy_loss: 0.070895
Policy train: iteration: 8500, policy_loss: 0.067988
Policy train: iteration: 9000, policy_loss: 0.079102

Background Trial: 1, reward: -32.7125865812273
Background Trial: 2, reward: -497.5534103691314
Background Trial: 3, reward: -44.02859609689945
Background Trial: 4, reward: -119.45957924186006
Background Trial: 5, reward: -34.98943183552694
Background Trial: 6, reward: -58.161293900807266
Background Trial: 7, reward: -53.52029378982047
Background Trial: 8, reward: -605.4482455157743
Background Trial: 9, reward: -55.97336464434718
Iteration: 63, average_reward: -166.87186688615492

Policy train: iteration: 500, policy_loss: 0.111747
Policy train: iteration: 1000, policy_loss: 0.059956
Policy train: iteration: 1500, policy_loss: 0.072822
Policy train: iteration: 2000, policy_loss: 0.061131
Policy train: iteration: 2500, policy_loss: 0.079021
Policy train: iteration: 3000, policy_loss: 0.098762
Policy train: iteration: 3500, policy_loss: 0.081915
Policy train: iteration: 4000, policy_loss: 0.080465
Policy train: iteration: 4500, policy_loss: 0.092099
Policy train: iteration: 5000, policy_loss: 0.094477
Policy train: iteration: 5500, policy_loss: 0.092532
Policy train: iteration: 6000, policy_loss: 0.086145
Policy train: iteration: 6500, policy_loss: 0.058786
Policy train: iteration: 7000, policy_loss: 0.119800
Policy train: iteration: 7500, policy_loss: 0.114165
Policy train: iteration: 8000, policy_loss: 0.069698
Policy train: iteration: 8500, policy_loss: 0.110729
Policy train: iteration: 9000, policy_loss: 0.102739

Background Trial: 1, reward: -341.30703893086473
Background Trial: 2, reward: -513.8759084084594
Background Trial: 3, reward: -766.5868529070978
Background Trial: 4, reward: -442.8310441691731
Background Trial: 5, reward: -480.98506920282887
Background Trial: 6, reward: -296.069635290553
Background Trial: 7, reward: -317.9724998546941
Background Trial: 8, reward: -466.1054860095267
Background Trial: 9, reward: -300.37685124519356
Iteration: 64, average_reward: -436.2344873353768

Policy train: iteration: 500, policy_loss: 0.095810
Policy train: iteration: 1000, policy_loss: 0.089750
Policy train: iteration: 1500, policy_loss: 0.084946
Policy train: iteration: 2000, policy_loss: 0.065766
Policy train: iteration: 2500, policy_loss: 0.174615
Policy train: iteration: 3000, policy_loss: 0.102644
Policy train: iteration: 3500, policy_loss: 0.080987
Policy train: iteration: 4000, policy_loss: 0.100399
Policy train: iteration: 4500, policy_loss: 0.077515
Policy train: iteration: 5000, policy_loss: 0.106401
Policy train: iteration: 5500, policy_loss: 0.091556
Policy train: iteration: 6000, policy_loss: 0.108050
Policy train: iteration: 6500, policy_loss: 0.077937
Policy train: iteration: 7000, policy_loss: 0.066415
Policy train: iteration: 7500, policy_loss: 0.083181
Policy train: iteration: 8000, policy_loss: 0.063557
Policy train: iteration: 8500, policy_loss: 0.106052
Policy train: iteration: 9000, policy_loss: 0.054386

Background Trial: 1, reward: -211.9397192742806
Background Trial: 2, reward: -36.09091490626744
Background Trial: 3, reward: -417.55348819138675
Background Trial: 4, reward: -422.85084933660175
Background Trial: 5, reward: -374.4205168825699
Background Trial: 6, reward: -45.54902771053458
Background Trial: 7, reward: -470.8142855306908
Background Trial: 8, reward: -378.1127622521098
Background Trial: 9, reward: -336.68135999298954
Iteration: 65, average_reward: -299.3347693419368

Policy train: iteration: 500, policy_loss: 0.074410
Policy train: iteration: 1000, policy_loss: 0.055583
Policy train: iteration: 1500, policy_loss: 0.099771
Policy train: iteration: 2000, policy_loss: 0.055782
Policy train: iteration: 2500, policy_loss: 0.094486
Policy train: iteration: 3000, policy_loss: 0.100477
Policy train: iteration: 3500, policy_loss: 0.096947
Policy train: iteration: 4000, policy_loss: 0.064758
Policy train: iteration: 4500, policy_loss: 0.089877
Policy train: iteration: 5000, policy_loss: 0.091994
Policy train: iteration: 5500, policy_loss: 0.116061
Policy train: iteration: 6000, policy_loss: 0.068420
Policy train: iteration: 6500, policy_loss: 0.076347
Policy train: iteration: 7000, policy_loss: 0.063567
Policy train: iteration: 7500, policy_loss: 0.098023
Policy train: iteration: 8000, policy_loss: 0.073931
Policy train: iteration: 8500, policy_loss: 0.080132
Policy train: iteration: 9000, policy_loss: 0.081887

Background Trial: 1, reward: -580.3432035615959
Background Trial: 2, reward: -396.10775030453266
Background Trial: 3, reward: -308.4116267232373
Background Trial: 4, reward: -345.0043294479857
Background Trial: 5, reward: -366.44427127947745
Background Trial: 6, reward: -263.91567990896925
Background Trial: 7, reward: -79.20874717128243
Background Trial: 8, reward: -70.58277432808454
Background Trial: 9, reward: -18.546206860698305
Iteration: 66, average_reward: -269.8405099539849

Policy train: iteration: 500, policy_loss: 0.074756
Policy train: iteration: 1000, policy_loss: 0.075745
Policy train: iteration: 1500, policy_loss: 0.091680
Policy train: iteration: 2000, policy_loss: 0.111678
Policy train: iteration: 2500, policy_loss: 0.084025
Policy train: iteration: 3000, policy_loss: 0.062903
Policy train: iteration: 3500, policy_loss: 0.095976
Policy train: iteration: 4000, policy_loss: 0.091857
Policy train: iteration: 4500, policy_loss: 0.093039
Policy train: iteration: 5000, policy_loss: 0.067576
Policy train: iteration: 5500, policy_loss: 0.058364
Policy train: iteration: 6000, policy_loss: 0.084704
Policy train: iteration: 6500, policy_loss: 0.082995
Policy train: iteration: 7000, policy_loss: 0.126098
Policy train: iteration: 7500, policy_loss: 0.083334
Policy train: iteration: 8000, policy_loss: 0.102256
Policy train: iteration: 8500, policy_loss: 0.063514
Policy train: iteration: 9000, policy_loss: 0.100045

Background Trial: 1, reward: -483.1372755153117
Background Trial: 2, reward: -129.27971380121198
Background Trial: 3, reward: -498.69168739010934
Background Trial: 4, reward: -99.70903440246492
Background Trial: 5, reward: -407.8494786756196
Background Trial: 6, reward: -24.0450668468185
Background Trial: 7, reward: -468.2995793687946
Background Trial: 8, reward: -468.2072512015926
Background Trial: 9, reward: -403.1939720954428
Iteration: 67, average_reward: -331.37922881081846

Policy train: iteration: 500, policy_loss: 0.077960
Policy train: iteration: 1000, policy_loss: 0.103267
Policy train: iteration: 1500, policy_loss: 0.080657
Policy train: iteration: 2000, policy_loss: 0.087485
Policy train: iteration: 2500, policy_loss: 0.059105
Policy train: iteration: 3000, policy_loss: 0.102235
Policy train: iteration: 3500, policy_loss: 0.090723
Policy train: iteration: 4000, policy_loss: 0.088252
Policy train: iteration: 4500, policy_loss: 0.105178
Policy train: iteration: 5000, policy_loss: 0.080450
Policy train: iteration: 5500, policy_loss: 0.094110
Policy train: iteration: 6000, policy_loss: 0.058842
Policy train: iteration: 6500, policy_loss: 0.049849
Policy train: iteration: 7000, policy_loss: 0.100277
Policy train: iteration: 7500, policy_loss: 0.095305
Policy train: iteration: 8000, policy_loss: 0.088071
Policy train: iteration: 8500, policy_loss: 0.064708
Policy train: iteration: 9000, policy_loss: 0.081778

Background Trial: 1, reward: -510.5654688423574
Background Trial: 2, reward: -16.736119446390944
Background Trial: 3, reward: -549.9267966745965
Background Trial: 4, reward: -444.1793210789943
Background Trial: 5, reward: 15.555917445680635
Background Trial: 6, reward: -404.97806831968666
Background Trial: 7, reward: -275.91999063470087
Background Trial: 8, reward: -89.95892813215897
Background Trial: 9, reward: -60.6942775485343
Iteration: 68, average_reward: -259.71145035908216

Policy train: iteration: 500, policy_loss: 0.075239
Policy train: iteration: 1000, policy_loss: 0.108520
Policy train: iteration: 1500, policy_loss: 0.079160
Policy train: iteration: 2000, policy_loss: 0.111950
Policy train: iteration: 2500, policy_loss: 0.092854
Policy train: iteration: 3000, policy_loss: 0.127962
Policy train: iteration: 3500, policy_loss: 0.121651
Policy train: iteration: 4000, policy_loss: 0.081422
Policy train: iteration: 4500, policy_loss: 0.075996
Policy train: iteration: 5000, policy_loss: 0.080064
Policy train: iteration: 5500, policy_loss: 0.089896
Policy train: iteration: 6000, policy_loss: 0.089932
Policy train: iteration: 6500, policy_loss: 0.114013
Policy train: iteration: 7000, policy_loss: 0.063621
Policy train: iteration: 7500, policy_loss: 0.102741
Policy train: iteration: 8000, policy_loss: 0.061220
Policy train: iteration: 8500, policy_loss: 0.065494
Policy train: iteration: 9000, policy_loss: 0.093677

Background Trial: 1, reward: -436.4335446923471
Background Trial: 2, reward: -268.51270805007493
Background Trial: 3, reward: -301.1256686836947
Background Trial: 4, reward: -474.5922439459143
Background Trial: 5, reward: -488.6567846198092
Background Trial: 6, reward: -88.38625214805953
Background Trial: 7, reward: -65.38868754494578
Background Trial: 8, reward: -274.5260388296005
Background Trial: 9, reward: -235.77140718812097
Iteration: 69, average_reward: -292.59925952250745

Policy train: iteration: 500, policy_loss: 0.082740
Policy train: iteration: 1000, policy_loss: 0.099001
Policy train: iteration: 1500, policy_loss: 0.101228
Policy train: iteration: 2000, policy_loss: 0.057988
Policy train: iteration: 2500, policy_loss: 0.090163
Policy train: iteration: 3000, policy_loss: 0.064502
Policy train: iteration: 3500, policy_loss: 0.054953
Policy train: iteration: 4000, policy_loss: 0.078497
Policy train: iteration: 4500, policy_loss: 0.091157
Policy train: iteration: 5000, policy_loss: 0.057535
Policy train: iteration: 5500, policy_loss: 0.091581
Policy train: iteration: 6000, policy_loss: 0.091666
Policy train: iteration: 6500, policy_loss: 0.079057
Policy train: iteration: 7000, policy_loss: 0.117039
Policy train: iteration: 7500, policy_loss: 0.069000
Policy train: iteration: 8000, policy_loss: 0.074732
Policy train: iteration: 8500, policy_loss: 0.068013
Policy train: iteration: 9000, policy_loss: 0.082629

Background Trial: 1, reward: -21.895961564226056
Background Trial: 2, reward: -46.43831606155413
Background Trial: 3, reward: -95.3236089996653
Background Trial: 4, reward: -523.7197111662324
Background Trial: 5, reward: -385.604996077648
Background Trial: 6, reward: -197.12708609681
Background Trial: 7, reward: -442.48172385485753
Background Trial: 8, reward: 32.366016092292114
Background Trial: 9, reward: -321.21080215950843
Iteration: 70, average_reward: -222.38179887646777

Policy train: iteration: 500, policy_loss: 0.064441
Policy train: iteration: 1000, policy_loss: 0.077964
Policy train: iteration: 1500, policy_loss: 0.084204
Policy train: iteration: 2000, policy_loss: 0.107778
Policy train: iteration: 2500, policy_loss: 0.082652
Policy train: iteration: 3000, policy_loss: 0.122168
Policy train: iteration: 3500, policy_loss: 0.078087
Policy train: iteration: 4000, policy_loss: 0.064806
Policy train: iteration: 4500, policy_loss: 0.047053
Policy train: iteration: 5000, policy_loss: 0.075709
Policy train: iteration: 5500, policy_loss: 0.079156
Policy train: iteration: 6000, policy_loss: 0.077577
Policy train: iteration: 6500, policy_loss: 0.077577
Policy train: iteration: 7000, policy_loss: 0.091551
Policy train: iteration: 7500, policy_loss: 0.091908
Policy train: iteration: 8000, policy_loss: 0.113375
Policy train: iteration: 8500, policy_loss: 0.045515
Policy train: iteration: 9000, policy_loss: 0.063733

Background Trial: 1, reward: -57.7225616225966
Background Trial: 2, reward: -34.070785978877325
Background Trial: 3, reward: -977.7192455965838
Background Trial: 4, reward: -431.27654949666805
Background Trial: 5, reward: -373.09108641135873
Background Trial: 6, reward: -343.1563951500324
Background Trial: 7, reward: -618.6333821689491
Background Trial: 8, reward: -620.8522676551995
Background Trial: 9, reward: -4.204700368921621
Iteration: 71, average_reward: -384.52521938324304

Policy train: iteration: 500, policy_loss: 0.071805
Policy train: iteration: 1000, policy_loss: 0.080567
Policy train: iteration: 1500, policy_loss: 0.085628
Policy train: iteration: 2000, policy_loss: 0.071309
Policy train: iteration: 2500, policy_loss: 0.105046
Policy train: iteration: 3000, policy_loss: 0.098436
Policy train: iteration: 3500, policy_loss: 0.071223
Policy train: iteration: 4000, policy_loss: 0.063374
Policy train: iteration: 4500, policy_loss: 0.114759
Policy train: iteration: 5000, policy_loss: 0.089698
Policy train: iteration: 5500, policy_loss: 0.105215
Policy train: iteration: 6000, policy_loss: 0.097553
Policy train: iteration: 6500, policy_loss: 0.079585
Policy train: iteration: 7000, policy_loss: 0.063142
Policy train: iteration: 7500, policy_loss: 0.071557
Policy train: iteration: 8000, policy_loss: 0.051439
Policy train: iteration: 8500, policy_loss: 0.082092
Policy train: iteration: 9000, policy_loss: 0.068592

Background Trial: 1, reward: 13.297286695928605
Background Trial: 2, reward: -333.6646961488822
Background Trial: 3, reward: -357.1795259661926
Background Trial: 4, reward: -583.7406821299774
Background Trial: 5, reward: -94.09757656601761
Background Trial: 6, reward: -461.6182314618078
Background Trial: 7, reward: -388.2471107858019
Background Trial: 8, reward: -385.4410193839011
Background Trial: 9, reward: -138.15620054412605
Iteration: 72, average_reward: -303.20530625453085

Policy train: iteration: 500, policy_loss: 0.068790
Policy train: iteration: 1000, policy_loss: 0.114514
Policy train: iteration: 1500, policy_loss: 0.059687
Policy train: iteration: 2000, policy_loss: 0.107022
Policy train: iteration: 2500, policy_loss: 0.138587
Policy train: iteration: 3000, policy_loss: 0.066485
Policy train: iteration: 3500, policy_loss: 0.087484
Policy train: iteration: 4000, policy_loss: 0.092409
Policy train: iteration: 4500, policy_loss: 0.074107
Policy train: iteration: 5000, policy_loss: 0.054152
Policy train: iteration: 5500, policy_loss: 0.107692
Policy train: iteration: 6000, policy_loss: 0.130228
Policy train: iteration: 6500, policy_loss: 0.074807
Policy train: iteration: 7000, policy_loss: 0.083980
Policy train: iteration: 7500, policy_loss: 0.115025
Policy train: iteration: 8000, policy_loss: 0.120125
Policy train: iteration: 8500, policy_loss: 0.086279
Policy train: iteration: 9000, policy_loss: 0.076687

Background Trial: 1, reward: -272.08823588380466
Background Trial: 2, reward: -322.89429719942905
Background Trial: 3, reward: -74.5526054483018
Background Trial: 4, reward: -21.926617054467428
Background Trial: 5, reward: -69.57523988898987
Background Trial: 6, reward: -70.01154871116282
Background Trial: 7, reward: -104.86566934993563
Background Trial: 8, reward: -73.62469918237747
Background Trial: 9, reward: -354.2515691250767
Iteration: 73, average_reward: -151.53227576039393

Policy train: iteration: 500, policy_loss: 0.054429
Policy train: iteration: 1000, policy_loss: 0.042905
Policy train: iteration: 1500, policy_loss: 0.084082
Policy train: iteration: 2000, policy_loss: 0.064010
Policy train: iteration: 2500, policy_loss: 0.090696
Policy train: iteration: 3000, policy_loss: 0.080873
Policy train: iteration: 3500, policy_loss: 0.088090
Policy train: iteration: 4000, policy_loss: 0.101891
Policy train: iteration: 4500, policy_loss: 0.090293
Policy train: iteration: 5000, policy_loss: 0.080871
Policy train: iteration: 5500, policy_loss: 0.096432
Policy train: iteration: 6000, policy_loss: 0.062315
Policy train: iteration: 6500, policy_loss: 0.078251
Policy train: iteration: 7000, policy_loss: 0.057485
Policy train: iteration: 7500, policy_loss: 0.072884
Policy train: iteration: 8000, policy_loss: 0.058436
Policy train: iteration: 8500, policy_loss: 0.090088
Policy train: iteration: 9000, policy_loss: 0.145865

Background Trial: 1, reward: -13.017074693121302
Background Trial: 2, reward: -153.22228630775118
Background Trial: 3, reward: -364.36007343010453
Background Trial: 4, reward: 42.60318118371816
Background Trial: 5, reward: -390.38188104737884
Background Trial: 6, reward: -471.2670679691659
Background Trial: 7, reward: -497.09752425461744
Background Trial: 8, reward: -293.61405161139203
Background Trial: 9, reward: -305.6989979460652
Iteration: 74, average_reward: -271.78397511954205

Policy train: iteration: 500, policy_loss: 0.089590
Policy train: iteration: 1000, policy_loss: 0.075757
Policy train: iteration: 1500, policy_loss: 0.071449
Policy train: iteration: 2000, policy_loss: 0.076958
Policy train: iteration: 2500, policy_loss: 0.070758
Policy train: iteration: 3000, policy_loss: 0.093350
Policy train: iteration: 3500, policy_loss: 0.060482
Policy train: iteration: 4000, policy_loss: 0.080136
Policy train: iteration: 4500, policy_loss: 0.109667
Policy train: iteration: 5000, policy_loss: 0.092618
Policy train: iteration: 5500, policy_loss: 0.069237
Policy train: iteration: 6000, policy_loss: 0.074385
Policy train: iteration: 6500, policy_loss: 0.091402
Policy train: iteration: 7000, policy_loss: 0.099224
Policy train: iteration: 7500, policy_loss: 0.074709
Policy train: iteration: 8000, policy_loss: 0.075776
Policy train: iteration: 8500, policy_loss: 0.079965
Policy train: iteration: 9000, policy_loss: 0.075813

Background Trial: 1, reward: -210.5480113331301
Background Trial: 2, reward: -88.78100915756602
Background Trial: 3, reward: -304.020174466772
Background Trial: 4, reward: -544.0936186866301
Background Trial: 5, reward: -683.9972752485478
Background Trial: 6, reward: -343.86528927890606
Background Trial: 7, reward: -202.21631101738183
Background Trial: 8, reward: -268.0829083787063
Background Trial: 9, reward: -94.50460576604031
Iteration: 75, average_reward: -304.45657814818674

Policy train: iteration: 500, policy_loss: 0.086384
Policy train: iteration: 1000, policy_loss: 0.058517
Policy train: iteration: 1500, policy_loss: 0.077872
Policy train: iteration: 2000, policy_loss: 0.053929
Policy train: iteration: 2500, policy_loss: 0.094806
Policy train: iteration: 3000, policy_loss: 0.077713
Policy train: iteration: 3500, policy_loss: 0.095833
Policy train: iteration: 4000, policy_loss: 0.092357
Policy train: iteration: 4500, policy_loss: 0.121965
Policy train: iteration: 5000, policy_loss: 0.098226
Policy train: iteration: 5500, policy_loss: 0.062387
Policy train: iteration: 6000, policy_loss: 0.085630
Policy train: iteration: 6500, policy_loss: 0.080086
Policy train: iteration: 7000, policy_loss: 0.092780
Policy train: iteration: 7500, policy_loss: 0.095463
Policy train: iteration: 8000, policy_loss: 0.036793
Policy train: iteration: 8500, policy_loss: 0.113463
Policy train: iteration: 9000, policy_loss: 0.089381

Background Trial: 1, reward: -72.48885505131513
Background Trial: 2, reward: -560.7010585080636
Background Trial: 3, reward: -29.628857190760343
Background Trial: 4, reward: -233.4090317772751
Background Trial: 5, reward: -29.11118231581503
Background Trial: 6, reward: -29.55938016325166
Background Trial: 7, reward: -392.31669710290663
Background Trial: 8, reward: -75.6495221237767
Background Trial: 9, reward: -221.41164400230923
Iteration: 76, average_reward: -182.69735869283036

Policy train: iteration: 500, policy_loss: 0.089770
Policy train: iteration: 1000, policy_loss: 0.084616
Policy train: iteration: 1500, policy_loss: 0.073960
Policy train: iteration: 2000, policy_loss: 0.061172
Policy train: iteration: 2500, policy_loss: 0.071719
Policy train: iteration: 3000, policy_loss: 0.074608
Policy train: iteration: 3500, policy_loss: 0.113217
Policy train: iteration: 4000, policy_loss: 0.059278
Policy train: iteration: 4500, policy_loss: 0.072238
Policy train: iteration: 5000, policy_loss: 0.101259
Policy train: iteration: 5500, policy_loss: 0.072886
Policy train: iteration: 6000, policy_loss: 0.076966
Policy train: iteration: 6500, policy_loss: 0.062655
Policy train: iteration: 7000, policy_loss: 0.073562
Policy train: iteration: 7500, policy_loss: 0.110328
Policy train: iteration: 8000, policy_loss: 0.082860
Policy train: iteration: 8500, policy_loss: 0.066598
Policy train: iteration: 9000, policy_loss: 0.116069

Background Trial: 1, reward: 40.373757725483756
Background Trial: 2, reward: -358.1076329025659
Background Trial: 3, reward: -96.51262749196732
Background Trial: 4, reward: -41.022108111850045
Background Trial: 5, reward: -685.0597966392671
Background Trial: 6, reward: -258.56052868182115
Background Trial: 7, reward: -92.41726017440229
Background Trial: 8, reward: -572.8284416064532
Background Trial: 9, reward: -271.6034615795144
Iteration: 77, average_reward: -259.5264554958175

Policy train: iteration: 500, policy_loss: 0.072800
Policy train: iteration: 1000, policy_loss: 0.066107
Policy train: iteration: 1500, policy_loss: 0.072488
Policy train: iteration: 2000, policy_loss: 0.079930
Policy train: iteration: 2500, policy_loss: 0.081882
Policy train: iteration: 3000, policy_loss: 0.080876
Policy train: iteration: 3500, policy_loss: 0.099034
Policy train: iteration: 4000, policy_loss: 0.111126
Policy train: iteration: 4500, policy_loss: 0.116489
Policy train: iteration: 5000, policy_loss: 0.085173
Policy train: iteration: 5500, policy_loss: 0.082111
Policy train: iteration: 6000, policy_loss: 0.090400
Policy train: iteration: 6500, policy_loss: 0.073285
Policy train: iteration: 7000, policy_loss: 0.076665
Policy train: iteration: 7500, policy_loss: 0.090935
Policy train: iteration: 8000, policy_loss: 0.094474
Policy train: iteration: 8500, policy_loss: 0.074966
Policy train: iteration: 9000, policy_loss: 0.065732

Background Trial: 1, reward: -612.4485120421573
Background Trial: 2, reward: -403.2074927275121
Background Trial: 3, reward: -66.89479740031285
Background Trial: 4, reward: -461.56831545883614
Background Trial: 5, reward: -377.27443108418214
Background Trial: 6, reward: -269.71080050015587
Background Trial: 7, reward: -175.79546840501368
Background Trial: 8, reward: -449.58361835382823
Background Trial: 9, reward: -120.94239661836866
Iteration: 78, average_reward: -326.38064806559635

Policy train: iteration: 500, policy_loss: 0.080217
Policy train: iteration: 1000, policy_loss: 0.067939
Policy train: iteration: 1500, policy_loss: 0.091745
Policy train: iteration: 2000, policy_loss: 0.076158
Policy train: iteration: 2500, policy_loss: 0.085113
Policy train: iteration: 3000, policy_loss: 0.081232
Policy train: iteration: 3500, policy_loss: 0.091266
Policy train: iteration: 4000, policy_loss: 0.080440
Policy train: iteration: 4500, policy_loss: 0.094402
Policy train: iteration: 5000, policy_loss: 0.051080
Policy train: iteration: 5500, policy_loss: 0.080864
Policy train: iteration: 6000, policy_loss: 0.085951
Policy train: iteration: 6500, policy_loss: 0.092753
Policy train: iteration: 7000, policy_loss: 0.061576
Policy train: iteration: 7500, policy_loss: 0.067774
Policy train: iteration: 8000, policy_loss: 0.035700
Policy train: iteration: 8500, policy_loss: 0.079054
Policy train: iteration: 9000, policy_loss: 0.081583

Background Trial: 1, reward: -363.58102523763944
Background Trial: 2, reward: -378.9888439197694
Background Trial: 3, reward: -481.7541581096467
Background Trial: 4, reward: -97.47053267590711
Background Trial: 5, reward: -9.865189409992482
Background Trial: 6, reward: -352.05448311678424
Background Trial: 7, reward: -72.73895270560664
Background Trial: 8, reward: -399.14023343755196
Background Trial: 9, reward: -86.81297307249908
Iteration: 79, average_reward: -249.1562657428219

Policy train: iteration: 500, policy_loss: 0.092607
Policy train: iteration: 1000, policy_loss: 0.087274
Policy train: iteration: 1500, policy_loss: 0.085448
Policy train: iteration: 2000, policy_loss: 0.074774
Policy train: iteration: 2500, policy_loss: 0.077688
Policy train: iteration: 3000, policy_loss: 0.089482
Policy train: iteration: 3500, policy_loss: 0.082207
Policy train: iteration: 4000, policy_loss: 0.077672
Policy train: iteration: 4500, policy_loss: 0.081131
Policy train: iteration: 5000, policy_loss: 0.075792
Policy train: iteration: 5500, policy_loss: 0.070958
Policy train: iteration: 6000, policy_loss: 0.062477
Policy train: iteration: 6500, policy_loss: 0.088616
Policy train: iteration: 7000, policy_loss: 0.088760
Policy train: iteration: 7500, policy_loss: 0.111949
Policy train: iteration: 8000, policy_loss: 0.056165
Policy train: iteration: 8500, policy_loss: 0.080520
Policy train: iteration: 9000, policy_loss: 0.099675

Background Trial: 1, reward: -456.6514016462514
Background Trial: 2, reward: -381.5918014898186
Background Trial: 3, reward: -231.53870752114656
Background Trial: 4, reward: -280.90662630987435
Background Trial: 5, reward: -187.8762018871285
Background Trial: 6, reward: -333.87882600422415
Background Trial: 7, reward: -46.77937570492986
Background Trial: 8, reward: -358.1298135177911
Background Trial: 9, reward: -256.23990577491594
Iteration: 80, average_reward: -281.5102955395645

Policy train: iteration: 500, policy_loss: 0.055945
Policy train: iteration: 1000, policy_loss: 0.064233
Policy train: iteration: 1500, policy_loss: 0.085775
Policy train: iteration: 2000, policy_loss: 0.068454
Policy train: iteration: 2500, policy_loss: 0.080935
Policy train: iteration: 3000, policy_loss: 0.090976
Policy train: iteration: 3500, policy_loss: 0.085965
Policy train: iteration: 4000, policy_loss: 0.130727
Policy train: iteration: 4500, policy_loss: 0.094546
Policy train: iteration: 5000, policy_loss: 0.069005
Policy train: iteration: 5500, policy_loss: 0.087385
Policy train: iteration: 6000, policy_loss: 0.091900
Policy train: iteration: 6500, policy_loss: 0.112578
Policy train: iteration: 7000, policy_loss: 0.066359
Policy train: iteration: 7500, policy_loss: 0.092116
Policy train: iteration: 8000, policy_loss: 0.068175
Policy train: iteration: 8500, policy_loss: 0.087229
Policy train: iteration: 9000, policy_loss: 0.103716

Background Trial: 1, reward: 7.102179797633184
Background Trial: 2, reward: -38.47588554974331
Background Trial: 3, reward: -70.1992878028907
Background Trial: 4, reward: -77.99792299617376
Background Trial: 5, reward: -166.01742503251575
Background Trial: 6, reward: -305.00152216417575
Background Trial: 7, reward: -65.11229989922303
Background Trial: 8, reward: -489.93428416929385
Background Trial: 9, reward: 259.09711173489814
Iteration: 81, average_reward: -105.17103734238718

Policy train: iteration: 500, policy_loss: 0.081478
Policy train: iteration: 1000, policy_loss: 0.068044
Policy train: iteration: 1500, policy_loss: 0.070228
Policy train: iteration: 2000, policy_loss: 0.088773
Policy train: iteration: 2500, policy_loss: 0.056400
Policy train: iteration: 3000, policy_loss: 0.072934
Policy train: iteration: 3500, policy_loss: 0.051083
Policy train: iteration: 4000, policy_loss: 0.147435
Policy train: iteration: 4500, policy_loss: 0.087409
Policy train: iteration: 5000, policy_loss: 0.082269
Policy train: iteration: 5500, policy_loss: 0.102493
Policy train: iteration: 6000, policy_loss: 0.093018
Policy train: iteration: 6500, policy_loss: 0.082123
Policy train: iteration: 7000, policy_loss: 0.103627
Policy train: iteration: 7500, policy_loss: 0.082017
Policy train: iteration: 8000, policy_loss: 0.075000
Policy train: iteration: 8500, policy_loss: 0.078641
Policy train: iteration: 9000, policy_loss: 0.093007

Background Trial: 1, reward: 27.87192978354794
Background Trial: 2, reward: -360.69185857955944
Background Trial: 3, reward: -582.1207516867636
Background Trial: 4, reward: -106.48225725727097
Background Trial: 5, reward: -65.4777478807179
Background Trial: 6, reward: -626.0582266684033
Background Trial: 7, reward: -318.73271140623825
Background Trial: 8, reward: -62.12438646880296
Background Trial: 9, reward: -280.5400199506995
Iteration: 82, average_reward: -263.8173366794342

Policy train: iteration: 500, policy_loss: 0.070614
Policy train: iteration: 1000, policy_loss: 0.085423
Policy train: iteration: 1500, policy_loss: 0.052220
Policy train: iteration: 2000, policy_loss: 0.084472
Policy train: iteration: 2500, policy_loss: 0.060169
Policy train: iteration: 3000, policy_loss: 0.094918
Policy train: iteration: 3500, policy_loss: 0.077813
Policy train: iteration: 4000, policy_loss: 0.109972
Policy train: iteration: 4500, policy_loss: 0.072575
Policy train: iteration: 5000, policy_loss: 0.079376
Policy train: iteration: 5500, policy_loss: 0.112598
Policy train: iteration: 6000, policy_loss: 0.063207
Policy train: iteration: 6500, policy_loss: 0.064830
Policy train: iteration: 7000, policy_loss: 0.049892
Policy train: iteration: 7500, policy_loss: 0.097658
Policy train: iteration: 8000, policy_loss: 0.078006
Policy train: iteration: 8500, policy_loss: 0.112828
Policy train: iteration: 9000, policy_loss: 0.074549

Background Trial: 1, reward: -340.55568496651887
Background Trial: 2, reward: -337.13163440352434
Background Trial: 3, reward: -492.03248603845645
Background Trial: 4, reward: -35.0124254196793
Background Trial: 5, reward: -252.37345744937238
Background Trial: 6, reward: -457.7594961025115
Background Trial: 7, reward: -83.73176877014565
Background Trial: 8, reward: -287.5125629634068
Background Trial: 9, reward: -105.22827676128473
Iteration: 83, average_reward: -265.7041992083222

Policy train: iteration: 500, policy_loss: 0.069680
Policy train: iteration: 1000, policy_loss: 0.075116
Policy train: iteration: 1500, policy_loss: 0.083628
Policy train: iteration: 2000, policy_loss: 0.070045
Policy train: iteration: 2500, policy_loss: 0.072872
Policy train: iteration: 3000, policy_loss: 0.054998
Policy train: iteration: 3500, policy_loss: 0.072476
Policy train: iteration: 4000, policy_loss: 0.115919
Policy train: iteration: 4500, policy_loss: 0.054033
Policy train: iteration: 5000, policy_loss: 0.065162
Policy train: iteration: 5500, policy_loss: 0.079166
Policy train: iteration: 6000, policy_loss: 0.080916
Policy train: iteration: 6500, policy_loss: 0.095088
Policy train: iteration: 7000, policy_loss: 0.097263
Policy train: iteration: 7500, policy_loss: 0.075481
Policy train: iteration: 8000, policy_loss: 0.108307
Policy train: iteration: 8500, policy_loss: 0.089907
Policy train: iteration: 9000, policy_loss: 0.059014

Background Trial: 1, reward: -331.54706876553536
Background Trial: 2, reward: -393.4824859113015
Background Trial: 3, reward: -537.0711014005876
Background Trial: 4, reward: -65.00799226592477
Background Trial: 5, reward: 31.139690451592855
Background Trial: 6, reward: -107.75687590782017
Background Trial: 7, reward: -547.4506454237924
Background Trial: 8, reward: -523.4107250235552
Background Trial: 9, reward: 1.485060055382192
Iteration: 84, average_reward: -274.78912713239356

Policy train: iteration: 500, policy_loss: 0.060329
Policy train: iteration: 1000, policy_loss: 0.078762
Policy train: iteration: 1500, policy_loss: 0.093854
Policy train: iteration: 2000, policy_loss: 0.099325
Policy train: iteration: 2500, policy_loss: 0.090557
Policy train: iteration: 3000, policy_loss: 0.057299
Policy train: iteration: 3500, policy_loss: 0.075903
Policy train: iteration: 4000, policy_loss: 0.065429
Policy train: iteration: 4500, policy_loss: 0.083779
Policy train: iteration: 5000, policy_loss: 0.100871
Policy train: iteration: 5500, policy_loss: 0.071661
Policy train: iteration: 6000, policy_loss: 0.104690
Policy train: iteration: 6500, policy_loss: 0.072046
Policy train: iteration: 7000, policy_loss: 0.090812
Policy train: iteration: 7500, policy_loss: 0.102430
Policy train: iteration: 8000, policy_loss: 0.084891
Policy train: iteration: 8500, policy_loss: 0.056635
Policy train: iteration: 9000, policy_loss: 0.101256

Background Trial: 1, reward: -279.3854465220828
Background Trial: 2, reward: -68.849426059432
Background Trial: 3, reward: -360.031340873513
Background Trial: 4, reward: -317.7631568690664
Background Trial: 5, reward: -450.96533751104187
Background Trial: 6, reward: 238.7745615857766
Background Trial: 7, reward: -463.63640929071653
Background Trial: 8, reward: -71.16960905648978
Background Trial: 9, reward: -305.05174874833597
Iteration: 85, average_reward: -230.89754592721127

Policy train: iteration: 500, policy_loss: 0.089301
Policy train: iteration: 1000, policy_loss: 0.079558
Policy train: iteration: 1500, policy_loss: 0.077200
Policy train: iteration: 2000, policy_loss: 0.088328
Policy train: iteration: 2500, policy_loss: 0.071817
Policy train: iteration: 3000, policy_loss: 0.093682
Policy train: iteration: 3500, policy_loss: 0.059669
Policy train: iteration: 4000, policy_loss: 0.071030
Policy train: iteration: 4500, policy_loss: 0.091071
Policy train: iteration: 5000, policy_loss: 0.083195
Policy train: iteration: 5500, policy_loss: 0.090250
Policy train: iteration: 6000, policy_loss: 0.073430
Policy train: iteration: 6500, policy_loss: 0.077692
Policy train: iteration: 7000, policy_loss: 0.103357
Policy train: iteration: 7500, policy_loss: 0.101256
Policy train: iteration: 8000, policy_loss: 0.084573
Policy train: iteration: 8500, policy_loss: 0.062764
Policy train: iteration: 9000, policy_loss: 0.067004

Background Trial: 1, reward: -367.9797863043357
Background Trial: 2, reward: -431.7068991007503
Background Trial: 3, reward: -605.8150652611424
Background Trial: 4, reward: -330.86431863837566
Background Trial: 5, reward: -563.049149446851
Background Trial: 6, reward: -535.9033300653359
Background Trial: 7, reward: 30.3087996794456
Background Trial: 8, reward: -499.6082208806285
Background Trial: 9, reward: -118.27850216900319
Iteration: 86, average_reward: -380.32183024299746

Policy train: iteration: 500, policy_loss: 0.071081
Policy train: iteration: 1000, policy_loss: 0.080774
Policy train: iteration: 1500, policy_loss: 0.050975
Policy train: iteration: 2000, policy_loss: 0.076094
Policy train: iteration: 2500, policy_loss: 0.052826
Policy train: iteration: 3000, policy_loss: 0.103471
Policy train: iteration: 3500, policy_loss: 0.071738
Policy train: iteration: 4000, policy_loss: 0.105718
Policy train: iteration: 4500, policy_loss: 0.076097
Policy train: iteration: 5000, policy_loss: 0.101199
Policy train: iteration: 5500, policy_loss: 0.073340
Policy train: iteration: 6000, policy_loss: 0.103592
Policy train: iteration: 6500, policy_loss: 0.075413
Policy train: iteration: 7000, policy_loss: 0.060840
Policy train: iteration: 7500, policy_loss: 0.104123
Policy train: iteration: 8000, policy_loss: 0.060834
Policy train: iteration: 8500, policy_loss: 0.076092
Policy train: iteration: 9000, policy_loss: 0.123516

Background Trial: 1, reward: -385.9045851308487
Background Trial: 2, reward: -363.2145355014385
Background Trial: 3, reward: 217.385336988483
Background Trial: 4, reward: -327.26828354126
Background Trial: 5, reward: -520.206013836412
Background Trial: 6, reward: -269.08720006159535
Background Trial: 7, reward: -425.6398754234865
Background Trial: 8, reward: -309.69489111997785
Background Trial: 9, reward: -473.18047781363515
Iteration: 87, average_reward: -317.42339171557455

Policy train: iteration: 500, policy_loss: 0.072100
Policy train: iteration: 1000, policy_loss: 0.054712
Policy train: iteration: 1500, policy_loss: 0.053235
Policy train: iteration: 2000, policy_loss: 0.082930
Policy train: iteration: 2500, policy_loss: 0.050043
Policy train: iteration: 3000, policy_loss: 0.084316
Policy train: iteration: 3500, policy_loss: 0.065898
Policy train: iteration: 4000, policy_loss: 0.086001
Policy train: iteration: 4500, policy_loss: 0.073979
Policy train: iteration: 5000, policy_loss: 0.081817
Policy train: iteration: 5500, policy_loss: 0.080523
Policy train: iteration: 6000, policy_loss: 0.094856
Policy train: iteration: 6500, policy_loss: 0.076342
Policy train: iteration: 7000, policy_loss: 0.094233
Policy train: iteration: 7500, policy_loss: 0.086225
Policy train: iteration: 8000, policy_loss: 0.085928
Policy train: iteration: 8500, policy_loss: 0.092456
Policy train: iteration: 9000, policy_loss: 0.086667

Background Trial: 1, reward: 6.227538630856586
Background Trial: 2, reward: -406.6226812267775
Background Trial: 3, reward: -87.15749107157333
Background Trial: 4, reward: -294.7844604318301
Background Trial: 5, reward: -502.17368163736126
Background Trial: 6, reward: -462.0819203524381
Background Trial: 7, reward: -485.5462805395029
Background Trial: 8, reward: -43.64390257564263
Background Trial: 9, reward: -432.87347038650455
Iteration: 88, average_reward: -300.9618166211971

Policy train: iteration: 500, policy_loss: 0.115907
Policy train: iteration: 1000, policy_loss: 0.073495
Policy train: iteration: 1500, policy_loss: 0.060279
Policy train: iteration: 2000, policy_loss: 0.081561
Policy train: iteration: 2500, policy_loss: 0.097542
Policy train: iteration: 3000, policy_loss: 0.085022
Policy train: iteration: 3500, policy_loss: 0.091822
Policy train: iteration: 4000, policy_loss: 0.063276
Policy train: iteration: 4500, policy_loss: 0.059314
Policy train: iteration: 5000, policy_loss: 0.096826
Policy train: iteration: 5500, policy_loss: 0.077201
Policy train: iteration: 6000, policy_loss: 0.048630
Policy train: iteration: 6500, policy_loss: 0.105591
Policy train: iteration: 7000, policy_loss: 0.093659
Policy train: iteration: 7500, policy_loss: 0.094464
Policy train: iteration: 8000, policy_loss: 0.054625
Policy train: iteration: 8500, policy_loss: 0.058060
Policy train: iteration: 9000, policy_loss: 0.093851

Background Trial: 1, reward: -71.92148584167032
Background Trial: 2, reward: -315.1462123551022
Background Trial: 3, reward: -220.33688711309046
Background Trial: 4, reward: -310.75417173382766
Background Trial: 5, reward: -340.00721676834155
Background Trial: 6, reward: -320.7047819777317
Background Trial: 7, reward: -400.15683117398424
Background Trial: 8, reward: -485.95107824229996
Background Trial: 9, reward: -111.97881696494485
Iteration: 89, average_reward: -286.3286091301104

Policy train: iteration: 500, policy_loss: 0.098216
Policy train: iteration: 1000, policy_loss: 0.083328
Policy train: iteration: 1500, policy_loss: 0.120975
Policy train: iteration: 2000, policy_loss: 0.089305
Policy train: iteration: 2500, policy_loss: 0.054835
Policy train: iteration: 3000, policy_loss: 0.100769
Policy train: iteration: 3500, policy_loss: 0.065423
Policy train: iteration: 4000, policy_loss: 0.139242
Policy train: iteration: 4500, policy_loss: 0.111484
Policy train: iteration: 5000, policy_loss: 0.082196
Policy train: iteration: 5500, policy_loss: 0.065230
Policy train: iteration: 6000, policy_loss: 0.062744
Policy train: iteration: 6500, policy_loss: 0.078209
Policy train: iteration: 7000, policy_loss: 0.116209
Policy train: iteration: 7500, policy_loss: 0.072799
Policy train: iteration: 8000, policy_loss: 0.058481
Policy train: iteration: 8500, policy_loss: 0.060517
Policy train: iteration: 9000, policy_loss: 0.086905

Background Trial: 1, reward: -63.87399326310297
Background Trial: 2, reward: -396.3941856125931
Background Trial: 3, reward: -461.195662821031
Background Trial: 4, reward: -70.27926469230677
Background Trial: 5, reward: -380.84917783056164
Background Trial: 6, reward: -87.02078495348917
Background Trial: 7, reward: -94.8247397220455
Background Trial: 8, reward: -65.85056529276358
Background Trial: 9, reward: -501.72336403617004
Iteration: 90, average_reward: -235.77908202489596

Policy train: iteration: 500, policy_loss: 0.138382
Policy train: iteration: 1000, policy_loss: 0.074602
Policy train: iteration: 1500, policy_loss: 0.087712
Policy train: iteration: 2000, policy_loss: 0.055392
Policy train: iteration: 2500, policy_loss: 0.086386
Policy train: iteration: 3000, policy_loss: 0.086927
Policy train: iteration: 3500, policy_loss: 0.075175
Policy train: iteration: 4000, policy_loss: 0.048341
Policy train: iteration: 4500, policy_loss: 0.070399
Policy train: iteration: 5000, policy_loss: 0.049114
Policy train: iteration: 5500, policy_loss: 0.068452
Policy train: iteration: 6000, policy_loss: 0.071775
Policy train: iteration: 6500, policy_loss: 0.127572
Policy train: iteration: 7000, policy_loss: 0.070531
Policy train: iteration: 7500, policy_loss: 0.087252
Policy train: iteration: 8000, policy_loss: 0.043549
Policy train: iteration: 8500, policy_loss: 0.104992
Policy train: iteration: 9000, policy_loss: 0.117156

Background Trial: 1, reward: -363.08247827046546
Background Trial: 2, reward: -403.38784699956113
Background Trial: 3, reward: -443.1183976084779
Background Trial: 4, reward: -361.5982188501714
Background Trial: 5, reward: 240.65724987133166
Background Trial: 6, reward: -322.4159783011734
Background Trial: 7, reward: -131.01413694292447
Background Trial: 8, reward: -454.0342271475195
Background Trial: 9, reward: -14.648437158424088
Iteration: 91, average_reward: -250.293607934154

Policy train: iteration: 500, policy_loss: 0.105149
