Collecting dynamics training data 1000
Collecting dynamics training data 2000
Collecting dynamics training data 3000
Collecting dynamics training data 4000
Collecting dynamics training data 5000
Collecting dynamics training data 6000
Collecting dynamics training data 7000
Collecting dynamics training data 8000
Collecting dynamics training data 9000
Collecting dynamics training data 10000
FDM train: iteration: 500, fdm_loss: 0.019440
FDM train: iteration: 1000, fdm_loss: 0.008536
FDM train: iteration: 1500, fdm_loss: 0.007862
FDM train: iteration: 2000, fdm_loss: 0.008775
FDM train: iteration: 2500, fdm_loss: 0.006084
FDM train: iteration: 3000, fdm_loss: 0.010216
FDM train: iteration: 3500, fdm_loss: 0.007208
FDM train: iteration: 4000, fdm_loss: 0.007070
FDM train: iteration: 4500, fdm_loss: 0.009200
FDM train: iteration: 5000, fdm_loss: 0.012540

episode_reward: -367.6
Background Trial: 1, reward: -382.1338059381233
Background Trial: 2, reward: -302.54856699400875
Background Trial: 3, reward: -274.3592208267054
Background Trial: 4, reward: -450.7649097404894
Background Trial: 5, reward: -272.3168145174657
Background Trial: 6, reward: -315.7341913129308
Background Trial: 7, reward: -323.1122699146194
Background Trial: 8, reward: -310.553783628353
Background Trial: 9, reward: -315.55115039592226
Iteration: 1, average_reward: -327.45274591873533, policy_loss: 0.385263, fdm_loss: 0.009949


episode_reward: -382.5Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.007171
FDM train: iteration: 1000, fdm_loss: 0.012635
FDM train: iteration: 1500, fdm_loss: 0.002760
FDM train: iteration: 2000, fdm_loss: 0.005628
FDM train: iteration: 2500, fdm_loss: 0.006812
FDM train: iteration: 3000, fdm_loss: 0.004334
FDM train: iteration: 3500, fdm_loss: 0.007923
FDM train: iteration: 4000, fdm_loss: 0.009332
FDM train: iteration: 4500, fdm_loss: 0.003425
FDM train: iteration: 5000, fdm_loss: 0.008441

Background Trial: 1, reward: -453.15792417551387
Background Trial: 2, reward: -370.80937361137126
Background Trial: 3, reward: -416.7705001938173
Background Trial: 4, reward: -376.85903235367596
Background Trial: 5, reward: -933.3594500477645
Background Trial: 6, reward: -741.0817441641267
Background Trial: 7, reward: -214.76345021605437
Background Trial: 8, reward: -393.83712974897526
Background Trial: 9, reward: -289.470974544066
Iteration: 2, average_reward: -465.5677310061517, policy_loss: 0.570288, fdm_loss: 0.006276


episode_reward: -214.5
Background Trial: 1, reward: -318.39200980723666
Background Trial: 2, reward: -762.6442652675097
Background Trial: 3, reward: -276.2081879367023
Background Trial: 4, reward: -225.807212229132
Background Trial: 5, reward: -576.1559283144018
Background Trial: 6, reward: -346.8003967214508
Background Trial: 7, reward: -409.2604860565177
Background Trial: 8, reward: -272.8287512397146
Background Trial: 9, reward: -391.21263786961197
Iteration: 3, average_reward: -397.70109727136423, policy_loss: 0.605777, fdm_loss: 0.010791


episode_reward: -230.3Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.004523
FDM train: iteration: 1000, fdm_loss: 0.002980
FDM train: iteration: 1500, fdm_loss: 0.004404
FDM train: iteration: 2000, fdm_loss: 0.007921
FDM train: iteration: 2500, fdm_loss: 0.006312
FDM train: iteration: 3000, fdm_loss: 0.007446
FDM train: iteration: 3500, fdm_loss: 0.002174
FDM train: iteration: 4000, fdm_loss: 0.004123
FDM train: iteration: 4500, fdm_loss: 0.008286
FDM train: iteration: 5000, fdm_loss: 0.012215

Background Trial: 1, reward: -341.077902728219
Background Trial: 2, reward: -335.3969643533343
Background Trial: 3, reward: -227.49650634915224
Background Trial: 4, reward: -365.5348009545363
Background Trial: 5, reward: -311.73909735156724
Background Trial: 6, reward: -276.00933969972857
Background Trial: 7, reward: -282.38083604030885
Background Trial: 8, reward: -339.79798680961585
Background Trial: 9, reward: -303.317280168877
Iteration: 4, average_reward: -309.1945238283711, policy_loss: 0.629402, fdm_loss: 0.008171


episode_reward: -230.1
Background Trial: 1, reward: -224.94960037532704
Background Trial: 2, reward: -681.479172637152
Background Trial: 3, reward: -451.15473958458557
Background Trial: 4, reward: -314.62212375733696
Background Trial: 5, reward: -429.50401481515064
Background Trial: 6, reward: -605.5048320314015
Background Trial: 7, reward: -552.3383783656427
Background Trial: 8, reward: -572.4092954245612
Background Trial: 9, reward: -684.6102651195839
Iteration: 5, average_reward: -501.8413802345268, policy_loss: 0.635924, fdm_loss: 0.004908


episode_reward: -117.2Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.006983
FDM train: iteration: 1000, fdm_loss: 0.005300
FDM train: iteration: 1500, fdm_loss: 0.007610
FDM train: iteration: 2000, fdm_loss: 0.007970
FDM train: iteration: 2500, fdm_loss: 0.004108
FDM train: iteration: 3000, fdm_loss: 0.007723
FDM train: iteration: 3500, fdm_loss: 0.004301
FDM train: iteration: 4000, fdm_loss: 0.004941
FDM train: iteration: 4500, fdm_loss: 0.006177
FDM train: iteration: 5000, fdm_loss: 0.006862

Background Trial: 1, reward: 154.26222879118544
Background Trial: 2, reward: -107.45525252995627
Background Trial: 3, reward: -60.781802345279544
Background Trial: 4, reward: -129.84593910470988
Background Trial: 5, reward: -60.87511853839758
Background Trial: 6, reward: -91.33997138540069
Background Trial: 7, reward: 207.85622663923868
Background Trial: 8, reward: -274.9779112385879
Background Trial: 9, reward: -252.7149154282594
Iteration: 6, average_reward: -68.4302727933519, policy_loss: 0.518426, fdm_loss: 0.007393


episode_reward: 255.6
Background Trial: 1, reward: -255.71429981443558
Background Trial: 2, reward: -92.32418559760816
Background Trial: 3, reward: -298.8488076820768
Background Trial: 4, reward: -75.52072829538977
Background Trial: 5, reward: -266.27350987676857
Background Trial: 6, reward: -138.98086091522498
Background Trial: 7, reward: -264.0879494152065
Background Trial: 8, reward: -402.53973629344256
Background Trial: 9, reward: -72.03641742649684
Iteration: 7, average_reward: -207.36961059073886, policy_loss: 0.590217, fdm_loss: 0.007909


episode_reward:  -4.1Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.004723
FDM train: iteration: 1000, fdm_loss: 0.010371
FDM train: iteration: 1500, fdm_loss: 0.005053
FDM train: iteration: 2000, fdm_loss: 0.002802
FDM train: iteration: 2500, fdm_loss: 0.008752
FDM train: iteration: 3000, fdm_loss: 0.009812
FDM train: iteration: 3500, fdm_loss: 0.006371
FDM train: iteration: 4000, fdm_loss: 0.006463
FDM train: iteration: 4500, fdm_loss: 0.003243
FDM train: iteration: 5000, fdm_loss: 0.002585

Background Trial: 1, reward: -84.27186943454022
Background Trial: 2, reward: -81.42061974542393
Background Trial: 3, reward: -159.09146558713934
Background Trial: 4, reward: -177.42970684730363
Background Trial: 5, reward: -129.15299179808503
Background Trial: 6, reward: -54.669911782153335
Background Trial: 7, reward: -70.17927061270281
Background Trial: 8, reward: -82.89236913089124
Background Trial: 9, reward: -129.74784680580842
Iteration: 8, average_reward: -107.65067241600532, policy_loss: 0.326926, fdm_loss: 0.005621


episode_reward: -42.5
Background Trial: 1, reward: -170.91000849420135
Background Trial: 2, reward: -76.29351178155468
Background Trial: 3, reward: -142.67289310295064
Background Trial: 4, reward: -201.45702401063932
Background Trial: 5, reward: -202.9335538334502
Background Trial: 6, reward: -139.9804693331073
Background Trial: 7, reward: -91.44055599974669
Background Trial: 8, reward: -255.65085108513674
Background Trial: 9, reward: -157.11079814831237
Iteration: 9, average_reward: -159.82774064323326, policy_loss: 0.305025, fdm_loss: 0.003627


episode_reward: 263.0Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.005938
FDM train: iteration: 1000, fdm_loss: 0.004892
FDM train: iteration: 1500, fdm_loss: 0.006363
FDM train: iteration: 2000, fdm_loss: 0.005917
FDM train: iteration: 2500, fdm_loss: 0.005039
FDM train: iteration: 3000, fdm_loss: 0.003927
FDM train: iteration: 3500, fdm_loss: 0.002410
FDM train: iteration: 4000, fdm_loss: 0.007209
FDM train: iteration: 4500, fdm_loss: 0.003958
FDM train: iteration: 5000, fdm_loss: 0.003720

Background Trial: 1, reward: -70.00622826328893
Background Trial: 2, reward: -56.09536997425184
Background Trial: 3, reward: -214.80738506828845
Background Trial: 4, reward: -81.25692574393003
Background Trial: 5, reward: -55.192933293439395
Background Trial: 6, reward: -51.035254644842205
Background Trial: 7, reward: -390.4956272239097
Background Trial: 8, reward: -27.559324220463495
Background Trial: 9, reward: -132.5195408177775
Iteration: 10, average_reward: -119.88539880557684, policy_loss: 0.385825, fdm_loss: 0.003260


episode_reward: -29.6
Background Trial: 1, reward: -153.7974008402138
Background Trial: 2, reward: -310.474720023402
Background Trial: 3, reward: -250.81644897518862
Background Trial: 4, reward: -146.81369735713923
Background Trial: 5, reward: -93.13422173546088
Background Trial: 6, reward: -185.10501603122222
Background Trial: 7, reward: -89.80352168751074
Background Trial: 8, reward: -332.7248516781924
Background Trial: 9, reward: -149.04551481160516
Iteration: 11, average_reward: -190.19059923777058, policy_loss: 0.366601, fdm_loss: 0.004851


episode_reward: 246.6Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.003812
FDM train: iteration: 1000, fdm_loss: 0.002872
FDM train: iteration: 1500, fdm_loss: 0.004560
FDM train: iteration: 2000, fdm_loss: 0.003086
FDM train: iteration: 2500, fdm_loss: 0.007858
FDM train: iteration: 3000, fdm_loss: 0.002842
FDM train: iteration: 3500, fdm_loss: 0.002344
FDM train: iteration: 4000, fdm_loss: 0.011215
FDM train: iteration: 4500, fdm_loss: 0.004393
FDM train: iteration: 5000, fdm_loss: 0.005137

Background Trial: 1, reward: -270.67006198217473
Background Trial: 2, reward: -304.87291057739264
Background Trial: 3, reward: -351.2229737563464
Background Trial: 4, reward: -110.84750720722062
Background Trial: 5, reward: -203.97946926265257
Background Trial: 6, reward: -264.89352604024646
Background Trial: 7, reward: -308.2093591783954
Background Trial: 8, reward: -282.6814894938442
Background Trial: 9, reward: -165.20125186557928
Iteration: 12, average_reward: -251.39761659598364, policy_loss: 0.368745, fdm_loss: 0.003889


episode_reward: -112.1
Background Trial: 1, reward: -18.29321909099319
Background Trial: 2, reward: -61.27317186098256
Background Trial: 3, reward: -187.2779993243771
Background Trial: 4, reward: 199.66588605336108
Background Trial: 5, reward: -190.91859221914189
Background Trial: 6, reward: -118.2516469093585
Background Trial: 7, reward: -227.61866158068764
Background Trial: 8, reward: -203.50927988316448
Background Trial: 9, reward: -305.8574767588077
Iteration: 13, average_reward: -123.70379573046132, policy_loss: 0.377392, fdm_loss: 0.006842


episode_reward: 203.2Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.005864
FDM train: iteration: 1000, fdm_loss: 0.003293
FDM train: iteration: 1500, fdm_loss: 0.005375
FDM train: iteration: 2000, fdm_loss: 0.002986
FDM train: iteration: 2500, fdm_loss: 0.004697
FDM train: iteration: 3000, fdm_loss: 0.003589
FDM train: iteration: 3500, fdm_loss: 0.002748
FDM train: iteration: 4000, fdm_loss: 0.003152
FDM train: iteration: 4500, fdm_loss: 0.003251
FDM train: iteration: 5000, fdm_loss: 0.005560

Background Trial: 1, reward: -149.2931890965903
Background Trial: 2, reward: -138.0697065103516
Background Trial: 3, reward: 161.2582422218249
Background Trial: 4, reward: -9.045237933311796
Background Trial: 5, reward: -16.953442053799918
Background Trial: 6, reward: -261.8713884308347
Background Trial: 7, reward: -299.7076244087172
Background Trial: 8, reward: -181.10531601761068
Background Trial: 9, reward: -144.93481623428713
Iteration: 14, average_reward: -115.52471982929761, policy_loss: 0.368300, fdm_loss: 0.005238


episode_reward: 251.7
Background Trial: 1, reward: 118.90864411093308
Background Trial: 2, reward: -114.57815584830442
Background Trial: 3, reward: -72.5173640070715
Background Trial: 4, reward: -127.3416779164826
Background Trial: 5, reward: -175.26206680209026
Background Trial: 6, reward: -152.27133652829832
Background Trial: 7, reward: 201.6194117514993
Background Trial: 8, reward: -79.14111029294997
Background Trial: 9, reward: 158.72482911799844
Iteration: 15, average_reward: -26.87320293497402, policy_loss: 0.501000, fdm_loss: 0.002458


episode_reward: -123.3Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.003620
FDM train: iteration: 1000, fdm_loss: 0.005303
FDM train: iteration: 1500, fdm_loss: 0.004828
FDM train: iteration: 2000, fdm_loss: 0.005691
FDM train: iteration: 2500, fdm_loss: 0.004011
FDM train: iteration: 3000, fdm_loss: 0.002683
FDM train: iteration: 3500, fdm_loss: 0.003615
FDM train: iteration: 4000, fdm_loss: 0.004410
FDM train: iteration: 4500, fdm_loss: 0.005046
FDM train: iteration: 5000, fdm_loss: 0.005635

Background Trial: 1, reward: -73.39390682863274
Background Trial: 2, reward: -77.88498495673846
Background Trial: 3, reward: -372.3605980927162
Background Trial: 4, reward: -63.18098029750185
Background Trial: 5, reward: 199.83663428298067
Background Trial: 6, reward: -74.28981050495835
Background Trial: 7, reward: -103.35445641911696
Background Trial: 8, reward: -73.08848920986671
Background Trial: 9, reward: -280.21732678036324
Iteration: 16, average_reward: -101.99265764521265, policy_loss: 0.490208, fdm_loss: 0.004581


episode_reward: -83.5
Background Trial: 1, reward: -2.7780382755950797
Background Trial: 2, reward: -100.47830788219481
Background Trial: 3, reward: -51.44968653370074
Background Trial: 4, reward: -76.22898709002422
Background Trial: 5, reward: -58.64090768434347
Background Trial: 6, reward: -56.1286021313263
Background Trial: 7, reward: -8.188278296375842
Background Trial: 8, reward: -77.65951606126575
Background Trial: 9, reward: 14.37579271450474
Iteration: 17, average_reward: -46.35294791559128, policy_loss: 0.461536, fdm_loss: 0.002940


episode_reward: 172.4Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.003233
FDM train: iteration: 1000, fdm_loss: 0.003256
FDM train: iteration: 1500, fdm_loss: 0.007602
FDM train: iteration: 2000, fdm_loss: 0.004141
FDM train: iteration: 2500, fdm_loss: 0.004144
FDM train: iteration: 3000, fdm_loss: 0.004142
FDM train: iteration: 3500, fdm_loss: 0.003686
FDM train: iteration: 4000, fdm_loss: 0.005325
FDM train: iteration: 4500, fdm_loss: 0.005493
FDM train: iteration: 5000, fdm_loss: 0.003149

Background Trial: 1, reward: -46.84240241322066
Background Trial: 2, reward: -127.10983868823135
Background Trial: 3, reward: -106.61169634686607
Background Trial: 4, reward: -116.77621285264377
Background Trial: 5, reward: -65.30527722848643
Background Trial: 6, reward: -169.68456989804702
Background Trial: 7, reward: -69.17710705891518
Background Trial: 8, reward: -10.307122857947277
Background Trial: 9, reward: -77.32500965588002
Iteration: 18, average_reward: -87.68213744447088, policy_loss: 0.575531, fdm_loss: 0.002777


episode_reward: 225.1
Background Trial: 1, reward: -124.24618957516873
Background Trial: 2, reward: -37.91674054418093
Background Trial: 3, reward: -53.78147102185659
Background Trial: 4, reward: 221.86251576162246
Background Trial: 5, reward: -14.645345654581632
Background Trial: 6, reward: -100.61828693611595
Background Trial: 7, reward: -10.954621741830962
Background Trial: 8, reward: -73.28723322645973
Background Trial: 9, reward: -147.07197128190685
Iteration: 19, average_reward: -37.85103824671988, policy_loss: 0.600886, fdm_loss: 0.005412


episode_reward: 285.9Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.004532
FDM train: iteration: 1000, fdm_loss: 0.002726
FDM train: iteration: 1500, fdm_loss: 0.004457
FDM train: iteration: 2000, fdm_loss: 0.003124
FDM train: iteration: 2500, fdm_loss: 0.003813
FDM train: iteration: 3000, fdm_loss: 0.003613
FDM train: iteration: 3500, fdm_loss: 0.002988
FDM train: iteration: 4000, fdm_loss: 0.004855
FDM train: iteration: 4500, fdm_loss: 0.001696
FDM train: iteration: 5000, fdm_loss: 0.005008

Background Trial: 1, reward: -413.6715260595308
Background Trial: 2, reward: -86.55340032620603
Background Trial: 3, reward: 214.69991452714396
Background Trial: 4, reward: -143.94613247907373
Background Trial: 5, reward: -316.6669617754867
Background Trial: 6, reward: -424.48275208043043
Background Trial: 7, reward: -173.40783249560116
Background Trial: 8, reward: -337.7996168294883
Background Trial: 9, reward: -314.6247815317795
Iteration: 20, average_reward: -221.82812100560582, policy_loss: 0.544834, fdm_loss: 0.003044


episode_reward:  -5.7
Background Trial: 1, reward: 168.14761518886235
Background Trial: 2, reward: 186.29859858843236
Background Trial: 3, reward: 27.81219122503319
Background Trial: 4, reward: 17.11965864153167
Background Trial: 5, reward: -41.22780438391073
Background Trial: 6, reward: 253.65926385708576
Background Trial: 7, reward: 212.74157234031378
Background Trial: 8, reward: -55.1687218265489
Background Trial: 9, reward: 2.9922489187569568
Iteration: 21, average_reward: 85.81940250550626, policy_loss: 0.542813, fdm_loss: 0.001739


episode_reward: 237.7Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.004145
FDM train: iteration: 1000, fdm_loss: 0.003207
FDM train: iteration: 1500, fdm_loss: 0.002862
FDM train: iteration: 2000, fdm_loss: 0.003121
FDM train: iteration: 2500, fdm_loss: 0.003174
FDM train: iteration: 3000, fdm_loss: 0.003429
FDM train: iteration: 3500, fdm_loss: 0.002369
FDM train: iteration: 4000, fdm_loss: 0.003562
FDM train: iteration: 4500, fdm_loss: 0.005375
FDM train: iteration: 5000, fdm_loss: 0.002017

Background Trial: 1, reward: -33.00293465563084
Background Trial: 2, reward: -86.13162792137288
Background Trial: 3, reward: 242.89321563374514
Background Trial: 4, reward: -23.72921262182092
Background Trial: 5, reward: 264.2513983490445
Background Trial: 6, reward: -28.829750860838786
Background Trial: 7, reward: -139.05440251700014
Background Trial: 8, reward: -116.17863089340045
Background Trial: 9, reward: 182.41960835596996
Iteration: 22, average_reward: 29.18196254096618, policy_loss: 0.622774, fdm_loss: 0.003430


episode_reward: 227.7
Background Trial: 1, reward: -222.07456250622909
Background Trial: 2, reward: 183.32924297131822
Background Trial: 3, reward: -80.61368506424748
Background Trial: 4, reward: -53.19137709448431
Background Trial: 5, reward: -39.35860450162602
Background Trial: 6, reward: -392.3185447023948
Background Trial: 7, reward: 185.9686999673915
Background Trial: 8, reward: 13.946677811589922
Background Trial: 9, reward: 4.743734854792891
Iteration: 23, average_reward: -44.3964909182099, policy_loss: 0.520739, fdm_loss: 0.002052


episode_reward: 238.4Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.008293
FDM train: iteration: 1000, fdm_loss: 0.003341
FDM train: iteration: 1500, fdm_loss: 0.004324
FDM train: iteration: 2000, fdm_loss: 0.002253
FDM train: iteration: 2500, fdm_loss: 0.004219
FDM train: iteration: 3000, fdm_loss: 0.002239
FDM train: iteration: 3500, fdm_loss: 0.005385
FDM train: iteration: 4000, fdm_loss: 0.003395
FDM train: iteration: 4500, fdm_loss: 0.004596
FDM train: iteration: 5000, fdm_loss: 0.002305

Background Trial: 1, reward: -28.345304777486618
Background Trial: 2, reward: 42.904488864198214
Background Trial: 3, reward: 5.349747895195634
Background Trial: 4, reward: 258.36428427026794
Background Trial: 5, reward: -19.23174492940359
Background Trial: 6, reward: -54.83509941441944
Background Trial: 7, reward: -317.33301530568167
Background Trial: 8, reward: -231.28855819717774
Background Trial: 9, reward: 187.98863605268545
Iteration: 24, average_reward: -17.38072950464687, policy_loss: 0.783584, fdm_loss: 0.003735


episode_reward: 260.6
Background Trial: 1, reward: -228.58904832680284
Background Trial: 2, reward: -46.709185578491116
Background Trial: 3, reward: -3.0564946276800953
Background Trial: 4, reward: 129.22294402769592
Background Trial: 5, reward: 265.76920402124983
Background Trial: 6, reward: -251.82114083616798
Background Trial: 7, reward: -379.55121158195726
Background Trial: 8, reward: -59.3629297529153
Background Trial: 9, reward: -150.06035015953591
Iteration: 25, average_reward: -80.4620236460672, policy_loss: 0.655508, fdm_loss: 0.004736

