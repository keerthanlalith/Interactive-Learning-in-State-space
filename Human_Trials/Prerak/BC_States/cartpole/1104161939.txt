Policy train: iteration: 500, policy_loss: 0.252216
Policy train: iteration: 1000, policy_loss: 0.218505
Policy train: iteration: 1500, policy_loss: 0.310707
Policy train: iteration: 2000, policy_loss: 0.167706
Policy train: iteration: 2500, policy_loss: 0.172574
Policy train: iteration: 3000, policy_loss: 0.145665
Policy train: iteration: 3500, policy_loss: 0.148481
Policy train: iteration: 4000, policy_loss: 0.239863

Background Trial: 1, reward: 105.0
Background Trial: 2, reward: 147.0
Background Trial: 3, reward: 67.0
Background Trial: 4, reward: 76.0
Background Trial: 5, reward: 97.0
Background Trial: 6, reward: 93.0
Background Trial: 7, reward: 135.0
Background Trial: 8, reward: 91.0
Background Trial: 9, reward: 107.0
Iteration: 1, average_reward: 102.0

Policy train: iteration: 500, policy_loss: 0.215259
Policy train: iteration: 1000, policy_loss: 0.269844
Policy train: iteration: 1500, policy_loss: 0.131924
Policy train: iteration: 2000, policy_loss: 0.102785
Policy train: iteration: 2500, policy_loss: 0.177771
Policy train: iteration: 3000, policy_loss: 0.207376
Policy train: iteration: 3500, policy_loss: 0.161733
Policy train: iteration: 4000, policy_loss: 0.206199

Background Trial: 1, reward: 156.0
Background Trial: 2, reward: 135.0
Background Trial: 3, reward: 165.0
Background Trial: 4, reward: 166.0
Background Trial: 5, reward: 192.0
Background Trial: 6, reward: 139.0
Background Trial: 7, reward: 154.0
Background Trial: 8, reward: 161.0
Background Trial: 9, reward: 159.0
Iteration: 2, average_reward: 158.55555555555554

Policy train: iteration: 500, policy_loss: 0.102471
Policy train: iteration: 1000, policy_loss: 0.148513
Policy train: iteration: 1500, policy_loss: 0.118118
Policy train: iteration: 2000, policy_loss: 0.193975
Policy train: iteration: 2500, policy_loss: 0.193426
Policy train: iteration: 3000, policy_loss: 0.149715
Policy train: iteration: 3500, policy_loss: 0.161775
Policy train: iteration: 4000, policy_loss: 0.147884

Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 200.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 3, average_reward: 200.0

Policy train: iteration: 500, policy_loss: 0.192085
Policy train: iteration: 1000, policy_loss: 0.163820
Policy train: iteration: 1500, policy_loss: 0.198648
Policy train: iteration: 2000, policy_loss: 0.100276
Policy train: iteration: 2500, policy_loss: 0.208489
Policy train: iteration: 3000, policy_loss: 0.168230
Policy train: iteration: 3500, policy_loss: 0.089321
Policy train: iteration: 4000, policy_loss: 0.205677

Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 200.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 4, average_reward: 200.0

Policy train: iteration: 500, policy_loss: 0.123663
Policy train: iteration: 1000, policy_loss: 0.118756
Policy train: iteration: 1500, policy_loss: 0.157341
Policy train: iteration: 2000, policy_loss: 0.138567
Policy train: iteration: 2500, policy_loss: 0.220535
Policy train: iteration: 3000, policy_loss: 0.116414
Policy train: iteration: 3500, policy_loss: 0.194723
Policy train: iteration: 4000, policy_loss: 0.226739

Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 200.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 5, average_reward: 200.0

Policy train: iteration: 500, policy_loss: 0.182197
Policy train: iteration: 1000, policy_loss: 0.119813
Policy train: iteration: 1500, policy_loss: 0.181673
Policy train: iteration: 2000, policy_loss: 0.161579
Policy train: iteration: 2500, policy_loss: 0.218743
Policy train: iteration: 3000, policy_loss: 0.177917
Policy train: iteration: 3500, policy_loss: 0.126272
Policy train: iteration: 4000, policy_loss: 0.164152

Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 188.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 166.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 6, average_reward: 194.88888888888889

Policy train: iteration: 500, policy_loss: 0.100845
Policy train: iteration: 1000, policy_loss: 0.206610
Policy train: iteration: 1500, policy_loss: 0.141038
Policy train: iteration: 2000, policy_loss: 0.221986
Policy train: iteration: 2500, policy_loss: 0.242005
Policy train: iteration: 3000, policy_loss: 0.110123
Policy train: iteration: 3500, policy_loss: 0.152406
Policy train: iteration: 4000, policy_loss: 0.149963

Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 200.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 7, average_reward: 200.0

Policy train: iteration: 500, policy_loss: 0.176568
Policy train: iteration: 1000, policy_loss: 0.185277
Policy train: iteration: 1500, policy_loss: 0.149902
Policy train: iteration: 2000, policy_loss: 0.202011
Policy train: iteration: 2500, policy_loss: 0.168936
Policy train: iteration: 3000, policy_loss: 0.128605
Policy train: iteration: 3500, policy_loss: 0.138434
Policy train: iteration: 4000, policy_loss: 0.178667

Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 183.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 200.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 8, average_reward: 198.11111111111111

Policy train: iteration: 500, policy_loss: 0.159742
Policy train: iteration: 1000, policy_loss: 0.108566
Policy train: iteration: 1500, policy_loss: 0.137009
Policy train: iteration: 2000, policy_loss: 0.090095
Policy train: iteration: 2500, policy_loss: 0.151594
Policy train: iteration: 3000, policy_loss: 0.224292
Policy train: iteration: 3500, policy_loss: 0.195798
Policy train: iteration: 4000, policy_loss: 0.096332

Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 200.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 9, average_reward: 200.0

Policy train: iteration: 500, policy_loss: 0.134342
Policy train: iteration: 1000, policy_loss: 0.131448
Policy train: iteration: 1500, policy_loss: 0.104538
Policy train: iteration: 2000, policy_loss: 0.162350
Policy train: iteration: 2500, policy_loss: 0.157418
Policy train: iteration: 3000, policy_loss: 0.121614
Policy train: iteration: 3500, policy_loss: 0.126279
Policy train: iteration: 4000, policy_loss: 0.116225

Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 200.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 10, average_reward: 200.0

Policy train: iteration: 500, policy_loss: 0.125004
Policy train: iteration: 1000, policy_loss: 0.106037
Policy train: iteration: 1500, policy_loss: 0.102755
Policy train: iteration: 2000, policy_loss: 0.110233
Policy train: iteration: 2500, policy_loss: 0.109060
Policy train: iteration: 3000, policy_loss: 0.117606
Policy train: iteration: 3500, policy_loss: 0.178002
Policy train: iteration: 4000, policy_loss: 0.144134

Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 200.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 11, average_reward: 200.0

Policy train: iteration: 500, policy_loss: 0.124646
Policy train: iteration: 1000, policy_loss: 0.111195
Policy train: iteration: 1500, policy_loss: 0.097356
Policy train: iteration: 2000, policy_loss: 0.136091
Policy train: iteration: 2500, policy_loss: 0.208117
Policy train: iteration: 3000, policy_loss: 0.154753
Policy train: iteration: 3500, policy_loss: 0.094572
Policy train: iteration: 4000, policy_loss: 0.225968

Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 200.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 12, average_reward: 200.0

Policy train: iteration: 500, policy_loss: 0.169399
Policy train: iteration: 1000, policy_loss: 0.132036
