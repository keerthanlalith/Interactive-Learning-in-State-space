
episode_reward: -388.7
Background Trial: 1, reward: -481.0745450612683
Background Trial: 2, reward: -428.24942054368506
Background Trial: 3, reward: -155.66578375447
Background Trial: 4, reward: -384.5485271808769
Background Trial: 5, reward: -667.3132195042654
Background Trial: 6, reward: -303.5163083746876
Background Trial: 7, reward: -531.418160902101
Background Trial: 8, reward: -424.8488318077374
Background Trial: 9, reward: -410.5676123790366
Iteration: 1, average_reward: -420.8002677231254, policy_loss: 0.138520, fdm_loss: 0.000000


episode_reward: 247.3Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.004123
FDM train: iteration: 1000, fdm_loss: 0.004592
FDM train: iteration: 1500, fdm_loss: 0.004301
FDM train: iteration: 2000, fdm_loss: 0.004052
FDM train: iteration: 2500, fdm_loss: 0.003114
FDM train: iteration: 3000, fdm_loss: 0.002907
FDM train: iteration: 3500, fdm_loss: 0.003529
FDM train: iteration: 4000, fdm_loss: 0.003180
FDM train: iteration: 4500, fdm_loss: 0.003762
FDM train: iteration: 5000, fdm_loss: 0.002347

Background Trial: 1, reward: 50.46218085180766
Background Trial: 2, reward: 50.65468929246208
Background Trial: 3, reward: 20.873034943968065
Background Trial: 4, reward: -55.124341654555145
Background Trial: 5, reward: 266.8620801839098
Background Trial: 6, reward: -25.105059327627444
Background Trial: 7, reward: 52.576787326392065
Background Trial: 8, reward: -19.633048178594066
Background Trial: 9, reward: 265.46139344754795
Iteration: 2, average_reward: 67.44752409836788, policy_loss: 0.869865, fdm_loss: 0.003406


episode_reward:  33.7
Background Trial: 1, reward: -82.03940016042182
Background Trial: 2, reward: 27.59917283299073
Background Trial: 3, reward: 28.79189698371377
Background Trial: 4, reward: 25.30205494187868
Background Trial: 5, reward: -6.169179765935141
Background Trial: 6, reward: -73.1533984579917
Background Trial: 7, reward: -60.013767382579374
Background Trial: 8, reward: -61.36119798735622
Background Trial: 9, reward: 10.043183420961114
Iteration: 3, average_reward: -21.222292841637778, policy_loss: 0.817441, fdm_loss: 0.003126


episode_reward: -84.4Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.004323
FDM train: iteration: 1000, fdm_loss: 0.004279
FDM train: iteration: 1500, fdm_loss: 0.002490
FDM train: iteration: 2000, fdm_loss: 0.004265
FDM train: iteration: 2500, fdm_loss: 0.003838
FDM train: iteration: 3000, fdm_loss: 0.003089
FDM train: iteration: 3500, fdm_loss: 0.003406
FDM train: iteration: 4000, fdm_loss: 0.002947
FDM train: iteration: 4500, fdm_loss: 0.003218
FDM train: iteration: 5000, fdm_loss: 0.003276

Background Trial: 1, reward: 46.34792877759543
Background Trial: 2, reward: -70.01883825514506
Background Trial: 3, reward: 50.63441980597719
Background Trial: 4, reward: 42.56139973356852
Background Trial: 5, reward: -33.254051485142696
Background Trial: 6, reward: -25.143947425039116
Background Trial: 7, reward: 32.148452272054726
Background Trial: 8, reward: -19.298012413145358
Background Trial: 9, reward: 19.068867169738425
Iteration: 4, average_reward: 4.782913131162451, policy_loss: 0.975283, fdm_loss: 0.002670


episode_reward: 225.8
Background Trial: 1, reward: 306.7127767749842
Background Trial: 2, reward: 267.6471755229146
Background Trial: 3, reward: 281.04681346091274
Background Trial: 4, reward: 4.392713302186024
Background Trial: 5, reward: 295.89623547598705
Background Trial: 6, reward: 97.03095749435403
Background Trial: 7, reward: 278.3213502085599
Background Trial: 8, reward: 320.851923982256
Background Trial: 9, reward: 208.69980798447773
Iteration: 5, average_reward: 228.9555282451814, policy_loss: 1.093627, fdm_loss: 0.003962


episode_reward: 196.3Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.003062
FDM train: iteration: 1000, fdm_loss: 0.002721
FDM train: iteration: 1500, fdm_loss: 0.003263
FDM train: iteration: 2000, fdm_loss: 0.002182
FDM train: iteration: 2500, fdm_loss: 0.002935
FDM train: iteration: 3000, fdm_loss: 0.003249
FDM train: iteration: 3500, fdm_loss: 0.002450
FDM train: iteration: 4000, fdm_loss: 0.002357
FDM train: iteration: 4500, fdm_loss: 0.003260
FDM train: iteration: 5000, fdm_loss: 0.003087

Background Trial: 1, reward: -117.44206666638746
Background Trial: 2, reward: 168.751382837988
Background Trial: 3, reward: 15.588248625267804
Background Trial: 4, reward: -76.95317608861684
Background Trial: 5, reward: -57.60858986155556
Background Trial: 6, reward: -68.62347384366463
Background Trial: 7, reward: -54.849637996716964
Background Trial: 8, reward: 189.21254791399244
Background Trial: 9, reward: 250.3313924265017
Iteration: 6, average_reward: 27.600736371867608, policy_loss: 0.684135, fdm_loss: 0.003384


episode_reward:  10.8
Background Trial: 1, reward: -44.128771945789964
Background Trial: 2, reward: -20.255494108404122
Background Trial: 3, reward: 26.394771421600808
Background Trial: 4, reward: -69.2121303939261
Background Trial: 5, reward: -57.308455831424084
Background Trial: 6, reward: 4.013458821023249
Background Trial: 7, reward: 26.92375277308153
Background Trial: 8, reward: 43.254346823899255
Background Trial: 9, reward: 73.33571753353999
Iteration: 7, average_reward: -1.8869783229332724, policy_loss: 0.874527, fdm_loss: 0.002674


episode_reward: 243.7Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.004538
FDM train: iteration: 1000, fdm_loss: 0.008422
FDM train: iteration: 1500, fdm_loss: 0.004976
FDM train: iteration: 2000, fdm_loss: 0.004240
FDM train: iteration: 2500, fdm_loss: 0.003686
FDM train: iteration: 3000, fdm_loss: 0.004826
FDM train: iteration: 3500, fdm_loss: 0.003944
FDM train: iteration: 4000, fdm_loss: 0.003312
FDM train: iteration: 4500, fdm_loss: 0.003514
FDM train: iteration: 5000, fdm_loss: 0.005054

Background Trial: 1, reward: 2.3607323589904183
Background Trial: 2, reward: -47.05383163250559
Background Trial: 3, reward: 0.7845654639639719
Background Trial: 4, reward: -251.49556894775878
Background Trial: 5, reward: -107.56813929343554
Background Trial: 6, reward: 280.72409650813034
Background Trial: 7, reward: 272.34456703956465
Background Trial: 8, reward: -203.77928562643513
Background Trial: 9, reward: -132.04030024876596
Iteration: 8, average_reward: -20.63590715313907, policy_loss: 0.546850, fdm_loss: 0.002545


episode_reward:  -3.5
Background Trial: 1, reward: -8.01394495026662
Background Trial: 2, reward: -90.68687271358152
Background Trial: 3, reward: 20.550507052833794
Background Trial: 4, reward: 44.44517761031517
Background Trial: 5, reward: -9.935531908154502
Background Trial: 6, reward: 7.860981354936811
Background Trial: 7, reward: 8.139309218897296
Background Trial: 8, reward: -17.488447983883574
Background Trial: 9, reward: 19.318959943416942
Iteration: 9, average_reward: -2.867762486165134, policy_loss: 0.491298, fdm_loss: 0.003891


episode_reward:  -8.2Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.002542
FDM train: iteration: 1000, fdm_loss: 0.004380
FDM train: iteration: 1500, fdm_loss: 0.002151
FDM train: iteration: 2000, fdm_loss: 0.004607
FDM train: iteration: 2500, fdm_loss: 0.003813
FDM train: iteration: 3000, fdm_loss: 0.004736
FDM train: iteration: 3500, fdm_loss: 0.004621
FDM train: iteration: 4000, fdm_loss: 0.004022
FDM train: iteration: 4500, fdm_loss: 0.003536
FDM train: iteration: 5000, fdm_loss: 0.003468

Background Trial: 1, reward: -25.051614864568677
Background Trial: 2, reward: -17.37011512170963
Background Trial: 3, reward: -115.78677082476345
Background Trial: 4, reward: 15.581371003970517
Background Trial: 5, reward: -66.1356007691752
Background Trial: 6, reward: 143.3359391199029
Background Trial: 7, reward: -46.09761139036674
Background Trial: 8, reward: -32.953804661430624
Background Trial: 9, reward: 130.1996880521838
Iteration: 10, average_reward: -1.5865021617730122, policy_loss: 0.899740, fdm_loss: 0.004335


episode_reward: 231.4
Background Trial: 1, reward: 11.72915494390088
Background Trial: 2, reward: 33.20152517088712
Background Trial: 3, reward: -86.54872550727576
Background Trial: 4, reward: -144.12440829867785
Background Trial: 5, reward: 221.87786718871098
Background Trial: 6, reward: -10.276266786317777
Background Trial: 7, reward: -6.605551017407848
Background Trial: 8, reward: 54.503673197593116
Background Trial: 9, reward: -149.29849889041606
Iteration: 11, average_reward: -8.393469999889245, policy_loss: 0.635067, fdm_loss: 0.005343


episode_reward: -34.7Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.003938
FDM train: iteration: 1000, fdm_loss: 0.002930
FDM train: iteration: 1500, fdm_loss: 0.003520
FDM train: iteration: 2000, fdm_loss: 0.003974
FDM train: iteration: 2500, fdm_loss: 0.004245
FDM train: iteration: 3000, fdm_loss: 0.003153
FDM train: iteration: 3500, fdm_loss: 0.003783
FDM train: iteration: 4000, fdm_loss: 0.003079
FDM train: iteration: 4500, fdm_loss: 0.003092
FDM train: iteration: 5000, fdm_loss: 0.003184

Background Trial: 1, reward: 57.72615445686222
Background Trial: 2, reward: -184.89437173106094
Background Trial: 3, reward: 44.804390276114134
Background Trial: 4, reward: -40.67007347391599
Background Trial: 5, reward: -151.08749583728257
Background Trial: 6, reward: 57.16915060045491
Background Trial: 7, reward: -22.646047971394765
Background Trial: 8, reward: 60.725310333603545
Background Trial: 9, reward: -92.2233154712637
Iteration: 12, average_reward: -30.121810979764795, policy_loss: 0.571204, fdm_loss: 0.004786


episode_reward: -17.0
Background Trial: 1, reward: -23.96878468272392
Background Trial: 2, reward: -109.46958756410355
Background Trial: 3, reward: -18.251032064966523
Background Trial: 4, reward: 118.60688286754743
Background Trial: 5, reward: 277.4348884093068
Background Trial: 6, reward: -43.68567558197201
Background Trial: 7, reward: -11.74958884811194
Background Trial: 8, reward: 254.7886660336346
Background Trial: 9, reward: 237.1938114693012
Iteration: 13, average_reward: 75.65550889310134, policy_loss: 0.548431, fdm_loss: 0.003293


episode_reward: 285.1Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.003228
FDM train: iteration: 1000, fdm_loss: 0.003878
FDM train: iteration: 1500, fdm_loss: 0.003879
FDM train: iteration: 2000, fdm_loss: 0.003495
FDM train: iteration: 2500, fdm_loss: 0.003368
FDM train: iteration: 3000, fdm_loss: 0.003996
FDM train: iteration: 3500, fdm_loss: 0.003096
FDM train: iteration: 4000, fdm_loss: 0.002693
FDM train: iteration: 4500, fdm_loss: 0.002934
FDM train: iteration: 5000, fdm_loss: 0.003151

Background Trial: 1, reward: 214.42319649328687
Background Trial: 2, reward: -108.27132387173253
Background Trial: 3, reward: 190.77092070869872
Background Trial: 4, reward: -2.4910931009256814
Background Trial: 5, reward: -57.779004660270125
Background Trial: 6, reward: 2.384341032865052
Background Trial: 7, reward: -28.93223148228155
Background Trial: 8, reward: -54.05981782230325
Background Trial: 9, reward: -21.41374183082715
Iteration: 14, average_reward: 14.959027274056703, policy_loss: 0.463678, fdm_loss: 0.003383


episode_reward: -73.6
Background Trial: 1, reward: -25.971244619192518
Background Trial: 2, reward: -52.05912390217969
Background Trial: 3, reward: -26.65992001936786
Background Trial: 4, reward: 244.78812120986404
Background Trial: 5, reward: -29.1684444503406
Background Trial: 6, reward: 12.24041589162546
Background Trial: 7, reward: 218.82417598234144
Background Trial: 8, reward: -60.40957084785865
Background Trial: 9, reward: 50.64483756926424
Iteration: 15, average_reward: 36.914360757128435, policy_loss: 0.477159, fdm_loss: 0.002482


episode_reward: 282.8Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.004696
FDM train: iteration: 1000, fdm_loss: 0.004874
FDM train: iteration: 1500, fdm_loss: 0.003523
FDM train: iteration: 2000, fdm_loss: 0.003340
FDM train: iteration: 2500, fdm_loss: 0.003835
FDM train: iteration: 3000, fdm_loss: 0.004686
FDM train: iteration: 3500, fdm_loss: 0.004007
FDM train: iteration: 4000, fdm_loss: 0.003228
FDM train: iteration: 4500, fdm_loss: 0.003776
FDM train: iteration: 5000, fdm_loss: 0.004188

Background Trial: 1, reward: 40.19366917479235
Background Trial: 2, reward: -22.881303673918637
Background Trial: 3, reward: -80.595492695391
Background Trial: 4, reward: -5.3827965963406825
Background Trial: 5, reward: 36.88080882879305
Background Trial: 6, reward: 17.922464745544602
Background Trial: 7, reward: 4.020815351256573
Background Trial: 8, reward: -119.0794957307233
Background Trial: 9, reward: -34.61686276191048
Iteration: 16, average_reward: -18.170910373099726, policy_loss: 0.565344, fdm_loss: 0.003140


episode_reward: 246.4
Background Trial: 1, reward: 3.637779378069908
Background Trial: 2, reward: 60.46604731654881
Background Trial: 3, reward: -0.8158647757582003
Background Trial: 4, reward: 7.467024392996166
Background Trial: 5, reward: -4.524195105304955
Background Trial: 6, reward: -84.68418379745081
Background Trial: 7, reward: 288.18613952380986
Background Trial: 8, reward: 3.2459028974571993
Background Trial: 9, reward: -10.019004209320414
Iteration: 17, average_reward: 29.217738402338618, policy_loss: 0.491283, fdm_loss: 0.004748


episode_reward: 324.9Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.004850
FDM train: iteration: 1000, fdm_loss: 0.003860
FDM train: iteration: 1500, fdm_loss: 0.003950
FDM train: iteration: 2000, fdm_loss: 0.002013
FDM train: iteration: 2500, fdm_loss: 0.003949
FDM train: iteration: 3000, fdm_loss: 0.005292
FDM train: iteration: 3500, fdm_loss: 0.004182
FDM train: iteration: 4000, fdm_loss: 0.002948
FDM train: iteration: 4500, fdm_loss: 0.003947
FDM train: iteration: 5000, fdm_loss: 0.003294

Background Trial: 1, reward: -12.955540706897821
Background Trial: 2, reward: 56.33334658859283
Background Trial: 3, reward: 327.00152988897185
Background Trial: 4, reward: -268.46356941519866
Background Trial: 5, reward: 267.6421644605233
Background Trial: 6, reward: 35.06730662190128
Background Trial: 7, reward: -257.7594289359446
Background Trial: 8, reward: -291.90518846972054
Background Trial: 9, reward: 247.81423547751638
Iteration: 18, average_reward: 11.41942838997156, policy_loss: 0.645065, fdm_loss: 0.002288


episode_reward: -89.5
Background Trial: 1, reward: 60.802059390667466
Background Trial: 2, reward: -144.2597524264552
Background Trial: 3, reward: 31.203555961678006
Background Trial: 4, reward: 6.4543847484117265
Background Trial: 5, reward: -351.85049617386625
Background Trial: 6, reward: -6.1294835570159165
Background Trial: 7, reward: -296.646804041581
Background Trial: 8, reward: -112.99588337271659
Background Trial: 9, reward: 55.484235672311456
Iteration: 19, average_reward: -84.21535375539627, policy_loss: 0.612138, fdm_loss: 0.002720


episode_reward: -274.0Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.003384
FDM train: iteration: 1000, fdm_loss: 0.003689
FDM train: iteration: 1500, fdm_loss: 0.004666
FDM train: iteration: 2000, fdm_loss: 0.002825
FDM train: iteration: 2500, fdm_loss: 0.004038
FDM train: iteration: 3000, fdm_loss: 0.002359
FDM train: iteration: 3500, fdm_loss: 0.003883
FDM train: iteration: 4000, fdm_loss: 0.003886
FDM train: iteration: 4500, fdm_loss: 0.003509
FDM train: iteration: 5000, fdm_loss: 0.003174

Background Trial: 1, reward: -4.456471555649401
Background Trial: 2, reward: 41.07061745326948
Background Trial: 3, reward: 259.63789654566983
Background Trial: 4, reward: 28.574159802873453
Background Trial: 5, reward: -325.0843085422882
Background Trial: 6, reward: -289.1782335613842
Background Trial: 7, reward: -17.99106487126228
Background Trial: 8, reward: 2.5621561382114635
Background Trial: 9, reward: -14.91769697819089
Iteration: 20, average_reward: -35.53143839652786, policy_loss: 0.408717, fdm_loss: 0.004009


episode_reward: 264.1
Background Trial: 1, reward: 41.15207031330499
Background Trial: 2, reward: -12.135953868284602
Background Trial: 3, reward: -493.6174278044351
Background Trial: 4, reward: -369.12452559263727
Background Trial: 5, reward: -31.499661354732055
Background Trial: 6, reward: -8.060029229041035
Background Trial: 7, reward: 169.84064774705473
Background Trial: 8, reward: 260.85928705008865
Background Trial: 9, reward: 17.988428782512116
Iteration: 21, average_reward: -47.177462661796625, policy_loss: 0.393545, fdm_loss: 0.003793


episode_reward:  29.6Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.003503
FDM train: iteration: 1000, fdm_loss: 0.003788
FDM train: iteration: 1500, fdm_loss: 0.003987
FDM train: iteration: 2000, fdm_loss: 0.004070
FDM train: iteration: 2500, fdm_loss: 0.004624
FDM train: iteration: 3000, fdm_loss: 0.003242
FDM train: iteration: 3500, fdm_loss: 0.003349
FDM train: iteration: 4000, fdm_loss: 0.003919
FDM train: iteration: 4500, fdm_loss: 0.005287
FDM train: iteration: 5000, fdm_loss: 0.006976

Background Trial: 1, reward: -376.12363154890255
Background Trial: 2, reward: -38.05911077404459
Background Trial: 3, reward: 305.163515579029
Background Trial: 4, reward: -41.56970727027886
Background Trial: 5, reward: 271.07741823420804
Background Trial: 6, reward: -3.153598146494403
Background Trial: 7, reward: 276.0938005351439
Background Trial: 8, reward: -43.25079178170347
Background Trial: 9, reward: 23.215460102911024
Iteration: 22, average_reward: 41.48815054776313, policy_loss: 0.771633, fdm_loss: 0.003443


episode_reward:  48.8
Background Trial: 1, reward: 4.347595707892381
Background Trial: 2, reward: 33.18764792287246
Background Trial: 3, reward: 71.19106549936026
Background Trial: 4, reward: 159.5874766622351
Background Trial: 5, reward: -50.22860332551111
Background Trial: 6, reward: -355.14647631561877
Background Trial: 7, reward: 28.218942469085675
Background Trial: 8, reward: -8.072892273270966
Background Trial: 9, reward: -7.6996824959094
Iteration: 23, average_reward: -13.846102905429374, policy_loss: 0.631158, fdm_loss: 0.004164


episode_reward:  42.0Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.002856
FDM train: iteration: 1000, fdm_loss: 0.003160
FDM train: iteration: 1500, fdm_loss: 0.002965
FDM train: iteration: 2000, fdm_loss: 0.005552
FDM train: iteration: 2500, fdm_loss: 0.004381
FDM train: iteration: 3000, fdm_loss: 0.003939
FDM train: iteration: 3500, fdm_loss: 0.004411
FDM train: iteration: 4000, fdm_loss: 0.002867
FDM train: iteration: 4500, fdm_loss: 0.003656
FDM train: iteration: 5000, fdm_loss: 0.003138

Background Trial: 1, reward: 223.24185385844459
Background Trial: 2, reward: 0.1108357065186425
Background Trial: 3, reward: -51.077899534166534
Background Trial: 4, reward: 290.0560583807903
Background Trial: 5, reward: -702.066695027601
Background Trial: 6, reward: 206.47006513137597
Background Trial: 7, reward: -16.348002843172864
Background Trial: 8, reward: 245.31134925204887
Background Trial: 9, reward: 15.961643244087554
Iteration: 24, average_reward: 23.517689796480617, policy_loss: 0.652186, fdm_loss: 0.004064


episode_reward:  33.1
Background Trial: 1, reward: 43.63956054308795
Background Trial: 2, reward: -589.8922987362679
Background Trial: 3, reward: -28.609112448008602
Background Trial: 4, reward: -371.2242813771603
Background Trial: 5, reward: 306.3006996596853
Background Trial: 6, reward: 232.3213780734644
Background Trial: 7, reward: 23.147313253654033
Background Trial: 8, reward: 188.13704461600838
Background Trial: 9, reward: 1.2053010767003798
Iteration: 25, average_reward: -21.66382170431515, policy_loss: 0.727286, fdm_loss: 0.003572


episode_reward:  18.5Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.003392
FDM train: iteration: 1000, fdm_loss: 0.002894
FDM train: iteration: 1500, fdm_loss: 0.002946
FDM train: iteration: 2000, fdm_loss: 0.003242
FDM train: iteration: 2500, fdm_loss: 0.005998
FDM train: iteration: 3000, fdm_loss: 0.004088
FDM train: iteration: 3500, fdm_loss: 0.003731
FDM train: iteration: 4000, fdm_loss: 0.004507
FDM train: iteration: 4500, fdm_loss: 0.003974
FDM train: iteration: 5000, fdm_loss: 0.002897

Background Trial: 1, reward: -360.3468412186947
Background Trial: 2, reward: -26.67824525157792
Background Trial: 3, reward: -69.82134916179308
Background Trial: 4, reward: -314.95187776391083
Background Trial: 5, reward: 38.09461751684438
Background Trial: 6, reward: -25.625165818475367
Background Trial: 7, reward: -144.0150886461972
Background Trial: 8, reward: -298.57807256209213
Background Trial: 9, reward: 41.659527008955706
Iteration: 26, average_reward: -128.9180550996601, policy_loss: 0.576970, fdm_loss: 0.003468


episode_reward:   7.3
Background Trial: 1, reward: -414.3150618925589
Background Trial: 2, reward: -366.9411897248543
Background Trial: 3, reward: 218.38483491793392
Background Trial: 4, reward: -302.49220205879146
Background Trial: 5, reward: -37.67567628288313
Background Trial: 6, reward: 43.813767246469524
Background Trial: 7, reward: -464.57203778716166
Background Trial: 8, reward: -233.81297234551988
Background Trial: 9, reward: 10.575301543928106
Iteration: 27, average_reward: -171.8928040426042, policy_loss: 0.483944, fdm_loss: 0.003653


episode_reward: -14.5Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.003167
FDM train: iteration: 1000, fdm_loss: 0.004629
FDM train: iteration: 1500, fdm_loss: 0.003821
FDM train: iteration: 2000, fdm_loss: 0.003318
FDM train: iteration: 2500, fdm_loss: 0.003624
FDM train: iteration: 3000, fdm_loss: 0.006394
FDM train: iteration: 3500, fdm_loss: 0.003178
FDM train: iteration: 4000, fdm_loss: 0.005435
FDM train: iteration: 4500, fdm_loss: 0.005887
FDM train: iteration: 5000, fdm_loss: 0.005485

Background Trial: 1, reward: 9.004592074047707
Background Trial: 2, reward: -10.15798973494499
Background Trial: 3, reward: -15.661833800358522
Background Trial: 4, reward: 30.089622292101865
Background Trial: 5, reward: 221.9524370536602
Background Trial: 6, reward: -382.94114952902163
Background Trial: 7, reward: -151.83772153995452
Background Trial: 8, reward: 17.152378290747848
Background Trial: 9, reward: 244.09015300187068
Iteration: 28, average_reward: -4.25661243242793, policy_loss: 0.483206, fdm_loss: 0.003514


episode_reward:  -6.4
Background Trial: 1, reward: -32.491375478190264
Background Trial: 2, reward: 22.62343772348052
Background Trial: 3, reward: 2.8953337484335577
Background Trial: 4, reward: 156.7946470812196
Background Trial: 5, reward: -5.867655051024656
Background Trial: 6, reward: 36.995194935700084
Background Trial: 7, reward: 308.73851526226065
Background Trial: 8, reward: 35.840192565346285
Background Trial: 9, reward: 2.019375942324757
Iteration: 29, average_reward: 58.616407414394516, policy_loss: 0.517310, fdm_loss: 0.004419


episode_reward: 269.9Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.003705
FDM train: iteration: 1000, fdm_loss: 0.003313
FDM train: iteration: 1500, fdm_loss: 0.004440
FDM train: iteration: 2000, fdm_loss: 0.003550
FDM train: iteration: 2500, fdm_loss: 0.004309
FDM train: iteration: 3000, fdm_loss: 0.003588
FDM train: iteration: 3500, fdm_loss: 0.006584
FDM train: iteration: 4000, fdm_loss: 0.002794
FDM train: iteration: 4500, fdm_loss: 0.002879
FDM train: iteration: 5000, fdm_loss: 0.002141

Background Trial: 1, reward: -52.42293486211514
Background Trial: 2, reward: -38.60887820148049
Background Trial: 3, reward: -14.263711554065068
Background Trial: 4, reward: -17.068242190013635
Background Trial: 5, reward: 243.21655977579408
Background Trial: 6, reward: 58.20651114549648
Background Trial: 7, reward: -405.4334161422693
Background Trial: 8, reward: -306.92436170034136
Background Trial: 9, reward: -683.142503775178
Iteration: 30, average_reward: -135.1601086115747, policy_loss: 0.414210, fdm_loss: 0.004730


episode_reward: 321.4
Background Trial: 1, reward: 38.67587986293822
Background Trial: 2, reward: -311.7908616671973
Background Trial: 3, reward: -301.9032673493862
Background Trial: 4, reward: 19.441503402767438
Background Trial: 5, reward: 33.173323844426676
Background Trial: 6, reward: 246.97013674855197
Background Trial: 7, reward: -418.1340355694689
Background Trial: 8, reward: 13.546412502506342
Background Trial: 9, reward: 79.74656260542747
Iteration: 31, average_reward: -66.69714951327047, policy_loss: 0.506277, fdm_loss: 0.002836


episode_reward:  39.9Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.003904
FDM train: iteration: 1000, fdm_loss: 0.003829
FDM train: iteration: 1500, fdm_loss: 0.006119
FDM train: iteration: 2000, fdm_loss: 0.005658
FDM train: iteration: 2500, fdm_loss: 0.003423
FDM train: iteration: 3000, fdm_loss: 0.005391
FDM train: iteration: 3500, fdm_loss: 0.004988
FDM train: iteration: 4000, fdm_loss: 0.004021
FDM train: iteration: 4500, fdm_loss: 0.005074
FDM train: iteration: 5000, fdm_loss: 0.004153

Background Trial: 1, reward: -41.50202982078629
Background Trial: 2, reward: 255.09328813426015
Background Trial: 3, reward: 36.343314229502795
Background Trial: 4, reward: 35.91177798588288
Background Trial: 5, reward: 45.59876523067487
Background Trial: 6, reward: 257.91529883648604
Background Trial: 7, reward: 18.753849850022064
Background Trial: 8, reward: 24.712302053320414
Background Trial: 9, reward: -44.6837507700815
Iteration: 32, average_reward: 65.34920174769795, policy_loss: 0.522909, fdm_loss: 0.003253


episode_reward: -68.0
Background Trial: 1, reward: -201.5045050910827
Background Trial: 2, reward: 209.11684876676094
Background Trial: 3, reward: 28.92643001021034
Background Trial: 4, reward: -415.6455865704863
Background Trial: 5, reward: 43.359793202154975
Background Trial: 6, reward: -16.841068046299185
Background Trial: 7, reward: -219.62441331574178
Background Trial: 8, reward: -95.22921832463516
Background Trial: 9, reward: -17.29548498559518
Iteration: 33, average_reward: -76.08191159496825, policy_loss: 0.575482, fdm_loss: 0.005212

