Policy train: iteration: 500, policy_loss: 0.208840
Policy train: iteration: 1000, policy_loss: 0.099990
Policy train: iteration: 1500, policy_loss: 0.051773
Policy train: iteration: 2000, policy_loss: 0.208031
Policy train: iteration: 2500, policy_loss: 0.161092
Policy train: iteration: 3000, policy_loss: 0.079338
Policy train: iteration: 3500, policy_loss: 0.084530
Policy train: iteration: 4000, policy_loss: 0.094748

Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 200.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 1, average_reward: 200.0

Policy train: iteration: 500, policy_loss: 0.068250
Policy train: iteration: 1000, policy_loss: 0.241553
Policy train: iteration: 1500, policy_loss: 0.073622
Policy train: iteration: 2000, policy_loss: 0.064136
Policy train: iteration: 2500, policy_loss: 0.075817
Policy train: iteration: 3000, policy_loss: 0.066648
Policy train: iteration: 3500, policy_loss: 0.134238
Policy train: iteration: 4000, policy_loss: 0.154449

Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 200.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 2, average_reward: 200.0

Policy train: iteration: 500, policy_loss: 0.115382
Policy train: iteration: 1000, policy_loss: 0.112117
Policy train: iteration: 1500, policy_loss: 0.133215
Policy train: iteration: 2000, policy_loss: 0.084309
Policy train: iteration: 2500, policy_loss: 0.128694
Policy train: iteration: 3000, policy_loss: 0.177780
Policy train: iteration: 3500, policy_loss: 0.111726
Policy train: iteration: 4000, policy_loss: 0.141626

Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 200.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 3, average_reward: 200.0

Policy train: iteration: 500, policy_loss: 0.041077
Policy train: iteration: 1000, policy_loss: 0.268345
Policy train: iteration: 1500, policy_loss: 0.172677
Policy train: iteration: 2000, policy_loss: 0.160216
Policy train: iteration: 2500, policy_loss: 0.047456
Policy train: iteration: 3000, policy_loss: 0.110771
Policy train: iteration: 3500, policy_loss: 0.067117
Policy train: iteration: 4000, policy_loss: 0.112041

Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 200.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 4, average_reward: 200.0

Policy train: iteration: 500, policy_loss: 0.082207
Policy train: iteration: 1000, policy_loss: 0.060914
Policy train: iteration: 1500, policy_loss: 0.081994
Policy train: iteration: 2000, policy_loss: 0.092188
Policy train: iteration: 2500, policy_loss: 0.083309
Policy train: iteration: 3000, policy_loss: 0.160543
Policy train: iteration: 3500, policy_loss: 0.032677
Policy train: iteration: 4000, policy_loss: 0.046254

Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 200.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 5, average_reward: 200.0

Policy train: iteration: 500, policy_loss: 0.137543
Policy train: iteration: 1000, policy_loss: 0.017482
Policy train: iteration: 1500, policy_loss: 0.181479
Policy train: iteration: 2000, policy_loss: 0.088109
Policy train: iteration: 2500, policy_loss: 0.075686
Policy train: iteration: 3000, policy_loss: 0.147245
Policy train: iteration: 3500, policy_loss: 0.113580
Policy train: iteration: 4000, policy_loss: 0.065175

Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 200.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 6, average_reward: 200.0

Policy train: iteration: 500, policy_loss: 0.187429
Policy train: iteration: 1000, policy_loss: 0.016448
Policy train: iteration: 1500, policy_loss: 0.019454
Policy train: iteration: 2000, policy_loss: 0.033244
Policy train: iteration: 2500, policy_loss: 0.026351
Policy train: iteration: 3000, policy_loss: 0.031798
Policy train: iteration: 3500, policy_loss: 0.075860
Policy train: iteration: 4000, policy_loss: 0.025465

Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 200.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 7, average_reward: 200.0

Policy train: iteration: 500, policy_loss: 0.161709
Policy train: iteration: 1000, policy_loss: 0.043427
Policy train: iteration: 1500, policy_loss: 0.194994
Policy train: iteration: 2000, policy_loss: 0.012313
Policy train: iteration: 2500, policy_loss: 0.124449
Policy train: iteration: 3000, policy_loss: 0.024344
Policy train: iteration: 3500, policy_loss: 0.017594
Policy train: iteration: 4000, policy_loss: 0.087826

Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 200.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 8, average_reward: 200.0

Policy train: iteration: 500, policy_loss: 0.083154
Policy train: iteration: 1000, policy_loss: 0.084403
Policy train: iteration: 1500, policy_loss: 0.091858
Policy train: iteration: 2000, policy_loss: 0.088615
Policy train: iteration: 2500, policy_loss: 0.107004
Policy train: iteration: 3000, policy_loss: 0.096981
Policy train: iteration: 3500, policy_loss: 0.027298
Policy train: iteration: 4000, policy_loss: 0.118146

Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 200.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 9, average_reward: 200.0

Policy train: iteration: 500, policy_loss: 0.025242
Policy train: iteration: 1000, policy_loss: 0.139195
Policy train: iteration: 1500, policy_loss: 0.050796
Policy train: iteration: 2000, policy_loss: 0.031103
Policy train: iteration: 2500, policy_loss: 0.066258
Policy train: iteration: 3000, policy_loss: 0.078140
Policy train: iteration: 3500, policy_loss: 0.018311
Policy train: iteration: 4000, policy_loss: 0.009110

Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 200.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 10, average_reward: 200.0

Policy train: iteration: 500, policy_loss: 0.053895
Policy train: iteration: 1000, policy_loss: 0.044697
Policy train: iteration: 1500, policy_loss: 0.004724
Policy train: iteration: 2000, policy_loss: 0.016119
Policy train: iteration: 2500, policy_loss: 0.004043
Policy train: iteration: 3000, policy_loss: 0.031651
Policy train: iteration: 3500, policy_loss: 0.016239
Policy train: iteration: 4000, policy_loss: 0.005352

Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 200.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 11, average_reward: 200.0

Policy train: iteration: 500, policy_loss: 0.025736
Policy train: iteration: 1000, policy_loss: 0.010404
Policy train: iteration: 1500, policy_loss: 0.049048
Policy train: iteration: 2000, policy_loss: 0.012738
Policy train: iteration: 2500, policy_loss: 0.042966
Policy train: iteration: 3000, policy_loss: 0.024049
Policy train: iteration: 3500, policy_loss: 0.013627
Policy train: iteration: 4000, policy_loss: 0.024117

Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 200.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 12, average_reward: 200.0

Policy train: iteration: 500, policy_loss: 0.010634
Policy train: iteration: 1000, policy_loss: 0.056146
Policy train: iteration: 1500, policy_loss: 0.011860
Policy train: iteration: 2000, policy_loss: 0.008313
Policy train: iteration: 2500, policy_loss: 0.064463
Policy train: iteration: 3000, policy_loss: 0.033834
Policy train: iteration: 3500, policy_loss: 0.028741
Policy train: iteration: 4000, policy_loss: 0.005871

Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 167.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 200.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 13, average_reward: 196.33333333333334

Policy train: iteration: 500, policy_loss: 0.008863
Policy train: iteration: 1000, policy_loss: 0.051439
Policy train: iteration: 1500, policy_loss: 0.009077
Policy train: iteration: 2000, policy_loss: 0.085934
Policy train: iteration: 2500, policy_loss: 0.007476
Policy train: iteration: 3000, policy_loss: 0.005628
Policy train: iteration: 3500, policy_loss: 0.002827
Policy train: iteration: 4000, policy_loss: 0.045213

Background Trial: 1, reward: 140.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 138.0
Background Trial: 4, reward: 163.0
Background Trial: 5, reward: 128.0
Background Trial: 6, reward: 156.0
Background Trial: 7, reward: 118.0
Background Trial: 8, reward: 127.0
Background Trial: 9, reward: 200.0
Iteration: 14, average_reward: 152.22222222222223

Policy train: iteration: 500, policy_loss: 0.055815
Policy train: iteration: 1000, policy_loss: 0.010191
Policy train: iteration: 1500, policy_loss: 0.014259
Policy train: iteration: 2000, policy_loss: 0.020784
Policy train: iteration: 2500, policy_loss: 0.009012
Policy train: iteration: 3000, policy_loss: 0.009135
Policy train: iteration: 3500, policy_loss: 0.005083
Policy train: iteration: 4000, policy_loss: 0.003819

Background Trial: 1, reward: 162.0
Background Trial: 2, reward: 133.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 121.0
Background Trial: 5, reward: 160.0
Background Trial: 6, reward: 159.0
Background Trial: 7, reward: 136.0
Background Trial: 8, reward: 174.0
Background Trial: 9, reward: 185.0
Iteration: 15, average_reward: 158.88888888888889

Policy train: iteration: 500, policy_loss: 0.019580
Policy train: iteration: 1000, policy_loss: 0.067598
Policy train: iteration: 1500, policy_loss: 0.004631
Policy train: iteration: 2000, policy_loss: 0.002887
Policy train: iteration: 2500, policy_loss: 0.007671
Policy train: iteration: 3000, policy_loss: 0.032801
Policy train: iteration: 3500, policy_loss: 0.002741
Policy train: iteration: 4000, policy_loss: 0.031507

Background Trial: 1, reward: 127.0
Background Trial: 2, reward: 140.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 145.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 137.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 16, average_reward: 172.11111111111111

Policy train: iteration: 500, policy_loss: 0.001781
Policy train: iteration: 1000, policy_loss: 0.099781
Policy train: iteration: 1500, policy_loss: 0.017762
Policy train: iteration: 2000, policy_loss: 0.022234
Policy train: iteration: 2500, policy_loss: 0.051989
Policy train: iteration: 3000, policy_loss: 0.001367
Policy train: iteration: 3500, policy_loss: 0.032534
Policy train: iteration: 4000, policy_loss: 0.015781

Background Trial: 1, reward: 165.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 131.0
Background Trial: 4, reward: 142.0
Background Trial: 5, reward: 120.0
Background Trial: 6, reward: 118.0
Background Trial: 7, reward: 133.0
Background Trial: 8, reward: 175.0
Background Trial: 9, reward: 175.0
Iteration: 17, average_reward: 151.0

Policy train: iteration: 500, policy_loss: 0.049878
Policy train: iteration: 1000, policy_loss: 0.013776
Policy train: iteration: 1500, policy_loss: 0.002140
Policy train: iteration: 2000, policy_loss: 0.018784
Policy train: iteration: 2500, policy_loss: 0.103099
Policy train: iteration: 3000, policy_loss: 0.013302
Policy train: iteration: 3500, policy_loss: 0.002758
Policy train: iteration: 4000, policy_loss: 0.051197

Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 200.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 118.0
Iteration: 18, average_reward: 190.88888888888889

Policy train: iteration: 500, policy_loss: 0.038690
Policy train: iteration: 1000, policy_loss: 0.040374
Policy train: iteration: 1500, policy_loss: 0.016236
Policy train: iteration: 2000, policy_loss: 0.019164
Policy train: iteration: 2500, policy_loss: 0.005015
Policy train: iteration: 3000, policy_loss: 0.040915
Policy train: iteration: 3500, policy_loss: 0.038588
Policy train: iteration: 4000, policy_loss: 0.007000

Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 200.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 19, average_reward: 200.0

Policy train: iteration: 500, policy_loss: 0.051162
Policy train: iteration: 1000, policy_loss: 0.047455
Policy train: iteration: 1500, policy_loss: 0.011236
Policy train: iteration: 2000, policy_loss: 0.022209
Policy train: iteration: 2500, policy_loss: 0.001167
Policy train: iteration: 3000, policy_loss: 0.023155
Policy train: iteration: 3500, policy_loss: 0.000702
Policy train: iteration: 4000, policy_loss: 0.028924

Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 196.0
Background Trial: 3, reward: 117.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 184.0
Background Trial: 6, reward: 114.0
Background Trial: 7, reward: 117.0
Background Trial: 8, reward: 152.0
Background Trial: 9, reward: 145.0
Iteration: 20, average_reward: 158.33333333333334

Policy train: iteration: 500, policy_loss: 0.036937
Policy train: iteration: 1000, policy_loss: 0.027762
Policy train: iteration: 1500, policy_loss: 0.008604
Policy train: iteration: 2000, policy_loss: 0.005968
Policy train: iteration: 2500, policy_loss: 0.018388
Policy train: iteration: 3000, policy_loss: 0.002382
Policy train: iteration: 3500, policy_loss: 0.011094
Policy train: iteration: 4000, policy_loss: 0.003723

Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 200.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 21, average_reward: 200.0

Policy train: iteration: 500, policy_loss: 0.010179
Policy train: iteration: 1000, policy_loss: 0.010467
Policy train: iteration: 1500, policy_loss: 0.010969
Policy train: iteration: 2000, policy_loss: 0.007323
Policy train: iteration: 2500, policy_loss: 0.012681
