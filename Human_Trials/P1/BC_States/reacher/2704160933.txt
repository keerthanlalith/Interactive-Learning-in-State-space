Policy train: iteration: 500, policy_loss: 0.010335
Policy train: iteration: 1000, policy_loss: 0.006938
Policy train: iteration: 1500, policy_loss: 0.003967
Policy train: iteration: 2000, policy_loss: 0.008554
Policy train: iteration: 2500, policy_loss: 0.005212
Policy train: iteration: 3000, policy_loss: 0.002986
Policy train: iteration: 3500, policy_loss: 0.006455
Policy train: iteration: 4000, policy_loss: 0.006872
Policy train: iteration: 4500, policy_loss: 0.004063
Policy train: iteration: 5000, policy_loss: 0.007548
Policy train: iteration: 5500, policy_loss: 0.008209
Policy train: iteration: 6000, policy_loss: 0.005939

Background Trial: 1, reward: -43.05681197911629
Background Trial: 2, reward: -42.89900187962684
Background Trial: 3, reward: -42.77307461380362
Background Trial: 4, reward: -43.68521400892601
Background Trial: 5, reward: -42.83613524863488
Background Trial: 6, reward: -43.212647007922705
Background Trial: 7, reward: -42.94163592389279
Background Trial: 8, reward: -43.56633328217211
Background Trial: 9, reward: -43.127824327127534
Iteration: 1, average_reward: -43.12207536346919

Policy train: iteration: 500, policy_loss: 0.006363
Policy train: iteration: 1000, policy_loss: 0.008289
Policy train: iteration: 1500, policy_loss: 0.006596
Policy train: iteration: 2000, policy_loss: 0.006569
Policy train: iteration: 2500, policy_loss: 0.005250
Policy train: iteration: 3000, policy_loss: 0.005063
Policy train: iteration: 3500, policy_loss: 0.006769
Policy train: iteration: 4000, policy_loss: 0.006666
Policy train: iteration: 4500, policy_loss: 0.006740
Policy train: iteration: 5000, policy_loss: 0.006128
Policy train: iteration: 5500, policy_loss: 0.008158
Policy train: iteration: 6000, policy_loss: 0.004514

Background Trial: 1, reward: -36.745533273082685
Background Trial: 2, reward: -36.61183563636369
Background Trial: 3, reward: -42.087358738078485
Background Trial: 4, reward: -36.920294893998935
Background Trial: 5, reward: -36.74473466775671
Background Trial: 6, reward: -41.64604677893155
Background Trial: 7, reward: -42.32522669584813
Background Trial: 8, reward: -41.853167179594585
Background Trial: 9, reward: -41.72416116991972
Iteration: 2, average_reward: -39.628706559286066

Policy train: iteration: 500, policy_loss: 0.006594
Policy train: iteration: 1000, policy_loss: 0.006667
Policy train: iteration: 1500, policy_loss: 0.004551
Policy train: iteration: 2000, policy_loss: 0.006401
Policy train: iteration: 2500, policy_loss: 0.007267
Policy train: iteration: 3000, policy_loss: 0.003875
Policy train: iteration: 3500, policy_loss: 0.007738
Policy train: iteration: 4000, policy_loss: 0.006133
Policy train: iteration: 4500, policy_loss: 0.007446
Policy train: iteration: 5000, policy_loss: 0.006833
Policy train: iteration: 5500, policy_loss: 0.007770
Policy train: iteration: 6000, policy_loss: 0.005296

Background Trial: 1, reward: -45.10983959867042
Background Trial: 2, reward: -44.05905548132961
Background Trial: 3, reward: -47.41471042347311
Background Trial: 4, reward: -45.06326567419444
Background Trial: 5, reward: -44.51140125267801
Background Trial: 6, reward: -43.86395398559267
Background Trial: 7, reward: -36.67875720113976
Background Trial: 8, reward: -44.93301061895748
Background Trial: 9, reward: -43.448325404550374
Iteration: 3, average_reward: -43.89803551562065

Policy train: iteration: 500, policy_loss: 0.005158
Policy train: iteration: 1000, policy_loss: 0.007648
Policy train: iteration: 1500, policy_loss: 0.004386
Policy train: iteration: 2000, policy_loss: 0.005726
Policy train: iteration: 2500, policy_loss: 0.007826
Policy train: iteration: 3000, policy_loss: 0.007309
Policy train: iteration: 3500, policy_loss: 0.003840
Policy train: iteration: 4000, policy_loss: 0.004895
Policy train: iteration: 4500, policy_loss: 0.006705
Policy train: iteration: 5000, policy_loss: 0.006152
Policy train: iteration: 5500, policy_loss: 0.006352
Policy train: iteration: 6000, policy_loss: 0.004573

Background Trial: 1, reward: -93.25232714680043
Background Trial: 2, reward: -42.7876552775981
Background Trial: 3, reward: -43.6409464841225
Background Trial: 4, reward: -93.62029756247522
Background Trial: 5, reward: -42.66819571073044
Background Trial: 6, reward: -41.77321266140143
Background Trial: 7, reward: -93.19159627620633
Background Trial: 8, reward: -44.55844339685742
Background Trial: 9, reward: -43.04914668419377
Iteration: 4, average_reward: -59.83798013337618

Policy train: iteration: 500, policy_loss: 0.004861
Policy train: iteration: 1000, policy_loss: 0.007381
Policy train: iteration: 1500, policy_loss: 0.004765
Policy train: iteration: 2000, policy_loss: 0.005201
Policy train: iteration: 2500, policy_loss: 0.008211
Policy train: iteration: 3000, policy_loss: 0.005772
Policy train: iteration: 3500, policy_loss: 0.005961
Policy train: iteration: 4000, policy_loss: 0.006305
Policy train: iteration: 4500, policy_loss: 0.002919
Policy train: iteration: 5000, policy_loss: 0.006104
Policy train: iteration: 5500, policy_loss: 0.004664
Policy train: iteration: 6000, policy_loss: 0.004957

Background Trial: 1, reward: -99.06856411963837
Background Trial: 2, reward: -53.95697226566932
Background Trial: 3, reward: -95.38109990819511
Background Trial: 4, reward: -53.413611070928326
Background Trial: 5, reward: -97.96165669192932
Background Trial: 6, reward: -51.725316000535045
Background Trial: 7, reward: -56.45064298448954
Background Trial: 8, reward: -98.35099473764566
Background Trial: 9, reward: -94.87264909144378
Iteration: 5, average_reward: -77.90905631894161

Policy train: iteration: 500, policy_loss: 0.007369
Policy train: iteration: 1000, policy_loss: 0.006361
Policy train: iteration: 1500, policy_loss: 0.003848
Policy train: iteration: 2000, policy_loss: 0.005050
Policy train: iteration: 2500, policy_loss: 0.005704
Policy train: iteration: 3000, policy_loss: 0.005736
Policy train: iteration: 3500, policy_loss: 0.005749
Policy train: iteration: 4000, policy_loss: 0.006276
Policy train: iteration: 4500, policy_loss: 0.004934
Policy train: iteration: 5000, policy_loss: 0.006599
Policy train: iteration: 5500, policy_loss: 0.004673
Policy train: iteration: 6000, policy_loss: 0.004686

Background Trial: 1, reward: -43.367897386116205
Background Trial: 2, reward: -41.654943462095204
Background Trial: 3, reward: -104.28667239844938
Background Trial: 4, reward: -39.058901094280856
Background Trial: 5, reward: -42.36873296243517
Background Trial: 6, reward: -43.034970324649386
Background Trial: 7, reward: -101.643014945115
Background Trial: 8, reward: -38.8305897628749
Background Trial: 9, reward: -42.323654102795594
Iteration: 6, average_reward: -55.17437515986798

Policy train: iteration: 500, policy_loss: 0.005751
Policy train: iteration: 1000, policy_loss: 0.005327
Policy train: iteration: 1500, policy_loss: 0.003420
Policy train: iteration: 2000, policy_loss: 0.004050
Policy train: iteration: 2500, policy_loss: 0.004679
Policy train: iteration: 3000, policy_loss: 0.006797
Policy train: iteration: 3500, policy_loss: 0.004392
Policy train: iteration: 4000, policy_loss: 0.005358
Policy train: iteration: 4500, policy_loss: 0.003934
Policy train: iteration: 5000, policy_loss: 0.008141
Policy train: iteration: 5500, policy_loss: 0.004648
Policy train: iteration: 6000, policy_loss: 0.006623

Background Trial: 1, reward: -40.325389444936874
Background Trial: 2, reward: -39.168784708402825
Background Trial: 3, reward: -60.84128123784035
Background Trial: 4, reward: -39.862524139388185
Background Trial: 5, reward: -41.6088891565795
Background Trial: 6, reward: -33.219571163441074
Background Trial: 7, reward: -39.17598667524989
Background Trial: 8, reward: -31.842610202299912
Background Trial: 9, reward: -38.78791482724136
Iteration: 7, average_reward: -40.53699461726444

Policy train: iteration: 500, policy_loss: 0.004825
Policy train: iteration: 1000, policy_loss: 0.003851
Policy train: iteration: 1500, policy_loss: 0.006193
Policy train: iteration: 2000, policy_loss: 0.005128
Policy train: iteration: 2500, policy_loss: 0.006585
Policy train: iteration: 3000, policy_loss: 0.005868
Policy train: iteration: 3500, policy_loss: 0.004003
Policy train: iteration: 4000, policy_loss: 0.005417
Policy train: iteration: 4500, policy_loss: 0.006113
Policy train: iteration: 5000, policy_loss: 0.005609
Policy train: iteration: 5500, policy_loss: 0.005378
Policy train: iteration: 6000, policy_loss: 0.005569

Background Trial: 1, reward: -51.066822391582214
Background Trial: 2, reward: -42.9466445209307
Background Trial: 3, reward: -41.89610951614921
Background Trial: 4, reward: -52.2528749213092
Background Trial: 5, reward: -52.65820552977848
Background Trial: 6, reward: -50.65915191283359
Background Trial: 7, reward: -46.35244569556545
Background Trial: 8, reward: -52.19182885289015
Background Trial: 9, reward: -44.30506789705625
Iteration: 8, average_reward: -48.25879458201058

Policy train: iteration: 500, policy_loss: 0.003945
Policy train: iteration: 1000, policy_loss: 0.005142
Policy train: iteration: 1500, policy_loss: 0.006582
Policy train: iteration: 2000, policy_loss: 0.007385
Policy train: iteration: 2500, policy_loss: 0.004986
Policy train: iteration: 3000, policy_loss: 0.006685
Policy train: iteration: 3500, policy_loss: 0.006557
Policy train: iteration: 4000, policy_loss: 0.005813
Policy train: iteration: 4500, policy_loss: 0.005764
Policy train: iteration: 5000, policy_loss: 0.004364
Policy train: iteration: 5500, policy_loss: 0.007328
Policy train: iteration: 6000, policy_loss: 0.004798

Background Trial: 1, reward: -37.35049668374209
Background Trial: 2, reward: -37.57133297262607
Background Trial: 3, reward: -37.94070162119534
Background Trial: 4, reward: -38.84323288987317
Background Trial: 5, reward: -36.49097042950295
Background Trial: 6, reward: -38.20719172851141
Background Trial: 7, reward: -61.612771588618564
Background Trial: 8, reward: -60.80519956797181
Background Trial: 9, reward: -38.23082945868708
Iteration: 9, average_reward: -43.00585854896983

Policy train: iteration: 500, policy_loss: 0.004871
Policy train: iteration: 1000, policy_loss: 0.005929
Policy train: iteration: 1500, policy_loss: 0.005158
Policy train: iteration: 2000, policy_loss: 0.003526
Policy train: iteration: 2500, policy_loss: 0.003925
Policy train: iteration: 3000, policy_loss: 0.002759
Policy train: iteration: 3500, policy_loss: 0.004351
Policy train: iteration: 4000, policy_loss: 0.005205
Policy train: iteration: 4500, policy_loss: 0.003276
Policy train: iteration: 5000, policy_loss: 0.004081
Policy train: iteration: 5500, policy_loss: 0.004780
Policy train: iteration: 6000, policy_loss: 0.002103

Background Trial: 1, reward: -47.08344993444877
Background Trial: 2, reward: -47.584701907575656
Background Trial: 3, reward: -47.27304345095367
Background Trial: 4, reward: -43.56624314259747
Background Trial: 5, reward: -47.537966849705825
Background Trial: 6, reward: -47.27003406860983
Background Trial: 7, reward: -47.40380965812731
Background Trial: 8, reward: -47.67529512880334
Background Trial: 9, reward: -47.33428413760909
Iteration: 10, average_reward: -46.96986980871455

Policy train: iteration: 500, policy_loss: 0.006801
Policy train: iteration: 1000, policy_loss: 0.004443
Policy train: iteration: 1500, policy_loss: 0.006222
