Policy train: iteration: 500, policy_loss: 0.383234
Policy train: iteration: 1000, policy_loss: 0.436909
Policy train: iteration: 1500, policy_loss: 0.360380
Policy train: iteration: 2000, policy_loss: 0.392724
Policy train: iteration: 2500, policy_loss: 0.424336
Policy train: iteration: 3000, policy_loss: 0.419969
Policy train: iteration: 3500, policy_loss: 0.360933
Policy train: iteration: 4000, policy_loss: 0.399238
Policy train: iteration: 4500, policy_loss: 0.425240
Policy train: iteration: 5000, policy_loss: 0.420307
Policy train: iteration: 5500, policy_loss: 0.426250
Policy train: iteration: 6000, policy_loss: 0.350202
Policy train: iteration: 6500, policy_loss: 0.417020
Policy train: iteration: 7000, policy_loss: 0.355843
Policy train: iteration: 7500, policy_loss: 0.371939
Policy train: iteration: 8000, policy_loss: 0.362301
Policy train: iteration: 8500, policy_loss: 0.314664
Policy train: iteration: 9000, policy_loss: 0.307958

Background Trial: 1, reward: 8.341931710373558
Background Trial: 2, reward: -168.58748418495605
Background Trial: 3, reward: -218.613801796563
Background Trial: 4, reward: -8.821751722165885
Background Trial: 5, reward: -51.03822395787098
Background Trial: 6, reward: -143.75434610937745
Background Trial: 7, reward: 32.58328598515257
Background Trial: 8, reward: 9.354984378536443
Background Trial: 9, reward: -172.23942767861874
Iteration: 1, average_reward: -79.19720370838773

Policy train: iteration: 500, policy_loss: 0.317390
Policy train: iteration: 1000, policy_loss: 0.328582
Policy train: iteration: 1500, policy_loss: 0.276897
Policy train: iteration: 2000, policy_loss: 0.312017
Policy train: iteration: 2500, policy_loss: 0.290025
Policy train: iteration: 3000, policy_loss: 0.303927
Policy train: iteration: 3500, policy_loss: 0.314463
Policy train: iteration: 4000, policy_loss: 0.289995
Policy train: iteration: 4500, policy_loss: 0.313029
Policy train: iteration: 5000, policy_loss: 0.319355
Policy train: iteration: 5500, policy_loss: 0.289224
Policy train: iteration: 6000, policy_loss: 0.296674
Policy train: iteration: 6500, policy_loss: 0.313149
Policy train: iteration: 7000, policy_loss: 0.269972
Policy train: iteration: 7500, policy_loss: 0.300937
Policy train: iteration: 8000, policy_loss: 0.327558
Policy train: iteration: 8500, policy_loss: 0.295104
Policy train: iteration: 9000, policy_loss: 0.301075

Background Trial: 1, reward: 292.63927535170654
Background Trial: 2, reward: 29.208441046575558
Background Trial: 3, reward: -135.8242496733556
Background Trial: 4, reward: 37.8768089659128
Background Trial: 5, reward: -236.00306501464968
Background Trial: 6, reward: 162.25013074430083
Background Trial: 7, reward: -254.4643067133396
Background Trial: 8, reward: -131.8970075293478
Background Trial: 9, reward: -233.24789077606863
Iteration: 2, average_reward: -52.16242928869618

Policy train: iteration: 500, policy_loss: 0.305544
Policy train: iteration: 1000, policy_loss: 0.341815
Policy train: iteration: 1500, policy_loss: 0.291013
Policy train: iteration: 2000, policy_loss: 0.313795
Policy train: iteration: 2500, policy_loss: 0.303374
Policy train: iteration: 3000, policy_loss: 0.300240
Policy train: iteration: 3500, policy_loss: 0.302475
Policy train: iteration: 4000, policy_loss: 0.289317
Policy train: iteration: 4500, policy_loss: 0.246630
Policy train: iteration: 5000, policy_loss: 0.272460
Policy train: iteration: 5500, policy_loss: 0.312826
Policy train: iteration: 6000, policy_loss: 0.267695
Policy train: iteration: 6500, policy_loss: 0.318334
Policy train: iteration: 7000, policy_loss: 0.304677
Policy train: iteration: 7500, policy_loss: 0.286624
Policy train: iteration: 8000, policy_loss: 0.254041
Policy train: iteration: 8500, policy_loss: 0.282533
Policy train: iteration: 9000, policy_loss: 0.281597

Background Trial: 1, reward: -122.49162950136007
Background Trial: 2, reward: 223.7705805053751
Background Trial: 3, reward: -139.82527630414384
Background Trial: 4, reward: -141.19729952015328
Background Trial: 5, reward: -50.842483370653284
Background Trial: 6, reward: -8.882049431561342
Background Trial: 7, reward: 17.623985062560465
Background Trial: 8, reward: -131.67516996196176
Background Trial: 9, reward: 39.86151990021702
Iteration: 3, average_reward: -34.85086918018678

Policy train: iteration: 500, policy_loss: 0.260669
Policy train: iteration: 1000, policy_loss: 0.293056
Policy train: iteration: 1500, policy_loss: 0.260922
Policy train: iteration: 2000, policy_loss: 0.242416
Policy train: iteration: 2500, policy_loss: 0.247863
Policy train: iteration: 3000, policy_loss: 0.299909
Policy train: iteration: 3500, policy_loss: 0.244399
Policy train: iteration: 4000, policy_loss: 0.212545
Policy train: iteration: 4500, policy_loss: 0.254135
Policy train: iteration: 5000, policy_loss: 0.281408
Policy train: iteration: 5500, policy_loss: 0.230057
Policy train: iteration: 6000, policy_loss: 0.212436
Policy train: iteration: 6500, policy_loss: 0.238682
Policy train: iteration: 7000, policy_loss: 0.251714
Policy train: iteration: 7500, policy_loss: 0.272908
Policy train: iteration: 8000, policy_loss: 0.257627
Policy train: iteration: 8500, policy_loss: 0.311862
Policy train: iteration: 9000, policy_loss: 0.211272

Background Trial: 1, reward: -253.0201898091625
Background Trial: 2, reward: -0.03714969552748926
Background Trial: 3, reward: 231.6934707510715
Background Trial: 4, reward: -113.60981306752777
Background Trial: 5, reward: -103.01170532270936
Background Trial: 6, reward: -91.25023484166941
Background Trial: 7, reward: -194.57217591294025
Background Trial: 8, reward: -106.96497192634413
Background Trial: 9, reward: -102.57978049631883
Iteration: 4, average_reward: -81.48361670234758

Policy train: iteration: 500, policy_loss: 0.221411
Policy train: iteration: 1000, policy_loss: 0.232997
Policy train: iteration: 1500, policy_loss: 0.187493
Policy train: iteration: 2000, policy_loss: 0.273235
Policy train: iteration: 2500, policy_loss: 0.251158
Policy train: iteration: 3000, policy_loss: 0.311809
Policy train: iteration: 3500, policy_loss: 0.283555
Policy train: iteration: 4000, policy_loss: 0.227567
Policy train: iteration: 4500, policy_loss: 0.239294
Policy train: iteration: 5000, policy_loss: 0.199203
Policy train: iteration: 5500, policy_loss: 0.234699
Policy train: iteration: 6000, policy_loss: 0.215063
Policy train: iteration: 6500, policy_loss: 0.193794
Policy train: iteration: 7000, policy_loss: 0.261024
Policy train: iteration: 7500, policy_loss: 0.246969
Policy train: iteration: 8000, policy_loss: 0.233227
Policy train: iteration: 8500, policy_loss: 0.235132
Policy train: iteration: 9000, policy_loss: 0.241964

Background Trial: 1, reward: -40.11196539405452
Background Trial: 2, reward: -64.08813015222901
Background Trial: 3, reward: -64.90006487880684
Background Trial: 4, reward: 254.74923264464596
Background Trial: 5, reward: -415.43962341362544
Background Trial: 6, reward: -256.9345170744052
Background Trial: 7, reward: -229.60891659071444
Background Trial: 8, reward: -131.2547356267488
Background Trial: 9, reward: 1.7143949509121938
Iteration: 5, average_reward: -105.09714728166958

Policy train: iteration: 500, policy_loss: 0.226961
Policy train: iteration: 1000, policy_loss: 0.248721
Policy train: iteration: 1500, policy_loss: 0.203442
Policy train: iteration: 2000, policy_loss: 0.193834
Policy train: iteration: 2500, policy_loss: 0.247270
Policy train: iteration: 3000, policy_loss: 0.277174
Policy train: iteration: 3500, policy_loss: 0.233701
Policy train: iteration: 4000, policy_loss: 0.248906
Policy train: iteration: 4500, policy_loss: 0.231040
Policy train: iteration: 5000, policy_loss: 0.215132
Policy train: iteration: 5500, policy_loss: 0.215922
Policy train: iteration: 6000, policy_loss: 0.229166
Policy train: iteration: 6500, policy_loss: 0.279999
Policy train: iteration: 7000, policy_loss: 0.265066
Policy train: iteration: 7500, policy_loss: 0.284020
Policy train: iteration: 8000, policy_loss: 0.230376
Policy train: iteration: 8500, policy_loss: 0.220135
Policy train: iteration: 9000, policy_loss: 0.215458

Background Trial: 1, reward: -21.183480780165183
Background Trial: 2, reward: -94.51420181745952
Background Trial: 3, reward: -464.73750222163494
Background Trial: 4, reward: 8.950439158900792
Background Trial: 5, reward: -818.4122401182245
Background Trial: 6, reward: -408.94951328235356
Background Trial: 7, reward: -247.53874680695677
Background Trial: 8, reward: -15.371463368233549
Background Trial: 9, reward: -11.170891707803435
Iteration: 6, average_reward: -230.3252889937701

Policy train: iteration: 500, policy_loss: 0.251697
Policy train: iteration: 1000, policy_loss: 0.216468
Policy train: iteration: 1500, policy_loss: 0.185258
Policy train: iteration: 2000, policy_loss: 0.195913
Policy train: iteration: 2500, policy_loss: 0.172521
Policy train: iteration: 3000, policy_loss: 0.218555
Policy train: iteration: 3500, policy_loss: 0.214196
Policy train: iteration: 4000, policy_loss: 0.238663
Policy train: iteration: 4500, policy_loss: 0.182665
Policy train: iteration: 5000, policy_loss: 0.249214
Policy train: iteration: 5500, policy_loss: 0.269574
Policy train: iteration: 6000, policy_loss: 0.184335
Policy train: iteration: 6500, policy_loss: 0.194462
Policy train: iteration: 7000, policy_loss: 0.208254
Policy train: iteration: 7500, policy_loss: 0.228199
Policy train: iteration: 8000, policy_loss: 0.207287
Policy train: iteration: 8500, policy_loss: 0.182562
Policy train: iteration: 9000, policy_loss: 0.212233

Background Trial: 1, reward: -52.91689082193313
Background Trial: 2, reward: 3.8061383271958107
Background Trial: 3, reward: 233.29380602356966
Background Trial: 4, reward: -49.63389055469264
Background Trial: 5, reward: -41.94596343882077
Background Trial: 6, reward: -32.90518247543551
Background Trial: 7, reward: -22.169578256041035
Background Trial: 8, reward: -401.0306584067391
Background Trial: 9, reward: 289.6894565553315
Iteration: 7, average_reward: -8.201418116396136

Policy train: iteration: 500, policy_loss: 0.230016
Policy train: iteration: 1000, policy_loss: 0.174317
Policy train: iteration: 1500, policy_loss: 0.201192
Policy train: iteration: 2000, policy_loss: 0.214414
Policy train: iteration: 2500, policy_loss: 0.236738
Policy train: iteration: 3000, policy_loss: 0.227636
Policy train: iteration: 3500, policy_loss: 0.225256
Policy train: iteration: 4000, policy_loss: 0.214741
Policy train: iteration: 4500, policy_loss: 0.278135
Policy train: iteration: 5000, policy_loss: 0.230713
Policy train: iteration: 5500, policy_loss: 0.207639
Policy train: iteration: 6000, policy_loss: 0.223343
Policy train: iteration: 6500, policy_loss: 0.337550
Policy train: iteration: 7000, policy_loss: 0.211493
Policy train: iteration: 7500, policy_loss: 0.190828
Policy train: iteration: 8000, policy_loss: 0.228859
Policy train: iteration: 8500, policy_loss: 0.186838
Policy train: iteration: 9000, policy_loss: 0.198361

Background Trial: 1, reward: -334.0930642278636
Background Trial: 2, reward: -412.4936143517958
Background Trial: 3, reward: -330.5630602983545
Background Trial: 4, reward: 33.061180722159975
Background Trial: 5, reward: 11.748171058472053
Background Trial: 6, reward: -183.38129284082592
Background Trial: 7, reward: -230.2259373887269
Background Trial: 8, reward: -100.32617430088078
Background Trial: 9, reward: -78.12983708442243
Iteration: 8, average_reward: -180.48929207913756

Policy train: iteration: 500, policy_loss: 0.245933
Policy train: iteration: 1000, policy_loss: 0.196201
Policy train: iteration: 1500, policy_loss: 0.201702
Policy train: iteration: 2000, policy_loss: 0.244511
Policy train: iteration: 2500, policy_loss: 0.217998
Policy train: iteration: 3000, policy_loss: 0.219388
Policy train: iteration: 3500, policy_loss: 0.193344
Policy train: iteration: 4000, policy_loss: 0.211692
Policy train: iteration: 4500, policy_loss: 0.179800
Policy train: iteration: 5000, policy_loss: 0.214135
Policy train: iteration: 5500, policy_loss: 0.173989
Policy train: iteration: 6000, policy_loss: 0.212457
Policy train: iteration: 6500, policy_loss: 0.208512
Policy train: iteration: 7000, policy_loss: 0.180853
Policy train: iteration: 7500, policy_loss: 0.207965
Policy train: iteration: 8000, policy_loss: 0.189310
Policy train: iteration: 8500, policy_loss: 0.180564
Policy train: iteration: 9000, policy_loss: 0.185659

Background Trial: 1, reward: -194.19259534125868
Background Trial: 2, reward: 5.528313295485077
Background Trial: 3, reward: -10.719633293535537
Background Trial: 4, reward: -691.0602665882939
Background Trial: 5, reward: 45.912402581567505
Background Trial: 6, reward: -63.25169822246083
Background Trial: 7, reward: -7.039999662178005
Background Trial: 8, reward: 23.95936267917797
Background Trial: 9, reward: -11.467797896193119
Iteration: 9, average_reward: -100.25910138307663

Policy train: iteration: 500, policy_loss: 0.233294
Policy train: iteration: 1000, policy_loss: 0.182067
Policy train: iteration: 1500, policy_loss: 0.258169
Policy train: iteration: 2000, policy_loss: 0.204938
Policy train: iteration: 2500, policy_loss: 0.151121
Policy train: iteration: 3000, policy_loss: 0.199800
Policy train: iteration: 3500, policy_loss: 0.184805
Policy train: iteration: 4000, policy_loss: 0.185648
Policy train: iteration: 4500, policy_loss: 0.142459
Policy train: iteration: 5000, policy_loss: 0.227556
Policy train: iteration: 5500, policy_loss: 0.207619
Policy train: iteration: 6000, policy_loss: 0.182820
Policy train: iteration: 6500, policy_loss: 0.230634
Policy train: iteration: 7000, policy_loss: 0.136502
Policy train: iteration: 7500, policy_loss: 0.163367
Policy train: iteration: 8000, policy_loss: 0.203101
Policy train: iteration: 8500, policy_loss: 0.202051
Policy train: iteration: 9000, policy_loss: 0.162399

Background Trial: 1, reward: -548.9544703790256
Background Trial: 2, reward: 45.23223039396521
Background Trial: 3, reward: -53.40497374728227
Background Trial: 4, reward: -567.4177862848264
Background Trial: 5, reward: -431.8491294881838
Background Trial: 6, reward: -15.532784623121543
Background Trial: 7, reward: -220.89744388108224
Background Trial: 8, reward: -79.42891360443552
Background Trial: 9, reward: -30.03050906909371
Iteration: 10, average_reward: -211.36486452034288

Policy train: iteration: 500, policy_loss: 0.172567
Policy train: iteration: 1000, policy_loss: 0.233233
Policy train: iteration: 1500, policy_loss: 0.216285
Policy train: iteration: 2000, policy_loss: 0.201140
Policy train: iteration: 2500, policy_loss: 0.176385
Policy train: iteration: 3000, policy_loss: 0.211746
Policy train: iteration: 3500, policy_loss: 0.222067
Policy train: iteration: 4000, policy_loss: 0.197614
Policy train: iteration: 4500, policy_loss: 0.174029
Policy train: iteration: 5000, policy_loss: 0.210552
Policy train: iteration: 5500, policy_loss: 0.221490
Policy train: iteration: 6000, policy_loss: 0.154759
Policy train: iteration: 6500, policy_loss: 0.185074
Policy train: iteration: 7000, policy_loss: 0.153143
Policy train: iteration: 7500, policy_loss: 0.187815
Policy train: iteration: 8000, policy_loss: 0.189949
Policy train: iteration: 8500, policy_loss: 0.277372
Policy train: iteration: 9000, policy_loss: 0.184640

Background Trial: 1, reward: 179.7572650864543
Background Trial: 2, reward: -305.46161751339594
Background Trial: 3, reward: 5.903131186630617
Background Trial: 4, reward: -769.0756946037698
Background Trial: 5, reward: -723.083174759768
Background Trial: 6, reward: 39.668045128018946
Background Trial: 7, reward: -37.827890690057586
Background Trial: 8, reward: -24.36142111016376
Background Trial: 9, reward: 40.248921150924275
Iteration: 11, average_reward: -177.1369373472363

Policy train: iteration: 500, policy_loss: 0.197316
Policy train: iteration: 1000, policy_loss: 0.226714
Policy train: iteration: 1500, policy_loss: 0.194885
Policy train: iteration: 2000, policy_loss: 0.204983
Policy train: iteration: 2500, policy_loss: 0.196987
Policy train: iteration: 3000, policy_loss: 0.196914
Policy train: iteration: 3500, policy_loss: 0.209892
Policy train: iteration: 4000, policy_loss: 0.196458
Policy train: iteration: 4500, policy_loss: 0.198582
Policy train: iteration: 5000, policy_loss: 0.211671
Policy train: iteration: 5500, policy_loss: 0.162435
Policy train: iteration: 6000, policy_loss: 0.159909
Policy train: iteration: 6500, policy_loss: 0.178837
Policy train: iteration: 7000, policy_loss: 0.194306
Policy train: iteration: 7500, policy_loss: 0.215456
Policy train: iteration: 8000, policy_loss: 0.225058
Policy train: iteration: 8500, policy_loss: 0.220994
Policy train: iteration: 9000, policy_loss: 0.133741

Background Trial: 1, reward: -295.0454902301995
Background Trial: 2, reward: -9.788738203085686
Background Trial: 3, reward: 22.356977604615167
Background Trial: 4, reward: 5.557982658222059
Background Trial: 5, reward: 3.362996847857687
Background Trial: 6, reward: 27.53179077747251
Background Trial: 7, reward: -353.73750728933703
Background Trial: 8, reward: -38.624322772202326
Background Trial: 9, reward: -21.533791766732307
Iteration: 12, average_reward: -73.3244558192655

Policy train: iteration: 500, policy_loss: 0.187822
Policy train: iteration: 1000, policy_loss: 0.174539
Policy train: iteration: 1500, policy_loss: 0.134722
Policy train: iteration: 2000, policy_loss: 0.148917
Policy train: iteration: 2500, policy_loss: 0.201287
Policy train: iteration: 3000, policy_loss: 0.151492
Policy train: iteration: 3500, policy_loss: 0.219650
Policy train: iteration: 4000, policy_loss: 0.171808
Policy train: iteration: 4500, policy_loss: 0.184154
Policy train: iteration: 5000, policy_loss: 0.206270
Policy train: iteration: 5500, policy_loss: 0.164863
Policy train: iteration: 6000, policy_loss: 0.206469
Policy train: iteration: 6500, policy_loss: 0.214081
Policy train: iteration: 7000, policy_loss: 0.159543
Policy train: iteration: 7500, policy_loss: 0.137213
Policy train: iteration: 8000, policy_loss: 0.168690
Policy train: iteration: 8500, policy_loss: 0.202302
Policy train: iteration: 9000, policy_loss: 0.167495

Background Trial: 1, reward: 1.8925860260945626
Background Trial: 2, reward: 14.172052897559979
Background Trial: 3, reward: -322.615551407715
Background Trial: 4, reward: -34.99292820889616
Background Trial: 5, reward: -80.02454359720701
Background Trial: 6, reward: -95.16565529038829
Background Trial: 7, reward: -82.11489850024124
Background Trial: 8, reward: -200.62398769266133
Background Trial: 9, reward: 20.383935058204045
Iteration: 13, average_reward: -86.56544341280559

Policy train: iteration: 500, policy_loss: 0.186954
Policy train: iteration: 1000, policy_loss: 0.151103
Policy train: iteration: 1500, policy_loss: 0.200915
Policy train: iteration: 2000, policy_loss: 0.270909
Policy train: iteration: 2500, policy_loss: 0.154895
Policy train: iteration: 3000, policy_loss: 0.229124
Policy train: iteration: 3500, policy_loss: 0.164770
Policy train: iteration: 4000, policy_loss: 0.193165
Policy train: iteration: 4500, policy_loss: 0.176141
Policy train: iteration: 5000, policy_loss: 0.159988
Policy train: iteration: 5500, policy_loss: 0.150844
Policy train: iteration: 6000, policy_loss: 0.227030
Policy train: iteration: 6500, policy_loss: 0.231172
Policy train: iteration: 7000, policy_loss: 0.182948
Policy train: iteration: 7500, policy_loss: 0.146859
Policy train: iteration: 8000, policy_loss: 0.161857
Policy train: iteration: 8500, policy_loss: 0.192101
Policy train: iteration: 9000, policy_loss: 0.178343

Background Trial: 1, reward: -29.75142187802767
Background Trial: 2, reward: 5.281179945379037
Background Trial: 3, reward: -45.634352048657824
Background Trial: 4, reward: -663.3634031083957
Background Trial: 5, reward: 9.47001242605009
Background Trial: 6, reward: 1.4637576901492935
Background Trial: 7, reward: -31.08929230960264
Background Trial: 8, reward: -37.00062112567349
Background Trial: 9, reward: -3.8908005891211275
Iteration: 14, average_reward: -88.27943788865558

Policy train: iteration: 500, policy_loss: 0.249022
Policy train: iteration: 1000, policy_loss: 0.214312
Policy train: iteration: 1500, policy_loss: 0.169944
Policy train: iteration: 2000, policy_loss: 0.227184
Policy train: iteration: 2500, policy_loss: 0.148823
Policy train: iteration: 3000, policy_loss: 0.145797
Policy train: iteration: 3500, policy_loss: 0.134919
Policy train: iteration: 4000, policy_loss: 0.118201
Policy train: iteration: 4500, policy_loss: 0.142439
Policy train: iteration: 5000, policy_loss: 0.165400
Policy train: iteration: 5500, policy_loss: 0.182970
Policy train: iteration: 6000, policy_loss: 0.139448
Policy train: iteration: 6500, policy_loss: 0.148155
Policy train: iteration: 7000, policy_loss: 0.152717
Policy train: iteration: 7500, policy_loss: 0.186484
Policy train: iteration: 8000, policy_loss: 0.162443
Policy train: iteration: 8500, policy_loss: 0.168831
Policy train: iteration: 9000, policy_loss: 0.174360

Background Trial: 1, reward: -42.02644105790242
Background Trial: 2, reward: -60.44037908741819
Background Trial: 3, reward: -0.08088032035112747
Background Trial: 4, reward: 11.969864164784411
Background Trial: 5, reward: 0.8810005544584243
Background Trial: 6, reward: -36.452693652122235
Background Trial: 7, reward: 3.74468964394039
Background Trial: 8, reward: -15.339974366815767
Background Trial: 9, reward: 30.448584991888538
Iteration: 15, average_reward: -11.92180323661533

Policy train: iteration: 500, policy_loss: 0.184102
Policy train: iteration: 1000, policy_loss: 0.158870
Policy train: iteration: 1500, policy_loss: 0.169066
Policy train: iteration: 2000, policy_loss: 0.155154
Policy train: iteration: 2500, policy_loss: 0.122671
Policy train: iteration: 3000, policy_loss: 0.163229
Policy train: iteration: 3500, policy_loss: 0.140250
Policy train: iteration: 4000, policy_loss: 0.170835
Policy train: iteration: 4500, policy_loss: 0.158212
Policy train: iteration: 5000, policy_loss: 0.168522
Policy train: iteration: 5500, policy_loss: 0.235883
Policy train: iteration: 6000, policy_loss: 0.140086
Policy train: iteration: 6500, policy_loss: 0.147136
Policy train: iteration: 7000, policy_loss: 0.141553
Policy train: iteration: 7500, policy_loss: 0.137974
Policy train: iteration: 8000, policy_loss: 0.147159
Policy train: iteration: 8500, policy_loss: 0.175587
Policy train: iteration: 9000, policy_loss: 0.184373

Background Trial: 1, reward: -11.291045434446431
Background Trial: 2, reward: 56.463139325403375
Background Trial: 3, reward: 15.930182642903176
Background Trial: 4, reward: -244.51059570851848
Background Trial: 5, reward: -18.88336736834239
Background Trial: 6, reward: 309.1296990175012
Background Trial: 7, reward: -13.743738656143933
Background Trial: 8, reward: -4.745329549021207
Background Trial: 9, reward: -18.710654646772497
Iteration: 16, average_reward: 7.737587735840314

Policy train: iteration: 500, policy_loss: 0.127317
Policy train: iteration: 1000, policy_loss: 0.215498
Policy train: iteration: 1500, policy_loss: 0.144025
Policy train: iteration: 2000, policy_loss: 0.195060
Policy train: iteration: 2500, policy_loss: 0.202616
Policy train: iteration: 3000, policy_loss: 0.166765
Policy train: iteration: 3500, policy_loss: 0.183924
Policy train: iteration: 4000, policy_loss: 0.154555
Policy train: iteration: 4500, policy_loss: 0.143682
Policy train: iteration: 5000, policy_loss: 0.170621
Policy train: iteration: 5500, policy_loss: 0.211162
Policy train: iteration: 6000, policy_loss: 0.199636
Policy train: iteration: 6500, policy_loss: 0.280617
Policy train: iteration: 7000, policy_loss: 0.194281
Policy train: iteration: 7500, policy_loss: 0.163873
Policy train: iteration: 8000, policy_loss: 0.149567
Policy train: iteration: 8500, policy_loss: 0.120741
Policy train: iteration: 9000, policy_loss: 0.162342

Background Trial: 1, reward: 226.7716475344042
Background Trial: 2, reward: 19.752305756281885
Background Trial: 3, reward: -33.419247165709535
Background Trial: 4, reward: -28.25777925128051
Background Trial: 5, reward: -10.443610154848542
Background Trial: 6, reward: -38.409312387578616
Background Trial: 7, reward: -44.68366634371099
Background Trial: 8, reward: 300.83264902671385
Background Trial: 9, reward: -6.359705914590265
Iteration: 17, average_reward: 42.86480901107572

Policy train: iteration: 500, policy_loss: 0.195817
Policy train: iteration: 1000, policy_loss: 0.162768
Policy train: iteration: 1500, policy_loss: 0.169901
Policy train: iteration: 2000, policy_loss: 0.185164
Policy train: iteration: 2500, policy_loss: 0.215443
Policy train: iteration: 3000, policy_loss: 0.187255
Policy train: iteration: 3500, policy_loss: 0.189292
Policy train: iteration: 4000, policy_loss: 0.168932
Policy train: iteration: 4500, policy_loss: 0.126807
Policy train: iteration: 5000, policy_loss: 0.128739
Policy train: iteration: 5500, policy_loss: 0.137277
Policy train: iteration: 6000, policy_loss: 0.132729
Policy train: iteration: 6500, policy_loss: 0.206847
Policy train: iteration: 7000, policy_loss: 0.151567
Policy train: iteration: 7500, policy_loss: 0.161044
Policy train: iteration: 8000, policy_loss: 0.172090
Policy train: iteration: 8500, policy_loss: 0.208584
Policy train: iteration: 9000, policy_loss: 0.163379

Background Trial: 1, reward: 12.815425921392219
Background Trial: 2, reward: -13.863387243194794
Background Trial: 3, reward: -11.694317744427778
Background Trial: 4, reward: -55.97892641380994
Background Trial: 5, reward: 15.124947360949292
Background Trial: 6, reward: -42.98648171404493
Background Trial: 7, reward: 29.404684517177856
Background Trial: 8, reward: -23.82854461069958
Background Trial: 9, reward: -47.843690196680626
Iteration: 18, average_reward: -15.427810013704251

Policy train: iteration: 500, policy_loss: 0.100164
Policy train: iteration: 1000, policy_loss: 0.170731
Policy train: iteration: 1500, policy_loss: 0.211200
Policy train: iteration: 2000, policy_loss: 0.125181
Policy train: iteration: 2500, policy_loss: 0.157943
Policy train: iteration: 3000, policy_loss: 0.194433
Policy train: iteration: 3500, policy_loss: 0.135134
Policy train: iteration: 4000, policy_loss: 0.151666
Policy train: iteration: 4500, policy_loss: 0.163393
Policy train: iteration: 5000, policy_loss: 0.161358
Policy train: iteration: 5500, policy_loss: 0.164998
Policy train: iteration: 6000, policy_loss: 0.219737
Policy train: iteration: 6500, policy_loss: 0.186363
Policy train: iteration: 7000, policy_loss: 0.167721
Policy train: iteration: 7500, policy_loss: 0.151906
Policy train: iteration: 8000, policy_loss: 0.159682
Policy train: iteration: 8500, policy_loss: 0.202428
Policy train: iteration: 9000, policy_loss: 0.130728

Background Trial: 1, reward: -55.594645168157356
Background Trial: 2, reward: 34.35899370369634
Background Trial: 3, reward: -76.99598930820298
Background Trial: 4, reward: 55.462230649178736
Background Trial: 5, reward: -331.7720815099809
Background Trial: 6, reward: -65.23199436333542
Background Trial: 7, reward: 253.83013698552122
Background Trial: 8, reward: -45.243144163548806
Background Trial: 9, reward: 34.247650316913166
Iteration: 19, average_reward: -21.882093650879554

Policy train: iteration: 500, policy_loss: 0.190489
Policy train: iteration: 1000, policy_loss: 0.165829
Policy train: iteration: 1500, policy_loss: 0.166617
Policy train: iteration: 2000, policy_loss: 0.127037
Policy train: iteration: 2500, policy_loss: 0.098610
Policy train: iteration: 3000, policy_loss: 0.129189
Policy train: iteration: 3500, policy_loss: 0.149358
Policy train: iteration: 4000, policy_loss: 0.149172
Policy train: iteration: 4500, policy_loss: 0.087893
Policy train: iteration: 5000, policy_loss: 0.156568
Policy train: iteration: 5500, policy_loss: 0.165849
Policy train: iteration: 6000, policy_loss: 0.163148
Policy train: iteration: 6500, policy_loss: 0.152510
Policy train: iteration: 7000, policy_loss: 0.173836
Policy train: iteration: 7500, policy_loss: 0.167270
Policy train: iteration: 8000, policy_loss: 0.205243
Policy train: iteration: 8500, policy_loss: 0.125418
Policy train: iteration: 9000, policy_loss: 0.164267

Background Trial: 1, reward: 8.930418490747627
Background Trial: 2, reward: -391.2637485975101
Background Trial: 3, reward: -40.65785015903746
Background Trial: 4, reward: 25.759487609105108
Background Trial: 5, reward: -74.2534642965181
Background Trial: 6, reward: 20.304542318732658
Background Trial: 7, reward: 12.729393122765643
Background Trial: 8, reward: -249.0580447409446
Background Trial: 9, reward: -65.34152034798224
Iteration: 20, average_reward: -83.65008740007129

Policy train: iteration: 500, policy_loss: 0.112687
Policy train: iteration: 1000, policy_loss: 0.164884
