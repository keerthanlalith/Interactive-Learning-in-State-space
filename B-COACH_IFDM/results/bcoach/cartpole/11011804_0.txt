Collecting idm training data 1000
IDM train: iteration: 500, idm_loss: 0.022827
IDM train: iteration: 1000, idm_loss: 0.003229

iteration: 1, total_reward: 9.0, policy_loss: 0.693194, idm_loss: 0.003077


iteration: 2, total_reward: 11.0, policy_loss: 0.693334, idm_loss: 0.003051


iteration: 3, total_reward: 10.0, policy_loss: 0.693170, idm_loss: 0.003035


iteration: 4, total_reward: 9.0, policy_loss: 0.693158, idm_loss: 0.001870


iteration: 5, total_reward: 9.0, policy_loss: 0.693034, idm_loss: 0.001982

Collecting idm training data 1000
IDM train: iteration: 500, idm_loss: 0.001117
IDM train: iteration: 1000, idm_loss: 0.000689

iteration: 6, total_reward: 10.0, policy_loss: 0.693013, idm_loss: 0.000375


iteration: 7, total_reward: 10.0, policy_loss: 0.693002, idm_loss: 0.000368


iteration: 8, total_reward: 9.0, policy_loss: 0.692858, idm_loss: 0.000671


iteration: 9, total_reward: 16.0, policy_loss: 0.692286, idm_loss: 0.000582


iteration: 10, total_reward: 56.0, policy_loss: 0.691314, idm_loss: 0.000595

Collecting idm training data 1000
IDM train: iteration: 500, idm_loss: 0.000336
IDM train: iteration: 1000, idm_loss: 0.000228

iteration: 11, total_reward: 17.0, policy_loss: 0.686287, idm_loss: 0.000229


iteration: 12, total_reward: 84.0, policy_loss: 0.679168, idm_loss: 0.000232


iteration: 13, total_reward: 23.0, policy_loss: 0.659572, idm_loss: 0.000230


iteration: 14, total_reward: 17.0, policy_loss: 0.686959, idm_loss: 0.000215


iteration: 15, total_reward: 16.0, policy_loss: 0.686807, idm_loss: 0.000199

Collecting idm training data 1000
IDM train: iteration: 500, idm_loss: 0.000129
IDM train: iteration: 1000, idm_loss: 0.000089

iteration: 16, total_reward: 20.0, policy_loss: 0.691778, idm_loss: 0.000088


iteration: 17, total_reward: 200.0, policy_loss: 0.675774, idm_loss: 0.000100

