IDM train: iteration: 500, idm_loss: 0.158161
IDM train: iteration: 1000, idm_loss: 0.119982
IDM train: iteration: 1500, idm_loss: 0.056046
IDM train: iteration: 2000, idm_loss: 0.053190
IDM train: iteration: 2500, idm_loss: 0.027682
IDM train: iteration: 3000, idm_loss: 0.027651
IDM train: iteration: 3500, idm_loss: 0.017207
IDM train: iteration: 4000, idm_loss: 0.011967
IDM train: iteration: 4500, idm_loss: 0.016314
IDM train: iteration: 5000, idm_loss: 0.010015
IDM train: iteration: 5500, idm_loss: 0.005215
IDM train: iteration: 6000, idm_loss: 0.004096
IDM train: iteration: 6500, idm_loss: 0.007893
IDM train: iteration: 7000, idm_loss: 0.008072
IDM train: iteration: 7500, idm_loss: 0.004797
IDM train: iteration: 8000, idm_loss: 0.003233
IDM train: iteration: 8500, idm_loss: 0.004341
IDM train: iteration: 9000, idm_loss: 0.002320
IDM train: iteration: 9500, idm_loss: 0.003206
IDM train: iteration: 10000, idm_loss: 0.003434
Policy train: iteration: 500, policy_loss: 0.123769
Policy train: iteration: 1000, policy_loss: 0.106036
Policy train: iteration: 1500, policy_loss: 0.105323
Policy train: iteration: 2000, policy_loss: 0.090531
Policy train: iteration: 2500, policy_loss: 0.089484
Policy train: iteration: 3000, policy_loss: 0.072585
Policy train: iteration: 3500, policy_loss: 0.100118
Policy train: iteration: 4000, policy_loss: 0.122303
Policy train: iteration: 4500, policy_loss: 0.066157
Policy train: iteration: 5000, policy_loss: 0.072546
Policy train: iteration: 5500, policy_loss: 0.089456
Policy train: iteration: 6000, policy_loss: 0.070745
Policy train: iteration: 6500, policy_loss: 0.063164
Policy train: iteration: 7000, policy_loss: 0.069906
Policy train: iteration: 7500, policy_loss: 0.057718
Policy train: iteration: 8000, policy_loss: 0.076824
Policy train: iteration: 8500, policy_loss: 0.047259
Policy train: iteration: 9000, policy_loss: 0.066468
Policy train: iteration: 9500, policy_loss: 0.049996
Policy train: iteration: 10000, policy_loss: 0.050089

iteration: 1, total_reward: -107.21246744677238, policy_loss: 0.072381, idm_loss: 0.291480

Policy train: iteration: 500, policy_loss: 0.060883
Policy train: iteration: 1000, policy_loss: 0.063413
Policy train: iteration: 1500, policy_loss: 0.065693
Policy train: iteration: 2000, policy_loss: 0.055471
Policy train: iteration: 2500, policy_loss: 0.049128
Policy train: iteration: 3000, policy_loss: 0.051486
Policy train: iteration: 3500, policy_loss: 0.092972
Policy train: iteration: 4000, policy_loss: 0.038012
Policy train: iteration: 4500, policy_loss: 0.058223
Policy train: iteration: 5000, policy_loss: 0.058246
Policy train: iteration: 5500, policy_loss: 0.060079
Policy train: iteration: 6000, policy_loss: 0.061322
Policy train: iteration: 6500, policy_loss: 0.072204
Policy train: iteration: 7000, policy_loss: 0.053024
Policy train: iteration: 7500, policy_loss: 0.062114
Policy train: iteration: 8000, policy_loss: 0.074956
Policy train: iteration: 8500, policy_loss: 0.068983
Policy train: iteration: 9000, policy_loss: 0.064680
Policy train: iteration: 9500, policy_loss: 0.044961
Policy train: iteration: 10000, policy_loss: 0.051423

iteration: 2, total_reward: -138.24818777055413, policy_loss: 0.091696, idm_loss: 0.072207

Policy train: iteration: 500, policy_loss: 0.037915
Policy train: iteration: 1000, policy_loss: 0.041445
Policy train: iteration: 1500, policy_loss: 0.065334
Policy train: iteration: 2000, policy_loss: 0.049116
Policy train: iteration: 2500, policy_loss: 0.067447
Policy train: iteration: 3000, policy_loss: 0.051889
Policy train: iteration: 3500, policy_loss: 0.026343
Policy train: iteration: 4000, policy_loss: 0.048267
Policy train: iteration: 4500, policy_loss: 0.070555
Policy train: iteration: 5000, policy_loss: 0.041351
Policy train: iteration: 5500, policy_loss: 0.047731
Policy train: iteration: 6000, policy_loss: 0.040855
Policy train: iteration: 6500, policy_loss: 0.064708
Policy train: iteration: 7000, policy_loss: 0.060744
Policy train: iteration: 7500, policy_loss: 0.040890
Policy train: iteration: 8000, policy_loss: 0.033728
Policy train: iteration: 8500, policy_loss: 0.036694
Policy train: iteration: 9000, policy_loss: 0.049899
Policy train: iteration: 9500, policy_loss: 0.055443
Policy train: iteration: 10000, policy_loss: 0.058411

iteration: 3, total_reward: -105.01814962043551, policy_loss: 0.047179, idm_loss: 0.147969

Policy train: iteration: 500, policy_loss: 0.061300
Policy train: iteration: 1000, policy_loss: 0.047165
Policy train: iteration: 1500, policy_loss: 0.043080
Policy train: iteration: 2000, policy_loss: 0.060730
Policy train: iteration: 2500, policy_loss: 0.057956
Policy train: iteration: 3000, policy_loss: 0.025182
Policy train: iteration: 3500, policy_loss: 0.043566
Policy train: iteration: 4000, policy_loss: 0.026613
Policy train: iteration: 4500, policy_loss: 0.046455
Policy train: iteration: 5000, policy_loss: 0.049318
