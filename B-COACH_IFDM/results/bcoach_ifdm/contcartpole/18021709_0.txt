Policy train: iteration: 500, policy_loss: 0.402657
Policy train: iteration: 1000, policy_loss: 0.384271

episode_reward: 189.0
iteration: 1, average_reward: 111.3, policy_loss: 0.723715, fdm_loss: 0.010333


episode_reward: 251.0
iteration: 2, average_reward: 48.2, policy_loss: 0.841258, fdm_loss: 0.027175


episode_reward: 500.0
iteration: 3, average_reward: 135.1, policy_loss: 0.794509, fdm_loss: 0.011349


episode_reward: 500.0
iteration: 4, average_reward: 161.6, policy_loss: 0.837441, fdm_loss: 0.007427


episode_reward: 500.0
iteration: 5, average_reward: 500.0, policy_loss: 0.848322, fdm_loss: 0.013853


episode_reward: 500.0
iteration: 6, average_reward: 500.0, policy_loss: 0.841593, fdm_loss: 0.011979


episode_reward: 500.0
iteration: 7, average_reward: 500.0, policy_loss: 0.809645, fdm_loss: 0.015341


episode_reward: 500.0
iteration: 8, average_reward: 500.0, policy_loss: 0.807846, fdm_loss: 0.015825

