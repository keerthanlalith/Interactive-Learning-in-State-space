Collecting idm training data 1000
IDM train: iteration: 500, idm_loss: 0.024242
IDM train: iteration: 1000, idm_loss: 0.002595

iteration: 1, total_reward: 10.0, policy_loss: 0.693812, idm_loss: 0.001208


iteration: 2, total_reward: 8.0, policy_loss: 0.695298, idm_loss: 0.001321


iteration: 3, total_reward: 10.0, policy_loss: 0.693379, idm_loss: 0.001636


iteration: 4, total_reward: 9.0, policy_loss: 0.691517, idm_loss: 0.001379


iteration: 5, total_reward: 8.0, policy_loss: 0.693708, idm_loss: 0.001718

Collecting idm training data 1000
IDM train: iteration: 500, idm_loss: 0.001192
IDM train: iteration: 1000, idm_loss: 0.000664

iteration: 6, total_reward: 9.0, policy_loss: 0.693974, idm_loss: 0.000228


iteration: 7, total_reward: 10.0, policy_loss: 0.694062, idm_loss: 0.000270


iteration: 8, total_reward: 8.0, policy_loss: 0.692343, idm_loss: 0.000193


iteration: 9, total_reward: 10.0, policy_loss: 0.693706, idm_loss: 0.000217


iteration: 10, total_reward: 9.0, policy_loss: 0.690989, idm_loss: 0.000274

Collecting idm training data 1000
IDM train: iteration: 500, idm_loss: 0.000311
IDM train: iteration: 1000, idm_loss: 0.000197

iteration: 11, total_reward: 10.0, policy_loss: 0.692620, idm_loss: 0.000075


iteration: 12, total_reward: 10.0, policy_loss: 0.694276, idm_loss: 0.000085


iteration: 13, total_reward: 8.0, policy_loss: 0.690712, idm_loss: 0.000071


iteration: 14, total_reward: 10.0, policy_loss: 0.692784, idm_loss: 0.000094


iteration: 15, total_reward: 8.0, policy_loss: 0.690097, idm_loss: 0.000082

Collecting idm training data 1000
IDM train: iteration: 500, idm_loss: 0.000116
IDM train: iteration: 1000, idm_loss: 0.000091

iteration: 16, total_reward: 10.0, policy_loss: 0.692267, idm_loss: 0.000040


iteration: 17, total_reward: 10.0, policy_loss: 0.692842, idm_loss: 0.000042


iteration: 18, total_reward: 10.0, policy_loss: 0.693104, idm_loss: 0.000033


iteration: 19, total_reward: 12.0, policy_loss: 0.690479, idm_loss: 0.000062


iteration: 20, total_reward: 15.0, policy_loss: 0.691796, idm_loss: 0.000074

Collecting idm training data 1000
IDM train: iteration: 500, idm_loss: 0.000046
IDM train: iteration: 1000, idm_loss: 0.000048

iteration: 21, total_reward: 12.0, policy_loss: 0.691193, idm_loss: 0.000048


iteration: 22, total_reward: 10.0, policy_loss: 0.690835, idm_loss: 0.000047


iteration: 23, total_reward: 16.0, policy_loss: 0.689431, idm_loss: 0.000046


iteration: 24, total_reward: 22.0, policy_loss: 0.689783, idm_loss: 0.000044


iteration: 25, total_reward: 45.0, policy_loss: 0.685904, idm_loss: 0.000041

Collecting idm training data 1000
IDM train: iteration: 500, idm_loss: 0.000027
IDM train: iteration: 1000, idm_loss: 0.000019

iteration: 26, total_reward: 42.0, policy_loss: 0.679484, idm_loss: 0.000021


iteration: 27, total_reward: 58.0, policy_loss: 0.677317, idm_loss: 0.000021


iteration: 28, total_reward: 32.0, policy_loss: 0.667623, idm_loss: 0.000020


iteration: 29, total_reward: 62.0, policy_loss: 0.662038, idm_loss: 0.000020


iteration: 30, total_reward: 85.0, policy_loss: 0.651049, idm_loss: 0.000020

Collecting idm training data 1000
IDM train: iteration: 500, idm_loss: 0.000015
IDM train: iteration: 1000, idm_loss: 0.000013

iteration: 31, total_reward: 77.0, policy_loss: 0.639079, idm_loss: 0.000011


iteration: 32, total_reward: 76.0, policy_loss: 0.635911, idm_loss: 0.000011


iteration: 33, total_reward: 56.0, policy_loss: 0.619536, idm_loss: 0.000011


iteration: 34, total_reward: 200.0, policy_loss: 0.606981, idm_loss: 0.000011


iteration: 35, total_reward: 85.0, policy_loss: 0.575316, idm_loss: 0.000011

Collecting idm training data 1000
IDM train: iteration: 500, idm_loss: 0.000006
IDM train: iteration: 1000, idm_loss: 0.000007

iteration: 36, total_reward: 200.0, policy_loss: 0.557904, idm_loss: 0.000006


iteration: 37, total_reward: 76.0, policy_loss: 0.557591, idm_loss: 0.000006


iteration: 38, total_reward: 200.0, policy_loss: 0.537062, idm_loss: 0.000006


iteration: 39, total_reward: 200.0, policy_loss: 0.515993, idm_loss: 0.000006

