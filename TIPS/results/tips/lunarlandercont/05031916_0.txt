Collecting dynamics training data 1000
Collecting dynamics training data 2000
Collecting dynamics training data 3000
Collecting dynamics training data 4000
Collecting dynamics training data 5000
FDM train: iteration: 500, fdm_loss: 0.004368
FDM train: iteration: 1000, fdm_loss: 0.002276
FDM train: iteration: 1500, fdm_loss: 0.021155
FDM train: iteration: 2000, fdm_loss: 0.000965
FDM train: iteration: 2500, fdm_loss: 0.025929
FDM train: iteration: 3000, fdm_loss: 0.037631
FDM train: iteration: 3500, fdm_loss: 0.011473
FDM train: iteration: 4000, fdm_loss: 0.010812
FDM train: iteration: 4500, fdm_loss: 0.001285
FDM train: iteration: 5000, fdm_loss: 0.004453
FDM train: iteration: 5500, fdm_loss: 0.005493
FDM train: iteration: 6000, fdm_loss: 0.003699

episode_reward: 286.0
iteration: 1, average_reward: -497.1284818170358, policy_loss: 0.573795, fdm_loss: 0.011489


episode_reward: 254.6Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.003042
FDM train: iteration: 1000, fdm_loss: 0.008082
FDM train: iteration: 1500, fdm_loss: 0.001379
FDM train: iteration: 2000, fdm_loss: 0.011200
FDM train: iteration: 2500, fdm_loss: 0.002660
FDM train: iteration: 3000, fdm_loss: 0.006120
FDM train: iteration: 3500, fdm_loss: 0.000547
FDM train: iteration: 4000, fdm_loss: 0.007507
FDM train: iteration: 4500, fdm_loss: 0.004651
FDM train: iteration: 5000, fdm_loss: 0.006713
FDM train: iteration: 5500, fdm_loss: 0.008621
FDM train: iteration: 6000, fdm_loss: 0.007721

iteration: 2, average_reward: -327.9907920386239, policy_loss: 0.435114, fdm_loss: 0.001806


episode_reward: 224.0
iteration: 3, average_reward: -237.78013681121882, policy_loss: 0.444161, fdm_loss: 0.006814


episode_reward: 248.0Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.004919
FDM train: iteration: 1000, fdm_loss: 0.004218
FDM train: iteration: 1500, fdm_loss: 0.004260
FDM train: iteration: 2000, fdm_loss: 0.001062
FDM train: iteration: 2500, fdm_loss: 0.001245
FDM train: iteration: 3000, fdm_loss: 0.000472
FDM train: iteration: 3500, fdm_loss: 0.023341
FDM train: iteration: 4000, fdm_loss: 0.000859
FDM train: iteration: 4500, fdm_loss: 0.000710
FDM train: iteration: 5000, fdm_loss: 0.000329
FDM train: iteration: 5500, fdm_loss: 0.007512
FDM train: iteration: 6000, fdm_loss: 0.021989

iteration: 4, average_reward: -122.34576710773891, policy_loss: 0.395472, fdm_loss: 0.013674


episode_reward: 244.5
iteration: 5, average_reward: -53.12590488927458, policy_loss: 0.547444, fdm_loss: 0.004944


episode_reward: 278.4Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.007800
FDM train: iteration: 1000, fdm_loss: 0.008432
FDM train: iteration: 1500, fdm_loss: 0.002934
FDM train: iteration: 2000, fdm_loss: 0.003830
FDM train: iteration: 2500, fdm_loss: 0.002937
FDM train: iteration: 3000, fdm_loss: 0.001450
FDM train: iteration: 3500, fdm_loss: 0.005878
FDM train: iteration: 4000, fdm_loss: 0.002226
FDM train: iteration: 4500, fdm_loss: 0.007480
FDM train: iteration: 5000, fdm_loss: 0.003035
FDM train: iteration: 5500, fdm_loss: 0.005728
FDM train: iteration: 6000, fdm_loss: 0.002390

iteration: 6, average_reward: 130.26958449368632, policy_loss: 0.365082, fdm_loss: 0.004388


episode_reward: 273.3
iteration: 7, average_reward: 128.65467171592508, policy_loss: 0.508797, fdm_loss: 0.002693


episode_reward: 253.9Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.004716
FDM train: iteration: 1000, fdm_loss: 0.004804
FDM train: iteration: 1500, fdm_loss: 0.017109
FDM train: iteration: 2000, fdm_loss: 0.003890
FDM train: iteration: 2500, fdm_loss: 0.000923
FDM train: iteration: 3000, fdm_loss: 0.000266
FDM train: iteration: 3500, fdm_loss: 0.000906
FDM train: iteration: 4000, fdm_loss: 0.013431
FDM train: iteration: 4500, fdm_loss: 0.002808
FDM train: iteration: 5000, fdm_loss: 0.022955
FDM train: iteration: 5500, fdm_loss: 0.004796
FDM train: iteration: 6000, fdm_loss: 0.000522

iteration: 8, average_reward: 119.82291877692055, policy_loss: 0.326042, fdm_loss: 0.004326


episode_reward: 228.7
iteration: 9, average_reward: 179.46423839632567, policy_loss: 0.351439, fdm_loss: 0.003086


episode_reward:  24.4Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.005828
FDM train: iteration: 1000, fdm_loss: 0.007434
FDM train: iteration: 1500, fdm_loss: 0.002755
FDM train: iteration: 2000, fdm_loss: 0.000529
FDM train: iteration: 2500, fdm_loss: 0.000490
FDM train: iteration: 3000, fdm_loss: 0.005103
FDM train: iteration: 3500, fdm_loss: 0.006968
FDM train: iteration: 4000, fdm_loss: 0.002649
FDM train: iteration: 4500, fdm_loss: 0.005783
FDM train: iteration: 5000, fdm_loss: 0.003403
FDM train: iteration: 5500, fdm_loss: 0.000721
FDM train: iteration: 6000, fdm_loss: 0.003215

iteration: 10, average_reward: 35.18636213045077, policy_loss: 0.405400, fdm_loss: 0.002569


episode_reward: 286.8
iteration: 11, average_reward: 210.00382630557436, policy_loss: 0.453115, fdm_loss: 0.001352


episode_reward: 281.7
iteration: 12, average_reward: 192.08563905697466, policy_loss: 0.451934, fdm_loss: 0.007077


episode_reward: 202.9
iteration: 13, average_reward: 155.72004326592383, policy_loss: 0.532523, fdm_loss: 0.005482


episode_reward: 206.7
iteration: 14, average_reward: 199.9279876867884, policy_loss: 0.509692, fdm_loss: 0.003166


episode_reward: 236.6
iteration: 15, average_reward: 216.50791838160194, policy_loss: 0.493429, fdm_loss: 0.005998


episode_reward: 288.2
iteration: 16, average_reward: 200.19929113652236, policy_loss: 0.484579, fdm_loss: 0.003407


episode_reward: 250.0
iteration: 17, average_reward: 185.25215341003383, policy_loss: 0.526837, fdm_loss: 0.004819

