Collecting dynamics training data 1000
Collecting dynamics training data 2000
Collecting dynamics training data 3000
Collecting dynamics training data 4000
Collecting dynamics training data 5000
Collecting dynamics training data 6000
Collecting dynamics training data 7000
Collecting dynamics training data 8000
Collecting dynamics training data 9000
Collecting dynamics training data 10000
FDM train: iteration: 500, fdm_loss: 0.019056
FDM train: iteration: 1000, fdm_loss: 0.009413
FDM train: iteration: 1500, fdm_loss: 0.009779
FDM train: iteration: 2000, fdm_loss: 0.009980
FDM train: iteration: 2500, fdm_loss: 0.007005
FDM train: iteration: 3000, fdm_loss: 0.017115
FDM train: iteration: 3500, fdm_loss: 0.006590
FDM train: iteration: 4000, fdm_loss: 0.005180
FDM train: iteration: 4500, fdm_loss: 0.005161
FDM train: iteration: 5000, fdm_loss: 0.006345

episode_reward: 218.7
iteration: 1, average_reward: -139.2611466935473, policy_loss: 0.596815, fdm_loss: 0.012599


episode_reward: 272.3Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.001836
FDM train: iteration: 1000, fdm_loss: 0.007701
FDM train: iteration: 1500, fdm_loss: 0.010454
FDM train: iteration: 2000, fdm_loss: 0.011599
FDM train: iteration: 2500, fdm_loss: 0.009240
FDM train: iteration: 3000, fdm_loss: 0.015192
FDM train: iteration: 3500, fdm_loss: 0.002288
FDM train: iteration: 4000, fdm_loss: 0.003646
FDM train: iteration: 4500, fdm_loss: 0.012521
FDM train: iteration: 5000, fdm_loss: 0.001811

iteration: 2, average_reward: -282.5618048951292, policy_loss: 0.502776, fdm_loss: 0.006339


episode_reward: 259.5
iteration: 3, average_reward: -163.25462647299673, policy_loss: 0.531046, fdm_loss: 0.006113


episode_reward: 224.9Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.009488
FDM train: iteration: 1000, fdm_loss: 0.004974
FDM train: iteration: 1500, fdm_loss: 0.006928
FDM train: iteration: 2000, fdm_loss: 0.004289
FDM train: iteration: 2500, fdm_loss: 0.002808
FDM train: iteration: 3000, fdm_loss: 0.004578
FDM train: iteration: 3500, fdm_loss: 0.005992
FDM train: iteration: 4000, fdm_loss: 0.003533
FDM train: iteration: 4500, fdm_loss: 0.007773
FDM train: iteration: 5000, fdm_loss: 0.005379

iteration: 4, average_reward: 22.922779441955793, policy_loss: 0.804919, fdm_loss: 0.006201


episode_reward: 265.6
iteration: 5, average_reward: 100.62196975982425, policy_loss: 0.700907, fdm_loss: 0.008638


episode_reward:  35.9Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.003515
FDM train: iteration: 1000, fdm_loss: 0.005174
FDM train: iteration: 1500, fdm_loss: 0.005033
FDM train: iteration: 2000, fdm_loss: 0.004713
FDM train: iteration: 2500, fdm_loss: 0.002994
FDM train: iteration: 3000, fdm_loss: 0.004114
FDM train: iteration: 3500, fdm_loss: 0.007290
FDM train: iteration: 4000, fdm_loss: 0.005618
FDM train: iteration: 4500, fdm_loss: 0.006810
FDM train: iteration: 5000, fdm_loss: 0.005399

iteration: 6, average_reward: 175.1235173115324, policy_loss: 0.659000, fdm_loss: 0.006260

