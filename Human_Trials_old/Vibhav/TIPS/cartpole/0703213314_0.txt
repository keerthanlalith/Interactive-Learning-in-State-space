FDM train: iteration: 500, fdm_loss: 0.045014
FDM train: iteration: 1000, fdm_loss: 0.017154
FDM train: iteration: 1500, fdm_loss: 0.006411
FDM train: iteration: 2000, fdm_loss: 0.002893
FDM train: iteration: 2500, fdm_loss: 0.003561
FDM train: iteration: 3000, fdm_loss: 0.002825
FDM train: iteration: 3500, fdm_loss: 0.001929
FDM train: iteration: 4000, fdm_loss: 0.002463
FDM train: iteration: 4500, fdm_loss: 0.001886
FDM train: iteration: 5000, fdm_loss: 0.004152

episode_reward:  20.0
Background Trial: 1, reward: 16.0
Background Trial: 2, reward: 16.0
Background Trial: 3, reward: 17.0
Background Trial: 4, reward: 15.0
Background Trial: 5, reward: 16.0
Background Trial: 6, reward: 15.0
Background Trial: 7, reward: 17.0
Background Trial: 8, reward: 17.0
Background Trial: 9, reward: 15.0
Iteration: 1, average_reward: 16.0, policy_loss: 0.694547, fdm_loss: 0.003302


episode_reward:  18.0FDM train: iteration: 500, fdm_loss: 0.002235
FDM train: iteration: 1000, fdm_loss: 0.002478
FDM train: iteration: 1500, fdm_loss: 0.001804
FDM train: iteration: 2000, fdm_loss: 0.000685
FDM train: iteration: 2500, fdm_loss: 0.000904
FDM train: iteration: 3000, fdm_loss: 0.000642
FDM train: iteration: 3500, fdm_loss: 0.000479
FDM train: iteration: 4000, fdm_loss: 0.000349
FDM train: iteration: 4500, fdm_loss: 0.000337
FDM train: iteration: 5000, fdm_loss: 0.000289

Background Trial: 1, reward: 17.0
Background Trial: 2, reward: 17.0
Background Trial: 3, reward: 18.0
Background Trial: 4, reward: 17.0
Background Trial: 5, reward: 17.0
Background Trial: 6, reward: 16.0
Background Trial: 7, reward: 16.0
Background Trial: 8, reward: 17.0
Background Trial: 9, reward: 17.0
Iteration: 2, average_reward: 16.88888888888889, policy_loss: 0.692413, fdm_loss: 0.000315


episode_reward:  25.0
Background Trial: 1, reward: 16.0
Background Trial: 2, reward: 17.0
Background Trial: 3, reward: 15.0
Background Trial: 4, reward: 16.0
Background Trial: 5, reward: 15.0
Background Trial: 6, reward: 16.0
Background Trial: 7, reward: 16.0
Background Trial: 8, reward: 16.0
Background Trial: 9, reward: 16.0
Iteration: 3, average_reward: 15.88888888888889, policy_loss: 0.683166, fdm_loss: 0.000245


episode_reward:  95.0FDM train: iteration: 500, fdm_loss: 0.002268
FDM train: iteration: 1000, fdm_loss: 0.002000
FDM train: iteration: 1500, fdm_loss: 0.001221
FDM train: iteration: 2000, fdm_loss: 0.000763
FDM train: iteration: 2500, fdm_loss: 0.000962
FDM train: iteration: 3000, fdm_loss: 0.000679
FDM train: iteration: 3500, fdm_loss: 0.000556
FDM train: iteration: 4000, fdm_loss: 0.001142
FDM train: iteration: 4500, fdm_loss: 0.000799
FDM train: iteration: 5000, fdm_loss: 0.001158

Background Trial: 1, reward: 68.0
Background Trial: 2, reward: 74.0
Background Trial: 3, reward: 69.0
Background Trial: 4, reward: 68.0
Background Trial: 5, reward: 70.0
Background Trial: 6, reward: 69.0
Background Trial: 7, reward: 67.0
Background Trial: 8, reward: 70.0
Background Trial: 9, reward: 66.0
Iteration: 4, average_reward: 69.0, policy_loss: 0.691581, fdm_loss: 0.000676


episode_reward:  28.0
Background Trial: 1, reward: 63.0
Background Trial: 2, reward: 65.0
Background Trial: 3, reward: 66.0
Background Trial: 4, reward: 63.0
Background Trial: 5, reward: 66.0
Background Trial: 6, reward: 63.0
Background Trial: 7, reward: 64.0
Background Trial: 8, reward: 64.0
Background Trial: 9, reward: 63.0
Iteration: 5, average_reward: 64.11111111111111, policy_loss: 0.681449, fdm_loss: 0.000993


episode_reward: 185.0FDM train: iteration: 500, fdm_loss: 0.001273
FDM train: iteration: 1000, fdm_loss: 0.001230
FDM train: iteration: 1500, fdm_loss: 0.001842
FDM train: iteration: 2000, fdm_loss: 0.001306
FDM train: iteration: 2500, fdm_loss: 0.001457
FDM train: iteration: 3000, fdm_loss: 0.001103
FDM train: iteration: 3500, fdm_loss: 0.000914
FDM train: iteration: 4000, fdm_loss: 0.000574
FDM train: iteration: 4500, fdm_loss: 0.001242
FDM train: iteration: 5000, fdm_loss: 0.000982

Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 200.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 6, average_reward: 200.0, policy_loss: 0.744753, fdm_loss: 0.000335


episode_reward: 200.0
Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 200.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 7, average_reward: 200.0, policy_loss: 0.679006, fdm_loss: 0.000403


episode_reward: 200.0FDM train: iteration: 500, fdm_loss: 0.000465
FDM train: iteration: 1000, fdm_loss: 0.000391
FDM train: iteration: 1500, fdm_loss: 0.000195
FDM train: iteration: 2000, fdm_loss: 0.000232
FDM train: iteration: 2500, fdm_loss: 0.000223
FDM train: iteration: 3000, fdm_loss: 0.000370
FDM train: iteration: 3500, fdm_loss: 0.000240
FDM train: iteration: 4000, fdm_loss: 0.000196
FDM train: iteration: 4500, fdm_loss: 0.000293
FDM train: iteration: 5000, fdm_loss: 0.000134

Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 200.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 8, average_reward: 200.0, policy_loss: 0.685979, fdm_loss: 0.000279


episode_reward: 200.0
Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 200.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 9, average_reward: 200.0, policy_loss: 0.708215, fdm_loss: 0.000106


episode_reward: 200.0FDM train: iteration: 500, fdm_loss: 0.000239
FDM train: iteration: 1000, fdm_loss: 0.000201
FDM train: iteration: 1500, fdm_loss: 0.000127
FDM train: iteration: 2000, fdm_loss: 0.000095
FDM train: iteration: 2500, fdm_loss: 0.000220
FDM train: iteration: 3000, fdm_loss: 0.000180
FDM train: iteration: 3500, fdm_loss: 0.000175
FDM train: iteration: 4000, fdm_loss: 0.000072
FDM train: iteration: 4500, fdm_loss: 0.000113
FDM train: iteration: 5000, fdm_loss: 0.000115

Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 200.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 10, average_reward: 200.0, policy_loss: 0.756516, fdm_loss: 0.000104


episode_reward: 200.0
Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 200.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 11, average_reward: 200.0, policy_loss: 0.770956, fdm_loss: 0.000267

