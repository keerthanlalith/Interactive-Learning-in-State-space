
episode_reward: -328.8
Background Trial: 1, reward: -642.7713238713105
Background Trial: 2, reward: -154.2781591169413
Background Trial: 3, reward: -309.2324541595633
Background Trial: 4, reward: -574.9861964661688
Background Trial: 5, reward: -252.5958250319934
Background Trial: 6, reward: -318.72292892449707
Background Trial: 7, reward: -425.12731148189954
Background Trial: 8, reward: -253.07629879255313
Background Trial: 9, reward: -679.6380507388055
Iteration: 1, average_reward: -401.15872762041477, policy_loss: 0.415956, fdm_loss: 0.009227


episode_reward: -128.8Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.000307
FDM train: iteration: 1000, fdm_loss: 0.000205
FDM train: iteration: 1500, fdm_loss: 0.000088
FDM train: iteration: 2000, fdm_loss: 0.000136
FDM train: iteration: 2500, fdm_loss: 0.000411
FDM train: iteration: 3000, fdm_loss: 0.000369
FDM train: iteration: 3500, fdm_loss: 0.000141
FDM train: iteration: 4000, fdm_loss: 0.000323
FDM train: iteration: 4500, fdm_loss: 0.000197
FDM train: iteration: 5000, fdm_loss: 0.000342

Background Trial: 1, reward: -320.8363771284207
Background Trial: 2, reward: -378.9272100734897
Background Trial: 3, reward: -282.01894662450536
Background Trial: 4, reward: -350.6182718763668
Background Trial: 5, reward: -354.5410311247854
Background Trial: 6, reward: -371.48068254548826
Background Trial: 7, reward: -393.85370818906546
Background Trial: 8, reward: -374.2258144088064
Background Trial: 9, reward: -252.01396511576007
Iteration: 2, average_reward: -342.0573341207431, policy_loss: 0.866926, fdm_loss: 0.000487


episode_reward: -376.2
Background Trial: 1, reward: -200.08596272770814
Background Trial: 2, reward: -3.994346761502328
Background Trial: 3, reward: -247.38611844053995
Background Trial: 4, reward: -239.04706657982678
Background Trial: 5, reward: -375.78257951032697
Background Trial: 6, reward: -207.21973968527942
Background Trial: 7, reward: -660.5022380521473
Background Trial: 8, reward: -259.94747395589866
Background Trial: 9, reward: -329.2751609375357
Iteration: 3, average_reward: -280.3600762945295, policy_loss: 1.040115, fdm_loss: 0.002390


episode_reward: -298.0Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.001659
FDM train: iteration: 1000, fdm_loss: 0.001503
FDM train: iteration: 1500, fdm_loss: 0.000672
FDM train: iteration: 2000, fdm_loss: 0.001615
FDM train: iteration: 2500, fdm_loss: 0.001068
FDM train: iteration: 3000, fdm_loss: 0.001351
FDM train: iteration: 3500, fdm_loss: 0.000903
FDM train: iteration: 4000, fdm_loss: 0.000811
FDM train: iteration: 4500, fdm_loss: 0.000789
FDM train: iteration: 5000, fdm_loss: 0.001387

Background Trial: 1, reward: -205.51137963698514
Background Trial: 2, reward: -193.17583465038123
Background Trial: 3, reward: -474.6134065568669
Background Trial: 4, reward: -61.87522151463906
Background Trial: 5, reward: -400.87552585208994
Background Trial: 6, reward: -251.80264847194618
Background Trial: 7, reward: -269.20900044107833
Background Trial: 8, reward: -386.2760336834744
Background Trial: 9, reward: -408.45401611796336
Iteration: 4, average_reward: -294.64367410282495, policy_loss: 0.708555, fdm_loss: 0.000560


episode_reward: -25.6
Background Trial: 1, reward: -206.34697351520612
Background Trial: 2, reward: -224.46387856628172
Background Trial: 3, reward: -17.2880860318365
Background Trial: 4, reward: -215.14884074490797
Background Trial: 5, reward: -14.08658168846685
Background Trial: 6, reward: -144.25544919945088
Background Trial: 7, reward: -7.941500717364207
Background Trial: 8, reward: -91.7578682738564
Background Trial: 9, reward: -239.9857878865266
Iteration: 5, average_reward: -129.03055184709967, policy_loss: 0.675475, fdm_loss: 0.000842


episode_reward:  -3.1Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.003006
FDM train: iteration: 1000, fdm_loss: 0.002819
FDM train: iteration: 1500, fdm_loss: 0.003279
FDM train: iteration: 2000, fdm_loss: 0.003618
FDM train: iteration: 2500, fdm_loss: 0.003742
FDM train: iteration: 3000, fdm_loss: 0.003324
FDM train: iteration: 3500, fdm_loss: 0.003116
FDM train: iteration: 4000, fdm_loss: 0.002695
FDM train: iteration: 4500, fdm_loss: 0.004420
FDM train: iteration: 5000, fdm_loss: 0.003301

Background Trial: 1, reward: -216.42447346126153
Background Trial: 2, reward: -139.11185546559776
Background Trial: 3, reward: 3.4992877910977995
Background Trial: 4, reward: -27.629978602666156
Background Trial: 5, reward: -53.995177266813585
Background Trial: 6, reward: -21.969167376263385
Background Trial: 7, reward: 34.108852473351504
Background Trial: 8, reward: 30.889035139757482
Background Trial: 9, reward: 241.11512573068592
Iteration: 6, average_reward: -16.613150115301078, policy_loss: 0.834093, fdm_loss: 0.003176


episode_reward: 249.5
Background Trial: 1, reward: -130.25637825517776
Background Trial: 2, reward: -67.52481000489482
Background Trial: 3, reward: -180.04974243628004
Background Trial: 4, reward: -294.41796700239803
Background Trial: 5, reward: -64.5073892113484
Background Trial: 6, reward: -236.05033252897732
Background Trial: 7, reward: -76.37830622309129
Background Trial: 8, reward: -85.89008147703191
Background Trial: 9, reward: -195.03336111272918
Iteration: 7, average_reward: -147.78981869465872, policy_loss: 0.713100, fdm_loss: 0.003018


episode_reward: -55.1Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.007890
FDM train: iteration: 1000, fdm_loss: 0.007889
FDM train: iteration: 1500, fdm_loss: 0.007108
FDM train: iteration: 2000, fdm_loss: 0.007589
FDM train: iteration: 2500, fdm_loss: 0.007947
FDM train: iteration: 3000, fdm_loss: 0.006793
FDM train: iteration: 3500, fdm_loss: 0.009163
FDM train: iteration: 4000, fdm_loss: 0.009886
FDM train: iteration: 4500, fdm_loss: 0.007080
FDM train: iteration: 5000, fdm_loss: 0.007077

Background Trial: 1, reward: -62.13220963046555
Background Trial: 2, reward: 136.55485082324418
Background Trial: 3, reward: -253.67571541859706
Background Trial: 4, reward: -302.10172297014753
Background Trial: 5, reward: -316.58130205988084
Background Trial: 6, reward: -443.87127313625064
Background Trial: 7, reward: -99.0104228107163
Background Trial: 8, reward: -159.15079437100803
Background Trial: 9, reward: -67.85590546682941
Iteration: 8, average_reward: -174.20272167118347, policy_loss: 0.697979, fdm_loss: 0.006110


episode_reward:  26.3
Background Trial: 1, reward: -52.059299481334826
Background Trial: 2, reward: -52.99937027621271
Background Trial: 3, reward: -57.414446404436305
Background Trial: 4, reward: -101.61350916534388
Background Trial: 5, reward: 204.95988026375767
Background Trial: 6, reward: 39.836933476335105
Background Trial: 7, reward: -18.52179762169166
Background Trial: 8, reward: -281.6977548929101
Background Trial: 9, reward: 6.855107110776032
Iteration: 9, average_reward: -34.739361887895626, policy_loss: 0.640820, fdm_loss: 0.008904


episode_reward: 188.4Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.007439
FDM train: iteration: 1000, fdm_loss: 0.008336
FDM train: iteration: 1500, fdm_loss: 0.006701
FDM train: iteration: 2000, fdm_loss: 0.006930
FDM train: iteration: 2500, fdm_loss: 0.006459
FDM train: iteration: 3000, fdm_loss: 0.006619
FDM train: iteration: 3500, fdm_loss: 0.006404
FDM train: iteration: 4000, fdm_loss: 0.008150
FDM train: iteration: 4500, fdm_loss: 0.005508
FDM train: iteration: 5000, fdm_loss: 0.006763

Background Trial: 1, reward: -135.15353576851723
Background Trial: 2, reward: -105.44173044090773
Background Trial: 3, reward: 8.595755156557189
Background Trial: 4, reward: 3.3939370302383765
Background Trial: 5, reward: -175.47731592527504
Background Trial: 6, reward: -242.6750838243553
Background Trial: 7, reward: -60.9916402364739
Background Trial: 8, reward: -82.9076311890122
Background Trial: 9, reward: -176.90901935194694
Iteration: 10, average_reward: -107.50736272774367, policy_loss: 0.809183, fdm_loss: 0.005981


episode_reward: 268.4
Background Trial: 1, reward: 47.27239297059191
Background Trial: 2, reward: -21.242843166993666
Background Trial: 3, reward: 266.1071541192222
Background Trial: 4, reward: -84.7852374998728
Background Trial: 5, reward: -17.811064909221244
Background Trial: 6, reward: 28.016687727621473
Background Trial: 7, reward: -243.31911567983636
Background Trial: 8, reward: 195.37849199057308
Background Trial: 9, reward: -45.845227570861155
Iteration: 11, average_reward: 13.752359775691488, policy_loss: 0.726910, fdm_loss: 0.006696


episode_reward: 197.9Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.005968
FDM train: iteration: 1000, fdm_loss: 0.005031
FDM train: iteration: 1500, fdm_loss: 0.004973
FDM train: iteration: 2000, fdm_loss: 0.005077
FDM train: iteration: 2500, fdm_loss: 0.005866
FDM train: iteration: 3000, fdm_loss: 0.005952
FDM train: iteration: 3500, fdm_loss: 0.005976
FDM train: iteration: 4000, fdm_loss: 0.006579
FDM train: iteration: 4500, fdm_loss: 0.004788
FDM train: iteration: 5000, fdm_loss: 0.006024

Background Trial: 1, reward: -464.9628143670647
Background Trial: 2, reward: 30.24182857084483
Background Trial: 3, reward: -57.98096816087543
Background Trial: 4, reward: -40.04496684346789
Background Trial: 5, reward: -23.21729718308505
Background Trial: 6, reward: -45.869430518587805
Background Trial: 7, reward: 246.34951001678036
Background Trial: 8, reward: 23.640799784073934
Background Trial: 9, reward: 186.27353611682162
Iteration: 12, average_reward: -16.174422509395576, policy_loss: 0.731934, fdm_loss: 0.005848


episode_reward: 256.3
Background Trial: 1, reward: -20.686155225846605
Background Trial: 2, reward: 221.32801595797184
Background Trial: 3, reward: -17.12249673945327
Background Trial: 4, reward: 273.4014117695402
Background Trial: 5, reward: -16.66625042868705
Background Trial: 6, reward: -14.974485518641501
Background Trial: 7, reward: -28.896519027162213
Background Trial: 8, reward: -4.056855773542779
Background Trial: 9, reward: -28.438498075621354
Iteration: 13, average_reward: 40.432018548728585, policy_loss: 0.674297, fdm_loss: 0.005724


episode_reward: 255.7Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.007025
FDM train: iteration: 1000, fdm_loss: 0.005161
FDM train: iteration: 1500, fdm_loss: 0.006595
FDM train: iteration: 2000, fdm_loss: 0.004584
FDM train: iteration: 2500, fdm_loss: 0.005636
FDM train: iteration: 3000, fdm_loss: 0.005123
FDM train: iteration: 3500, fdm_loss: 0.006542
FDM train: iteration: 4000, fdm_loss: 0.005139
FDM train: iteration: 4500, fdm_loss: 0.004498
FDM train: iteration: 5000, fdm_loss: 0.006269

Background Trial: 1, reward: 2.8510677104412423
Background Trial: 2, reward: 170.2829776300531
Background Trial: 3, reward: 236.33013348699026
Background Trial: 4, reward: 224.40100746429403
Background Trial: 5, reward: -37.881870225076156
Background Trial: 6, reward: 234.6218860097631
Background Trial: 7, reward: -41.967108574446286
Background Trial: 8, reward: 217.2542104212784
Background Trial: 9, reward: -41.274733455535795
Iteration: 14, average_reward: 107.17973005197354, policy_loss: 0.963589, fdm_loss: 0.005345


episode_reward:  40.2
Background Trial: 1, reward: 8.725400272712164
Background Trial: 2, reward: 231.2941841122084
Background Trial: 3, reward: -0.8856747622776027
Background Trial: 4, reward: -15.245947550929642
Background Trial: 5, reward: -12.495707241979062
Background Trial: 6, reward: -63.84462756553661
Background Trial: 7, reward: 221.3655264784352
Background Trial: 8, reward: -426.99389192936553
Background Trial: 9, reward: -215.49713303596005
Iteration: 15, average_reward: -30.397541246965854, policy_loss: 0.905869, fdm_loss: 0.005769


episode_reward:   9.6Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.003722
FDM train: iteration: 1000, fdm_loss: 0.006068
FDM train: iteration: 1500, fdm_loss: 0.004593
FDM train: iteration: 2000, fdm_loss: 0.005534
FDM train: iteration: 2500, fdm_loss: 0.005552
FDM train: iteration: 3000, fdm_loss: 0.004958
FDM train: iteration: 3500, fdm_loss: 0.007444
FDM train: iteration: 4000, fdm_loss: 0.005284
FDM train: iteration: 4500, fdm_loss: 0.005109
FDM train: iteration: 5000, fdm_loss: 0.004727

Background Trial: 1, reward: -67.28643920297537
Background Trial: 2, reward: 9.19389210911784
Background Trial: 3, reward: -21.700493363322806
Background Trial: 4, reward: 56.3366885424054
Background Trial: 5, reward: -37.718306517565445
Background Trial: 6, reward: 13.291226999034578
Background Trial: 7, reward: -56.56426886063417
Background Trial: 8, reward: -50.74995211178677
Background Trial: 9, reward: 259.4402261356374
Iteration: 16, average_reward: 11.582508192212291, policy_loss: 0.957113, fdm_loss: 0.004799


episode_reward: 269.5
Background Trial: 1, reward: -72.06924763856
Background Trial: 2, reward: -90.83190834653928
Background Trial: 3, reward: -8.718531214205143
Background Trial: 4, reward: -86.53027204189364
Background Trial: 5, reward: -94.35607814456796
Background Trial: 6, reward: -30.954900323342926
Background Trial: 7, reward: -261.9817944509016
Background Trial: 8, reward: -40.483581854871034
Background Trial: 9, reward: -30.77331032313731
Iteration: 17, average_reward: -79.63329159311323, policy_loss: 0.965033, fdm_loss: 0.003984

