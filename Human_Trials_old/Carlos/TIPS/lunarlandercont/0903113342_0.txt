Collecting dynamics training data 1000
Collecting dynamics training data 2000
Collecting dynamics training data 3000
Collecting dynamics training data 4000
Collecting dynamics training data 5000
Collecting dynamics training data 6000
Collecting dynamics training data 7000
Collecting dynamics training data 8000
Collecting dynamics training data 9000
Collecting dynamics training data 10000
FDM train: iteration: 500, fdm_loss: 0.022006
FDM train: iteration: 1000, fdm_loss: 0.020584
FDM train: iteration: 1500, fdm_loss: 0.014444
FDM train: iteration: 2000, fdm_loss: 0.009893
FDM train: iteration: 2500, fdm_loss: 0.009801
FDM train: iteration: 3000, fdm_loss: 0.007585
FDM train: iteration: 3500, fdm_loss: 0.012859
FDM train: iteration: 4000, fdm_loss: 0.010548
FDM train: iteration: 4500, fdm_loss: 0.017354
FDM train: iteration: 5000, fdm_loss: 0.014921

episode_reward: -15.9
Background Trial: 1, reward: -594.4251424314367
Background Trial: 2, reward: -289.6470061864416
Background Trial: 3, reward: -437.38594319941865
Background Trial: 4, reward: -330.8376495605245
Background Trial: 5, reward: -389.5853007485926
Background Trial: 6, reward: -323.014251733651
Background Trial: 7, reward: -291.3528344572327
Background Trial: 8, reward: -255.70614332908303
Background Trial: 9, reward: -322.86426869356956
Iteration: 1, average_reward: -359.4242822599945, policy_loss: 0.541011, fdm_loss: 0.013045


episode_reward: -10.6Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.012013
FDM train: iteration: 1000, fdm_loss: 0.012350
FDM train: iteration: 1500, fdm_loss: 0.009320
FDM train: iteration: 2000, fdm_loss: 0.009588
FDM train: iteration: 2500, fdm_loss: 0.010023
FDM train: iteration: 3000, fdm_loss: 0.008399
FDM train: iteration: 3500, fdm_loss: 0.015237
FDM train: iteration: 4000, fdm_loss: 0.023218
FDM train: iteration: 4500, fdm_loss: 0.008014
FDM train: iteration: 5000, fdm_loss: 0.009960

Background Trial: 1, reward: -354.7902880115552
Background Trial: 2, reward: -312.8899531749438
Background Trial: 3, reward: -142.9461583332559
Background Trial: 4, reward: -295.01405054680026
Background Trial: 5, reward: -219.81868642102557
Background Trial: 6, reward: -271.9188381532998
Background Trial: 7, reward: -320.21529692602894
Background Trial: 8, reward: -325.4091246004466
Background Trial: 9, reward: -295.3725029303075
Iteration: 2, average_reward: -282.041655455296, policy_loss: 0.541104, fdm_loss: 0.007080


episode_reward: -433.7
Background Trial: 1, reward: -668.6212294659819
Background Trial: 2, reward: -701.246049160672
Background Trial: 3, reward: -666.5037185013784
Background Trial: 4, reward: -655.0401267910944
Background Trial: 5, reward: -547.2911448215029
Background Trial: 6, reward: -687.7834844840522
Background Trial: 7, reward: -659.9582574098501
Background Trial: 8, reward: -412.95149928909876
Background Trial: 9, reward: -350.3427402716041
Iteration: 3, average_reward: -594.4153611328039, policy_loss: 0.648066, fdm_loss: 0.012194


episode_reward: -318.0Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.008702
FDM train: iteration: 1000, fdm_loss: 0.008453
FDM train: iteration: 1500, fdm_loss: 0.010562
FDM train: iteration: 2000, fdm_loss: 0.007153
FDM train: iteration: 2500, fdm_loss: 0.008828
FDM train: iteration: 3000, fdm_loss: 0.013695
FDM train: iteration: 3500, fdm_loss: 0.006889
FDM train: iteration: 4000, fdm_loss: 0.005847
FDM train: iteration: 4500, fdm_loss: 0.014554
FDM train: iteration: 5000, fdm_loss: 0.006596

Background Trial: 1, reward: -222.88112702743376
Background Trial: 2, reward: -331.4827125055681
Background Trial: 3, reward: -321.8312667289163
Background Trial: 4, reward: -274.25371510310964
Background Trial: 5, reward: -357.1022609006024
Background Trial: 6, reward: -432.01218947855676
Background Trial: 7, reward: -323.6523027370116
Background Trial: 8, reward: -305.183569013186
Background Trial: 9, reward: -321.8144261064349
Iteration: 4, average_reward: -321.13484106675776, policy_loss: 0.549967, fdm_loss: 0.006212


episode_reward: -79.2
Background Trial: 1, reward: -59.39815812527149
Background Trial: 2, reward: -96.33006251247079
Background Trial: 3, reward: -124.96383507546736
Background Trial: 4, reward: -29.907874319195884
Background Trial: 5, reward: -33.01726330678545
Background Trial: 6, reward: -75.51171974479749
Background Trial: 7, reward: -83.79875639964845
Background Trial: 8, reward: -122.3453894006559
Background Trial: 9, reward: -25.09781590539717
Iteration: 5, average_reward: -72.26343053218778, policy_loss: 0.712169, fdm_loss: 0.007770


episode_reward: -291.3Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.005679
FDM train: iteration: 1000, fdm_loss: 0.006135
FDM train: iteration: 1500, fdm_loss: 0.009778
FDM train: iteration: 2000, fdm_loss: 0.004504
FDM train: iteration: 2500, fdm_loss: 0.010419
FDM train: iteration: 3000, fdm_loss: 0.007906
FDM train: iteration: 3500, fdm_loss: 0.005515
FDM train: iteration: 4000, fdm_loss: 0.015817
FDM train: iteration: 4500, fdm_loss: 0.006310
FDM train: iteration: 5000, fdm_loss: 0.011583

Background Trial: 1, reward: -521.9634690930473
Background Trial: 2, reward: -449.55160364639306
Background Trial: 3, reward: -284.45833451914865
Background Trial: 4, reward: -449.5199103336591
Background Trial: 5, reward: -372.7138185063219
Background Trial: 6, reward: -568.2609642355271
Background Trial: 7, reward: -551.8790774686581
Background Trial: 8, reward: -507.4847379146692
Background Trial: 9, reward: -451.5165007846625
Iteration: 6, average_reward: -461.9276018335652, policy_loss: 0.813744, fdm_loss: 0.011281


episode_reward: -378.6
Background Trial: 1, reward: -378.50039347403
Background Trial: 2, reward: -257.8159913973856
Background Trial: 3, reward: -596.2975981634363
Background Trial: 4, reward: -230.27653386275549
Background Trial: 5, reward: -188.46494073972343
Background Trial: 6, reward: -312.25048993993363
Background Trial: 7, reward: -99.18716236216991
Background Trial: 8, reward: -498.7954453808823
Background Trial: 9, reward: -220.82525517372522
Iteration: 7, average_reward: -309.15709005489356, policy_loss: 0.710864, fdm_loss: 0.008549


episode_reward: -47.6Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.007862
FDM train: iteration: 1000, fdm_loss: 0.008078
FDM train: iteration: 1500, fdm_loss: 0.007602
FDM train: iteration: 2000, fdm_loss: 0.007047
FDM train: iteration: 2500, fdm_loss: 0.007191
FDM train: iteration: 3000, fdm_loss: 0.007111
FDM train: iteration: 3500, fdm_loss: 0.007874
FDM train: iteration: 4000, fdm_loss: 0.009696
FDM train: iteration: 4500, fdm_loss: 0.009789
FDM train: iteration: 5000, fdm_loss: 0.013823

Background Trial: 1, reward: -53.79535968950921
Background Trial: 2, reward: -135.03816860390305
Background Trial: 3, reward: -170.03595067455808
Background Trial: 4, reward: -61.47597872100726
Background Trial: 5, reward: -66.11395395275032
Background Trial: 6, reward: -27.813066218471917
Background Trial: 7, reward: -33.06340095974667
Background Trial: 8, reward: -109.97923905506022
Background Trial: 9, reward: -221.50137762198048
Iteration: 8, average_reward: -97.64627727744303, policy_loss: 0.755434, fdm_loss: 0.005662


episode_reward: -87.7
Background Trial: 1, reward: -39.96002895766967
Background Trial: 2, reward: -127.09141051940591
Background Trial: 3, reward: -104.6431468651453
Background Trial: 4, reward: -200.94335674341238
Background Trial: 5, reward: -23.180899200602397
Background Trial: 6, reward: -226.85529018953474
Background Trial: 7, reward: -311.67632642248986
Background Trial: 8, reward: 1.574453328849188
Background Trial: 9, reward: -123.3358591563202
Iteration: 9, average_reward: -128.45687385841458, policy_loss: 0.665853, fdm_loss: 0.013209


episode_reward: -113.9Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.007725
FDM train: iteration: 1000, fdm_loss: 0.011616
FDM train: iteration: 1500, fdm_loss: 0.005528
FDM train: iteration: 2000, fdm_loss: 0.007821
FDM train: iteration: 2500, fdm_loss: 0.005910
FDM train: iteration: 3000, fdm_loss: 0.009577
FDM train: iteration: 3500, fdm_loss: 0.005607
FDM train: iteration: 4000, fdm_loss: 0.011770
FDM train: iteration: 4500, fdm_loss: 0.005527
FDM train: iteration: 5000, fdm_loss: 0.010578

Background Trial: 1, reward: -99.64410322244227
Background Trial: 2, reward: -78.36017532875177
Background Trial: 3, reward: -55.04273203689209
Background Trial: 4, reward: -95.255012709464
Background Trial: 5, reward: 42.99149900885149
Background Trial: 6, reward: -55.301190959881424
Background Trial: 7, reward: -35.49025875359405
Background Trial: 8, reward: -101.13387508249396
Background Trial: 9, reward: -80.2373350206735
Iteration: 10, average_reward: -61.9414649005935, policy_loss: 0.675791, fdm_loss: 0.010561


episode_reward: -74.6
Background Trial: 1, reward: -108.93508586422618
Background Trial: 2, reward: -347.822531640234
Background Trial: 3, reward: -20.273140628009955
Background Trial: 4, reward: -306.267450346104
Background Trial: 5, reward: -3.047370762701817
Background Trial: 6, reward: -33.46783420005602
Background Trial: 7, reward: -316.74589705190976
Background Trial: 8, reward: -272.9074912084926
Background Trial: 9, reward: -289.99502409286043
Iteration: 11, average_reward: -188.82909175495496, policy_loss: 0.586297, fdm_loss: 0.008064


episode_reward: -102.9Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.007541
FDM train: iteration: 1000, fdm_loss: 0.007498
FDM train: iteration: 1500, fdm_loss: 0.006641
FDM train: iteration: 2000, fdm_loss: 0.006940
FDM train: iteration: 2500, fdm_loss: 0.006163
FDM train: iteration: 3000, fdm_loss: 0.009270
FDM train: iteration: 3500, fdm_loss: 0.007288
FDM train: iteration: 4000, fdm_loss: 0.005776
FDM train: iteration: 4500, fdm_loss: 0.010385
FDM train: iteration: 5000, fdm_loss: 0.007737

Background Trial: 1, reward: -243.19798102525536
Background Trial: 2, reward: -296.7982265676057
Background Trial: 3, reward: -261.5821194379356
Background Trial: 4, reward: -309.10923343307786
Background Trial: 5, reward: -14.388513647436568
Background Trial: 6, reward: -66.12221726156824
Background Trial: 7, reward: -32.20715210403563
Background Trial: 8, reward: 10.93114831432517
Background Trial: 9, reward: -277.380415215845
Iteration: 12, average_reward: -165.53941226427054, policy_loss: 0.623985, fdm_loss: 0.008963


episode_reward: -50.9
Background Trial: 1, reward: -73.68982002410198
Background Trial: 2, reward: -93.4385610768266
Background Trial: 3, reward: -50.20959713341479
Background Trial: 4, reward: -19.353430396289397
Background Trial: 5, reward: -12.303243649546829
Background Trial: 6, reward: -84.6664044679635
Background Trial: 7, reward: -76.80834941175674
Background Trial: 8, reward: -150.1740702905876
Background Trial: 9, reward: -51.77445142894997
Iteration: 13, average_reward: -68.0464364310486, policy_loss: 0.597378, fdm_loss: 0.005647


episode_reward: -13.6Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.005757
FDM train: iteration: 1000, fdm_loss: 0.006859
FDM train: iteration: 1500, fdm_loss: 0.008637
FDM train: iteration: 2000, fdm_loss: 0.005709
FDM train: iteration: 2500, fdm_loss: 0.009108
FDM train: iteration: 3000, fdm_loss: 0.008699
FDM train: iteration: 3500, fdm_loss: 0.008470
FDM train: iteration: 4000, fdm_loss: 0.008141
FDM train: iteration: 4500, fdm_loss: 0.009202
FDM train: iteration: 5000, fdm_loss: 0.007823

Background Trial: 1, reward: -3.7515410597485896
Background Trial: 2, reward: -42.325642466235244
Background Trial: 3, reward: -75.06147584748507
Background Trial: 4, reward: -34.75927839013718
Background Trial: 5, reward: -57.53733816623531
Background Trial: 6, reward: -46.269786245778775
Background Trial: 7, reward: -47.871883570142685
Background Trial: 8, reward: -20.740706364477575
Background Trial: 9, reward: -60.95895164935733
Iteration: 14, average_reward: -43.25295597328865, policy_loss: 0.613316, fdm_loss: 0.008429


episode_reward: -26.7
Background Trial: 1, reward: -69.50462170966415
Background Trial: 2, reward: -58.62590857881057
Background Trial: 3, reward: -42.92049383643715
Background Trial: 4, reward: -56.10185682547317
Background Trial: 5, reward: -80.1930111085198
Background Trial: 6, reward: -94.3206092251537
Background Trial: 7, reward: -90.95984008550907
Background Trial: 8, reward: -127.25938138522534
Background Trial: 9, reward: -302.7444130512348
Iteration: 15, average_reward: -102.51445953400308, policy_loss: 0.586163, fdm_loss: 0.005676


episode_reward: -44.8Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.007239
FDM train: iteration: 1000, fdm_loss: 0.007462
FDM train: iteration: 1500, fdm_loss: 0.006092
FDM train: iteration: 2000, fdm_loss: 0.008068
FDM train: iteration: 2500, fdm_loss: 0.011045
FDM train: iteration: 3000, fdm_loss: 0.006622
FDM train: iteration: 3500, fdm_loss: 0.008044
FDM train: iteration: 4000, fdm_loss: 0.007350
FDM train: iteration: 4500, fdm_loss: 0.007023
FDM train: iteration: 5000, fdm_loss: 0.008856

Background Trial: 1, reward: -81.67019691269807
Background Trial: 2, reward: -83.27332934350002
Background Trial: 3, reward: -89.84729485346892
Background Trial: 4, reward: -56.86870653424622
Background Trial: 5, reward: 6.05215137971112
Background Trial: 6, reward: -99.16554509017557
Background Trial: 7, reward: -99.13980023596926
Background Trial: 8, reward: -98.40422365346359
Background Trial: 9, reward: -41.06763228095739
Iteration: 16, average_reward: -71.48717528052977, policy_loss: 0.589866, fdm_loss: 0.008760


episode_reward: -46.5
Background Trial: 1, reward: -237.18234135029797
Background Trial: 2, reward: -286.26352743898246
Background Trial: 3, reward: -176.30718893689422
Background Trial: 4, reward: -289.9458544929686
Background Trial: 5, reward: -73.71165183199327
Background Trial: 6, reward: 40.679619824401016
Background Trial: 7, reward: -0.5102093467036042
Background Trial: 8, reward: -170.38138381941755
Background Trial: 9, reward: -224.94875435465747
Iteration: 17, average_reward: -157.61903241639047, policy_loss: 0.617282, fdm_loss: 0.005813


episode_reward: -35.9Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.006485
FDM train: iteration: 1000, fdm_loss: 0.005284
FDM train: iteration: 1500, fdm_loss: 0.009098
FDM train: iteration: 2000, fdm_loss: 0.008781
FDM train: iteration: 2500, fdm_loss: 0.009153
FDM train: iteration: 3000, fdm_loss: 0.005922
FDM train: iteration: 3500, fdm_loss: 0.008283
FDM train: iteration: 4000, fdm_loss: 0.006723
FDM train: iteration: 4500, fdm_loss: 0.007249
FDM train: iteration: 5000, fdm_loss: 0.008345

Background Trial: 1, reward: -36.77243316679933
Background Trial: 2, reward: 10.105964880590221
Background Trial: 3, reward: -25.780180875324206
Background Trial: 4, reward: -6.87695254187139
Background Trial: 5, reward: -22.233215982303776
Background Trial: 6, reward: -128.73834100598572
Background Trial: 7, reward: -58.92008360374163
Background Trial: 8, reward: -64.53186022955794
Background Trial: 9, reward: -43.28046064999597
Iteration: 18, average_reward: -41.89195146388775, policy_loss: 0.553104, fdm_loss: 0.008654


episode_reward: -18.4
Background Trial: 1, reward: -132.77930567118653
Background Trial: 2, reward: -117.6194011776191
Background Trial: 3, reward: 2.048205859169954
Background Trial: 4, reward: -169.69488821999533
Background Trial: 5, reward: -63.79725305011667
Background Trial: 6, reward: -233.75515658199492
Background Trial: 7, reward: -167.4453348722585
Background Trial: 8, reward: -255.2423157260038
Background Trial: 9, reward: -199.40351492748619
Iteration: 19, average_reward: -148.63210715194344, policy_loss: 0.514388, fdm_loss: 0.009419


episode_reward: -12.3Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.008149
FDM train: iteration: 1000, fdm_loss: 0.004488
FDM train: iteration: 1500, fdm_loss: 0.008566
FDM train: iteration: 2000, fdm_loss: 0.007832
FDM train: iteration: 2500, fdm_loss: 0.006254
FDM train: iteration: 3000, fdm_loss: 0.006066
FDM train: iteration: 3500, fdm_loss: 0.009808
FDM train: iteration: 4000, fdm_loss: 0.005865
FDM train: iteration: 4500, fdm_loss: 0.008578
FDM train: iteration: 5000, fdm_loss: 0.006753

Background Trial: 1, reward: 3.8563592233481643
Background Trial: 2, reward: -86.13752474777749
Background Trial: 3, reward: -44.9340700044266
Background Trial: 4, reward: -208.51053642138717
Background Trial: 5, reward: -242.65861016959022
Background Trial: 6, reward: 5.492400625190243
Background Trial: 7, reward: -26.691683512098223
Background Trial: 8, reward: -61.87991695556348
Background Trial: 9, reward: -58.70425469204221
Iteration: 20, average_reward: -80.01864851714966, policy_loss: 0.744743, fdm_loss: 0.010909


episode_reward:  13.1
Background Trial: 1, reward: -54.45541239462685
Background Trial: 2, reward: -37.29352485299229
Background Trial: 3, reward: -74.68076371884024
Background Trial: 4, reward: -143.98750442840964
Background Trial: 5, reward: -43.916950805016846
Background Trial: 6, reward: -32.52367774541045
Background Trial: 7, reward: -36.188673284180815
Background Trial: 8, reward: -14.677872493765776
Background Trial: 9, reward: -122.09986245511902
Iteration: 21, average_reward: -62.20269357537355, policy_loss: 0.649050, fdm_loss: 0.010597


episode_reward:  -1.7Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.012047
FDM train: iteration: 1000, fdm_loss: 0.008189
FDM train: iteration: 1500, fdm_loss: 0.009694
FDM train: iteration: 2000, fdm_loss: 0.004676
FDM train: iteration: 2500, fdm_loss: 0.005335
FDM train: iteration: 3000, fdm_loss: 0.007153
FDM train: iteration: 3500, fdm_loss: 0.007177
FDM train: iteration: 4000, fdm_loss: 0.006393
FDM train: iteration: 4500, fdm_loss: 0.004373
FDM train: iteration: 5000, fdm_loss: 0.010322

Background Trial: 1, reward: -81.45021606426326
Background Trial: 2, reward: -1.972703650049283
Background Trial: 3, reward: -185.36861891610863
Background Trial: 4, reward: -167.40575745350304
Background Trial: 5, reward: -147.3787552984421
Background Trial: 6, reward: 34.28908576462129
Background Trial: 7, reward: -484.43615559851804
Background Trial: 8, reward: -276.2224380021245
Background Trial: 9, reward: -179.9770630179406
Iteration: 22, average_reward: -165.5469580262587, policy_loss: 0.531192, fdm_loss: 0.010289


episode_reward: -45.9
Background Trial: 1, reward: -393.0063538781754
Background Trial: 2, reward: -202.8419138107203
Background Trial: 3, reward: -163.75294434562886
Background Trial: 4, reward: -87.73568768261634
Background Trial: 5, reward: -316.0837885739953
Background Trial: 6, reward: -30.690123515229487
Background Trial: 7, reward: -115.5943146535722
Background Trial: 8, reward: -372.2168077846217
Background Trial: 9, reward: -475.9386442882066
Iteration: 23, average_reward: -239.7622865036407, policy_loss: 0.507468, fdm_loss: 0.008100


episode_reward: -47.5Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.006546
FDM train: iteration: 1000, fdm_loss: 0.008248
FDM train: iteration: 1500, fdm_loss: 0.004759
FDM train: iteration: 2000, fdm_loss: 0.005582
FDM train: iteration: 2500, fdm_loss: 0.009278
FDM train: iteration: 3000, fdm_loss: 0.006131
FDM train: iteration: 3500, fdm_loss: 0.007835
FDM train: iteration: 4000, fdm_loss: 0.005872
FDM train: iteration: 4500, fdm_loss: 0.010172
FDM train: iteration: 5000, fdm_loss: 0.006363

Background Trial: 1, reward: -66.6423221091816
Background Trial: 2, reward: -45.00405419698481
Background Trial: 3, reward: -55.32964460008198
Background Trial: 4, reward: -41.12813063081737
Background Trial: 5, reward: 27.143613089151344
Background Trial: 6, reward: -183.1739250216246
Background Trial: 7, reward: -167.6944933982849
Background Trial: 8, reward: -30.823100534091026
Background Trial: 9, reward: -125.72792389409466
Iteration: 24, average_reward: -76.48666458844552, policy_loss: 0.552324, fdm_loss: 0.004703


episode_reward:  30.6
Background Trial: 1, reward: -48.00762152553686
Background Trial: 2, reward: -48.65360620533323
Background Trial: 3, reward: -476.7242331885758
Background Trial: 4, reward: -43.88463439622815
Background Trial: 5, reward: -28.226358112588855
Background Trial: 6, reward: -353.3436033116649
Background Trial: 7, reward: -23.681360152886953
Background Trial: 8, reward: -187.15083104603147
Background Trial: 9, reward: -97.14966639976798
Iteration: 25, average_reward: -145.2024349265127, policy_loss: 0.544443, fdm_loss: 0.009851


episode_reward:  21.2Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.005513
FDM train: iteration: 1000, fdm_loss: 0.006985
FDM train: iteration: 1500, fdm_loss: 0.006410
FDM train: iteration: 2000, fdm_loss: 0.006382
FDM train: iteration: 2500, fdm_loss: 0.004451
FDM train: iteration: 3000, fdm_loss: 0.005216
FDM train: iteration: 3500, fdm_loss: 0.006252
FDM train: iteration: 4000, fdm_loss: 0.012224
FDM train: iteration: 4500, fdm_loss: 0.007031
FDM train: iteration: 5000, fdm_loss: 0.005461

Background Trial: 1, reward: -11.469315547866572
Background Trial: 2, reward: -69.32289070543555
Background Trial: 3, reward: 23.138190480458178
Background Trial: 4, reward: -33.66134134029089
Background Trial: 5, reward: -93.87978541056263
Background Trial: 6, reward: -37.49101030117901
Background Trial: 7, reward: -91.67373988701425
Background Trial: 8, reward: -80.72971625123918
Background Trial: 9, reward: -16.562505498625697
Iteration: 26, average_reward: -45.739123829083965, policy_loss: 0.594877, fdm_loss: 0.007028


episode_reward: -42.5
Background Trial: 1, reward: -333.69516275647743
Background Trial: 2, reward: -45.36170594656345
Background Trial: 3, reward: -29.26267449682632
Background Trial: 4, reward: -201.6130037727055
Background Trial: 5, reward: -98.71582977868266
Background Trial: 6, reward: -35.379617591885335
Background Trial: 7, reward: -229.66599129947627
Background Trial: 8, reward: -27.527579963677653
Background Trial: 9, reward: -13.755677600598148
Iteration: 27, average_reward: -112.77524924521032, policy_loss: 0.609600, fdm_loss: 0.004435


episode_reward: -70.1Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.004150
FDM train: iteration: 1000, fdm_loss: 0.005241
FDM train: iteration: 1500, fdm_loss: 0.008964
FDM train: iteration: 2000, fdm_loss: 0.005159
FDM train: iteration: 2500, fdm_loss: 0.006340
FDM train: iteration: 3000, fdm_loss: 0.006228
FDM train: iteration: 3500, fdm_loss: 0.006553
FDM train: iteration: 4000, fdm_loss: 0.006692
FDM train: iteration: 4500, fdm_loss: 0.008404
FDM train: iteration: 5000, fdm_loss: 0.007635

Background Trial: 1, reward: -321.31733039837457
Background Trial: 2, reward: -233.84621913452222
Background Trial: 3, reward: -285.5081791422044
Background Trial: 4, reward: 4.412563788238245
Background Trial: 5, reward: -307.3501134414935
Background Trial: 6, reward: -57.97509030387916
Background Trial: 7, reward: -345.38663523516846
Background Trial: 8, reward: 26.29314776876859
Background Trial: 9, reward: -235.00308699339348
Iteration: 28, average_reward: -195.07566034355878, policy_loss: 0.566750, fdm_loss: 0.009094


episode_reward: -13.8
Background Trial: 1, reward: -14.82615005117988
Background Trial: 2, reward: -12.077827004087368
Background Trial: 3, reward: -27.38080922195823
Background Trial: 4, reward: -23.896594598743903
Background Trial: 5, reward: -419.3952738445041
Background Trial: 6, reward: -23.102675048173822
Background Trial: 7, reward: 10.140945982533808
Background Trial: 8, reward: -30.896115600193284
Background Trial: 9, reward: -29.256696102233363
Iteration: 29, average_reward: -63.41013283206002, policy_loss: 0.563166, fdm_loss: 0.008446


episode_reward:   8.5Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.008130
FDM train: iteration: 1000, fdm_loss: 0.006942
FDM train: iteration: 1500, fdm_loss: 0.005881
FDM train: iteration: 2000, fdm_loss: 0.005503
FDM train: iteration: 2500, fdm_loss: 0.009119
FDM train: iteration: 3000, fdm_loss: 0.005867
FDM train: iteration: 3500, fdm_loss: 0.007427
FDM train: iteration: 4000, fdm_loss: 0.004867
FDM train: iteration: 4500, fdm_loss: 0.004825
FDM train: iteration: 5000, fdm_loss: 0.006151

Background Trial: 1, reward: -604.9659908391104
Background Trial: 2, reward: -216.42448207030662
Background Trial: 3, reward: -29.693004587773885
Background Trial: 4, reward: -32.14177441658863
Background Trial: 5, reward: -40.274017436077145
Background Trial: 6, reward: -24.098766674681443
Background Trial: 7, reward: -33.27852051054552
Background Trial: 8, reward: -210.66904926695025
Background Trial: 9, reward: -78.80974730584663
Iteration: 30, average_reward: -141.1505947897645, policy_loss: 0.570077, fdm_loss: 0.004774

