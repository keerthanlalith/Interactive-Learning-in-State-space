Collecting dynamics training data 1000
Collecting dynamics training data 2000
Collecting dynamics training data 3000
Collecting dynamics training data 4000
Collecting dynamics training data 5000
Collecting dynamics training data 6000
Collecting dynamics training data 7000
Collecting dynamics training data 8000
Collecting dynamics training data 9000
Collecting dynamics training data 10000
FDM train: iteration: 500, fdm_loss: 0.022193
FDM train: iteration: 1000, fdm_loss: 0.011987
FDM train: iteration: 1500, fdm_loss: 0.007797
FDM train: iteration: 2000, fdm_loss: 0.006312
FDM train: iteration: 2500, fdm_loss: 0.009918
FDM train: iteration: 3000, fdm_loss: 0.006356
FDM train: iteration: 3500, fdm_loss: 0.008515
FDM train: iteration: 4000, fdm_loss: 0.009632
FDM train: iteration: 4500, fdm_loss: 0.008048
FDM train: iteration: 5000, fdm_loss: 0.007370

episode_reward: 247.1
Background Trial: 1, reward: -122.79239662066169
Background Trial: 2, reward: -43.69855075528025
Background Trial: 3, reward: -186.21136154560406
Background Trial: 4, reward: -74.27899799856242
Background Trial: 5, reward: -282.8853962852178
Background Trial: 6, reward: -72.78019881495969
Background Trial: 7, reward: -90.94021361469822
Background Trial: 8, reward: -83.90953146549494
Background Trial: 9, reward: -131.91870687418668
Iteration: 1, average_reward: -121.04615044162954, policy_loss: 0.981489, fdm_loss: 0.003481


episode_reward: 298.6Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.005720
FDM train: iteration: 1000, fdm_loss: 0.007210
FDM train: iteration: 1500, fdm_loss: 0.007576
FDM train: iteration: 2000, fdm_loss: 0.011915
FDM train: iteration: 2500, fdm_loss: 0.006510
FDM train: iteration: 3000, fdm_loss: 0.011751
FDM train: iteration: 3500, fdm_loss: 0.003486
FDM train: iteration: 4000, fdm_loss: 0.004525
FDM train: iteration: 4500, fdm_loss: 0.007278
FDM train: iteration: 5000, fdm_loss: 0.005230

Background Trial: 1, reward: -47.83983173258878
Background Trial: 2, reward: -28.516211775296327
Background Trial: 3, reward: -11.270103652003144
Background Trial: 4, reward: -72.56069841772509
Background Trial: 5, reward: -20.69185821798912
Background Trial: 6, reward: -36.607758448041324
Background Trial: 7, reward: -100.18083214784765
Background Trial: 8, reward: -44.42628719014226
Background Trial: 9, reward: -28.276836131538403
Iteration: 2, average_reward: -43.374490857019126, policy_loss: 0.984760, fdm_loss: 0.006944


episode_reward: 258.5
Background Trial: 1, reward: -321.4191249881113
Background Trial: 2, reward: -275.054271408592
Background Trial: 3, reward: -366.5632573362275
Background Trial: 4, reward: -274.8921501790562
Background Trial: 5, reward: -312.18354188278215
Background Trial: 6, reward: -353.2117764498453
Background Trial: 7, reward: -95.19639131826332
Background Trial: 8, reward: -278.18998678748767
Background Trial: 9, reward: -279.1793324144875
Iteration: 3, average_reward: -283.98775919609477, policy_loss: 0.826421, fdm_loss: 0.006816


episode_reward: -335.3Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.006247
FDM train: iteration: 1000, fdm_loss: 0.005944
FDM train: iteration: 1500, fdm_loss: 0.004204
FDM train: iteration: 2000, fdm_loss: 0.009012
FDM train: iteration: 2500, fdm_loss: 0.006561
FDM train: iteration: 3000, fdm_loss: 0.003948
FDM train: iteration: 3500, fdm_loss: 0.006848
FDM train: iteration: 4000, fdm_loss: 0.002466
FDM train: iteration: 4500, fdm_loss: 0.005839
FDM train: iteration: 5000, fdm_loss: 0.005312

Background Trial: 1, reward: -359.247385546283
Background Trial: 2, reward: -370.1920248885796
Background Trial: 3, reward: -333.66426500943066
Background Trial: 4, reward: -204.15241772630034
Background Trial: 5, reward: -350.0485562712612
Background Trial: 6, reward: -353.07131060977304
Background Trial: 7, reward: -259.0612245725832
Background Trial: 8, reward: -290.522373884871
Background Trial: 9, reward: -396.92649633953283
Iteration: 4, average_reward: -324.09845053873494, policy_loss: 0.757948, fdm_loss: 0.005040


episode_reward: 220.9
Background Trial: 1, reward: -265.89285773607253
Background Trial: 2, reward: -257.0917817425979
Background Trial: 3, reward: -259.68184901206644
Background Trial: 4, reward: -186.097344868456
Background Trial: 5, reward: -261.0411667814474
Background Trial: 6, reward: -191.13118737390164
Background Trial: 7, reward: -192.97598989358468
Background Trial: 8, reward: -257.768204937781
Background Trial: 9, reward: -264.01812988528286
Iteration: 5, average_reward: -237.29983469235452, policy_loss: 0.607963, fdm_loss: 0.006044


episode_reward: -257.6Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.005261
FDM train: iteration: 1000, fdm_loss: 0.006216
FDM train: iteration: 1500, fdm_loss: 0.005450
FDM train: iteration: 2000, fdm_loss: 0.004256
FDM train: iteration: 2500, fdm_loss: 0.005741
FDM train: iteration: 3000, fdm_loss: 0.009447
FDM train: iteration: 3500, fdm_loss: 0.007184
FDM train: iteration: 4000, fdm_loss: 0.003381
FDM train: iteration: 4500, fdm_loss: 0.007660
FDM train: iteration: 5000, fdm_loss: 0.003879

Background Trial: 1, reward: -367.9972089135948
Background Trial: 2, reward: -599.508779434148
Background Trial: 3, reward: -147.14357595368512
Background Trial: 4, reward: -18.210948975228348
Background Trial: 5, reward: -289.8027846597581
Background Trial: 6, reward: -384.2751273486223
Background Trial: 7, reward: -80.62902438880917
Background Trial: 8, reward: 28.77036899564149
Background Trial: 9, reward: -69.48589781888919
Iteration: 6, average_reward: -214.25366427745487, policy_loss: 0.739056, fdm_loss: 0.008691


episode_reward: 277.9
Background Trial: 1, reward: -499.10764948902136
Background Trial: 2, reward: -366.83841320962676
Background Trial: 3, reward: -100.81892920305118
Background Trial: 4, reward: 255.45417152225824
Background Trial: 5, reward: 238.73103191468192
Background Trial: 6, reward: -363.3327881694413
Background Trial: 7, reward: -381.55683521881105
Background Trial: 8, reward: -412.6609872078505
Background Trial: 9, reward: -430.288426286024
Iteration: 7, average_reward: -228.9354250385429, policy_loss: 0.694268, fdm_loss: 0.007495


episode_reward: 261.4Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.010007
FDM train: iteration: 1000, fdm_loss: 0.002861
FDM train: iteration: 1500, fdm_loss: 0.003367
FDM train: iteration: 2000, fdm_loss: 0.005151
FDM train: iteration: 2500, fdm_loss: 0.006886
FDM train: iteration: 3000, fdm_loss: 0.004317
FDM train: iteration: 3500, fdm_loss: 0.004159
FDM train: iteration: 4000, fdm_loss: 0.004245
FDM train: iteration: 4500, fdm_loss: 0.004147
FDM train: iteration: 5000, fdm_loss: 0.006660

Background Trial: 1, reward: -267.65551419129815
Background Trial: 2, reward: -270.38681709659176
Background Trial: 3, reward: -372.97251230373905
Background Trial: 4, reward: -404.56700213079324
Background Trial: 5, reward: -294.3131022216607
Background Trial: 6, reward: -348.5948544547724
Background Trial: 7, reward: -314.1935640370397
Background Trial: 8, reward: -182.0819504184977
Background Trial: 9, reward: -196.77348483711773
Iteration: 8, average_reward: -294.61542241016787, policy_loss: 0.662961, fdm_loss: 0.007049


episode_reward: 251.8
Background Trial: 1, reward: -68.18468874426577
Background Trial: 2, reward: -325.7627670668862
Background Trial: 3, reward: -63.64989014168392
Background Trial: 4, reward: -390.14256556537987
Background Trial: 5, reward: -25.247031308158128
Background Trial: 6, reward: -124.5347714698493
Background Trial: 7, reward: -20.594242135039778
Background Trial: 8, reward: -19.565392629821318
Background Trial: 9, reward: -339.3561996666619
Iteration: 9, average_reward: -153.0041720808607, policy_loss: 0.583826, fdm_loss: 0.008771


episode_reward: 254.1Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.003596
FDM train: iteration: 1000, fdm_loss: 0.008160
FDM train: iteration: 1500, fdm_loss: 0.004346
FDM train: iteration: 2000, fdm_loss: 0.005737
FDM train: iteration: 2500, fdm_loss: 0.005206
FDM train: iteration: 3000, fdm_loss: 0.004812
FDM train: iteration: 3500, fdm_loss: 0.007015
FDM train: iteration: 4000, fdm_loss: 0.005034
FDM train: iteration: 4500, fdm_loss: 0.006762
FDM train: iteration: 5000, fdm_loss: 0.003579

Background Trial: 1, reward: -33.377924464351224
Background Trial: 2, reward: -251.16466826520715
Background Trial: 3, reward: -52.8018321109257
Background Trial: 4, reward: -73.48046340376672
Background Trial: 5, reward: -108.23785641734956
Background Trial: 6, reward: -72.47230727989752
Background Trial: 7, reward: -425.17444549492285
Background Trial: 8, reward: -398.8761305960604
Background Trial: 9, reward: -274.4602675052041
Iteration: 10, average_reward: -187.78287728196503, policy_loss: 0.675113, fdm_loss: 0.003630


episode_reward: 242.6
Background Trial: 1, reward: -337.44232061623524
Background Trial: 2, reward: 246.5409519218416
Background Trial: 3, reward: -192.23388899669487
Background Trial: 4, reward: -302.1071440199869
Background Trial: 5, reward: -305.53366916784216
Background Trial: 6, reward: -114.62067966771104
Background Trial: 7, reward: -169.52279423354557
Background Trial: 8, reward: 29.027522590502485
Background Trial: 9, reward: -149.1423935817022
Iteration: 11, average_reward: -143.892712863486, policy_loss: 0.692329, fdm_loss: 0.006254


episode_reward:  46.3Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.006037
FDM train: iteration: 1000, fdm_loss: 0.004769
FDM train: iteration: 1500, fdm_loss: 0.003933
FDM train: iteration: 2000, fdm_loss: 0.010949
FDM train: iteration: 2500, fdm_loss: 0.008204
FDM train: iteration: 3000, fdm_loss: 0.004915
FDM train: iteration: 3500, fdm_loss: 0.005290
FDM train: iteration: 4000, fdm_loss: 0.005822
FDM train: iteration: 4500, fdm_loss: 0.004392
FDM train: iteration: 5000, fdm_loss: 0.008590

Background Trial: 1, reward: -341.72375158851565
Background Trial: 2, reward: -185.90164156874698
Background Trial: 3, reward: -307.7387982437733
Background Trial: 4, reward: 9.059183114159893
Background Trial: 5, reward: -321.27974523991577
Background Trial: 6, reward: 240.06411902372275
Background Trial: 7, reward: -345.5730195450094
Background Trial: 8, reward: -309.72068433348227
Background Trial: 9, reward: -338.625629697833
Iteration: 12, average_reward: -211.27110756437708, policy_loss: 0.625347, fdm_loss: 0.007350


episode_reward: 276.6
Background Trial: 1, reward: -325.47264821681154
Background Trial: 2, reward: -329.3914476999365
Background Trial: 3, reward: 31.607654615722424
Background Trial: 4, reward: -323.3387891448001
Background Trial: 5, reward: -203.62957510154655
Background Trial: 6, reward: -311.6872757183959
Background Trial: 7, reward: -56.25677027141478
Background Trial: 8, reward: -269.55066591335424
Background Trial: 9, reward: -81.20697553192004
Iteration: 13, average_reward: -207.65849922027303, policy_loss: 0.673667, fdm_loss: 0.003862


episode_reward: 267.1Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.004182
FDM train: iteration: 1000, fdm_loss: 0.003938
FDM train: iteration: 1500, fdm_loss: 0.006445
FDM train: iteration: 2000, fdm_loss: 0.004773
FDM train: iteration: 2500, fdm_loss: 0.004588
FDM train: iteration: 3000, fdm_loss: 0.010527
FDM train: iteration: 3500, fdm_loss: 0.011973
FDM train: iteration: 4000, fdm_loss: 0.007685
FDM train: iteration: 4500, fdm_loss: 0.003680
FDM train: iteration: 5000, fdm_loss: 0.005319

Background Trial: 1, reward: -268.72207158449805
Background Trial: 2, reward: -371.3913423594092
Background Trial: 3, reward: -433.3964163725842
Background Trial: 4, reward: -263.3745293674829
Background Trial: 5, reward: 8.082097258680491
Background Trial: 6, reward: 9.470676962751156
Background Trial: 7, reward: -373.8603245628398
Background Trial: 8, reward: -251.72093866693416
Background Trial: 9, reward: -264.8005586851724
Iteration: 14, average_reward: -245.52371193083212, policy_loss: 0.631126, fdm_loss: 0.002262


episode_reward:  23.2
Background Trial: 1, reward: -33.14410780989354
Background Trial: 2, reward: -192.75921650641135
Background Trial: 3, reward: -57.995762064265186
Background Trial: 4, reward: -45.32770166019008
Background Trial: 5, reward: 2.377227783461322
Background Trial: 6, reward: 22.108526986990654
Background Trial: 7, reward: -197.93145950619885
Background Trial: 8, reward: -23.688997324398542
Background Trial: 9, reward: -56.77315332831196
Iteration: 15, average_reward: -64.79273815880195, policy_loss: 0.687038, fdm_loss: 0.002891


episode_reward: 295.4Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.002020
FDM train: iteration: 1000, fdm_loss: 0.003254
FDM train: iteration: 1500, fdm_loss: 0.004662
FDM train: iteration: 2000, fdm_loss: 0.005341
FDM train: iteration: 2500, fdm_loss: 0.005002
FDM train: iteration: 3000, fdm_loss: 0.005432
FDM train: iteration: 3500, fdm_loss: 0.004200
FDM train: iteration: 4000, fdm_loss: 0.005589
FDM train: iteration: 4500, fdm_loss: 0.007658
FDM train: iteration: 5000, fdm_loss: 0.005834

Background Trial: 1, reward: -264.35105495619723
Background Trial: 2, reward: -399.8029960767529
Background Trial: 3, reward: -432.65455903805895
Background Trial: 4, reward: -600.3668041942868
Background Trial: 5, reward: -470.7348911288705
Background Trial: 6, reward: -464.55173164470176
Background Trial: 7, reward: -446.00097827153047
Background Trial: 8, reward: -396.50180363335824
Background Trial: 9, reward: -390.8860076443041
Iteration: 16, average_reward: -429.5389807320068, policy_loss: 0.510104, fdm_loss: 0.003737


episode_reward: 293.2
Background Trial: 1, reward: 279.5287408249279
Background Trial: 2, reward: -78.9877425361906
Background Trial: 3, reward: -245.06351558126784
Background Trial: 4, reward: 233.98627286010085
Background Trial: 5, reward: -75.9615351232559
Background Trial: 6, reward: 238.29196718542192
Background Trial: 7, reward: 44.07051530351745
Background Trial: 8, reward: -27.149916426617864
Background Trial: 9, reward: 24.506553698670075
Iteration: 17, average_reward: 43.69126002281178, policy_loss: 0.595579, fdm_loss: 0.002587


episode_reward: -23.7Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.002841
FDM train: iteration: 1000, fdm_loss: 0.003317
FDM train: iteration: 1500, fdm_loss: 0.006444
FDM train: iteration: 2000, fdm_loss: 0.004248
FDM train: iteration: 2500, fdm_loss: 0.003341
FDM train: iteration: 3000, fdm_loss: 0.005264
FDM train: iteration: 3500, fdm_loss: 0.003135
FDM train: iteration: 4000, fdm_loss: 0.004902
FDM train: iteration: 4500, fdm_loss: 0.005008
FDM train: iteration: 5000, fdm_loss: 0.004783

Background Trial: 1, reward: -220.2098865785723
Background Trial: 2, reward: 26.659423454230705
Background Trial: 3, reward: -257.1571600086471
Background Trial: 4, reward: 20.263519490402345
Background Trial: 5, reward: -249.8539980486162
Background Trial: 6, reward: -46.83613966609457
Background Trial: 7, reward: -5.551293818560254
Background Trial: 8, reward: 261.84785123040047
Background Trial: 9, reward: -90.21338680197734
Iteration: 18, average_reward: -62.33900786082602, policy_loss: 0.616298, fdm_loss: 0.004874


episode_reward: 257.1
Background Trial: 1, reward: 20.72884208659687
Background Trial: 2, reward: 60.34871439457524
Background Trial: 3, reward: -129.44250164079833
Background Trial: 4, reward: 32.836569879998706
Background Trial: 5, reward: 12.092754441150149
Background Trial: 6, reward: -12.10827010177897
Background Trial: 7, reward: -24.986291562398804
Background Trial: 8, reward: 39.30053818731986
Background Trial: 9, reward: -240.89772563301437
Iteration: 19, average_reward: -26.903041105372186, policy_loss: 0.680376, fdm_loss: 0.008160


episode_reward:  12.2Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.005720
FDM train: iteration: 1000, fdm_loss: 0.003647
FDM train: iteration: 1500, fdm_loss: 0.005094
FDM train: iteration: 2000, fdm_loss: 0.005061
FDM train: iteration: 2500, fdm_loss: 0.003982
FDM train: iteration: 3000, fdm_loss: 0.004769
FDM train: iteration: 3500, fdm_loss: 0.004082
FDM train: iteration: 4000, fdm_loss: 0.002802
FDM train: iteration: 4500, fdm_loss: 0.002744
FDM train: iteration: 5000, fdm_loss: 0.002034

Background Trial: 1, reward: 5.405914736552035
Background Trial: 2, reward: 1.8918968020122122
Background Trial: 3, reward: 40.545036366368066
Background Trial: 4, reward: 59.89022484497522
Background Trial: 5, reward: 157.18108129306447
Background Trial: 6, reward: -0.2454990598720599
Background Trial: 7, reward: -33.741927446140636
Background Trial: 8, reward: 14.122401994157656
Background Trial: 9, reward: -25.189296424506026
Iteration: 20, average_reward: 24.428870345178993, policy_loss: 0.691825, fdm_loss: 0.002322


episode_reward: 242.7
Background Trial: 1, reward: 1.1064567779519336
Background Trial: 2, reward: 50.42984385048234
Background Trial: 3, reward: 2.3113477433753076
Background Trial: 4, reward: -6.846696492103234
Background Trial: 5, reward: -24.342918335528708
Background Trial: 6, reward: 33.28202756966567
Background Trial: 7, reward: 6.937978238031647
Background Trial: 8, reward: 36.760647353670265
Background Trial: 9, reward: 55.93999864460935
Iteration: 21, average_reward: 17.286520594461617, policy_loss: 0.794264, fdm_loss: 0.003654


episode_reward:  57.9Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.005265
FDM train: iteration: 1000, fdm_loss: 0.005759
FDM train: iteration: 1500, fdm_loss: 0.004348
FDM train: iteration: 2000, fdm_loss: 0.004874
FDM train: iteration: 2500, fdm_loss: 0.003769
FDM train: iteration: 3000, fdm_loss: 0.002743
FDM train: iteration: 3500, fdm_loss: 0.004141
FDM train: iteration: 4000, fdm_loss: 0.003397
FDM train: iteration: 4500, fdm_loss: 0.001976
FDM train: iteration: 5000, fdm_loss: 0.004498

Background Trial: 1, reward: -152.08723629134636
Background Trial: 2, reward: 278.06745085945903
Background Trial: 3, reward: -2.4841957780745645
Background Trial: 4, reward: -348.6031482777921
Background Trial: 5, reward: 11.921663174201797
Background Trial: 6, reward: 187.14843963001988
Background Trial: 7, reward: -292.27812615967594
Background Trial: 8, reward: 48.123852243418895
Background Trial: 9, reward: -226.34714117703652
Iteration: 22, average_reward: -55.17093797520287, policy_loss: 0.568135, fdm_loss: 0.003575


episode_reward: 264.4
Background Trial: 1, reward: 242.3096033042326
Background Trial: 2, reward: -70.95904596314847
Background Trial: 3, reward: -300.71473082642234
Background Trial: 4, reward: 6.0964454467552684
Background Trial: 5, reward: -225.8264356109774
Background Trial: 6, reward: 1.495841694911718
Background Trial: 7, reward: -310.95186504197216
Background Trial: 8, reward: -125.76082730547759
Background Trial: 9, reward: 52.66368989183198
Iteration: 23, average_reward: -81.29414715669627, policy_loss: 0.549458, fdm_loss: 0.003690


episode_reward: -106.8Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.005823
FDM train: iteration: 1000, fdm_loss: 0.002577
FDM train: iteration: 1500, fdm_loss: 0.003200
FDM train: iteration: 2000, fdm_loss: 0.007395
FDM train: iteration: 2500, fdm_loss: 0.005994
FDM train: iteration: 3000, fdm_loss: 0.005016
FDM train: iteration: 3500, fdm_loss: 0.004659
FDM train: iteration: 4000, fdm_loss: 0.006114
FDM train: iteration: 4500, fdm_loss: 0.004218
FDM train: iteration: 5000, fdm_loss: 0.003230

Background Trial: 1, reward: 287.87349287573534
Background Trial: 2, reward: -136.75952776894482
Background Trial: 3, reward: 282.71635204915765
Background Trial: 4, reward: 265.4578960883763
Background Trial: 5, reward: 279.16487662646136
Background Trial: 6, reward: 262.05354573896227
Background Trial: 7, reward: 11.067680912652975
Background Trial: 8, reward: 231.21888656192294
Background Trial: 9, reward: -202.7843186399119
Iteration: 24, average_reward: 142.2232093827125, policy_loss: 0.508262, fdm_loss: 0.002222


episode_reward: 259.1
Background Trial: 1, reward: 4.873194519370685
Background Trial: 2, reward: -19.293792794802798
Background Trial: 3, reward: -0.4827794909531917
Background Trial: 4, reward: 17.555763292391802
Background Trial: 5, reward: -12.451838361544247
Background Trial: 6, reward: 30.622713309190743
Background Trial: 7, reward: 14.187906563099375
Background Trial: 8, reward: -222.77065331760997
Background Trial: 9, reward: 1.5893306971318992
Iteration: 25, average_reward: -20.68557284263619, policy_loss: 0.689641, fdm_loss: 0.003876


episode_reward:  39.6Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.007046
FDM train: iteration: 1000, fdm_loss: 0.002591
FDM train: iteration: 1500, fdm_loss: 0.002368
FDM train: iteration: 2000, fdm_loss: 0.005341
FDM train: iteration: 2500, fdm_loss: 0.004154
FDM train: iteration: 3000, fdm_loss: 0.004730
FDM train: iteration: 3500, fdm_loss: 0.004495
FDM train: iteration: 4000, fdm_loss: 0.004082
FDM train: iteration: 4500, fdm_loss: 0.003575
FDM train: iteration: 5000, fdm_loss: 0.005682

Background Trial: 1, reward: 29.073708287410795
Background Trial: 2, reward: 26.544760938080486
Background Trial: 3, reward: 3.0525633434047137
Background Trial: 4, reward: 14.837213829898005
Background Trial: 5, reward: -215.24345196371775
Background Trial: 6, reward: 7.283376656413338
Background Trial: 7, reward: -0.09983129894374088
Background Trial: 8, reward: -12.451553931205297
Background Trial: 9, reward: -35.113139873672026
Iteration: 26, average_reward: -20.235150445814607, policy_loss: 0.536698, fdm_loss: 0.002378


episode_reward: 250.7
Background Trial: 1, reward: 270.95799663174563
Background Trial: 2, reward: 251.37934962439448
Background Trial: 3, reward: -300.8474086312408
Background Trial: 4, reward: -258.79914633336494
Background Trial: 5, reward: -245.7772227676471
Background Trial: 6, reward: -63.17542393090618
Background Trial: 7, reward: 187.26273786689666
Background Trial: 8, reward: 262.1411013324756
Background Trial: 9, reward: 171.48612714756095
Iteration: 27, average_reward: 30.514234548879372, policy_loss: 0.501917, fdm_loss: 0.002576


episode_reward: 230.6Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.004652
FDM train: iteration: 1000, fdm_loss: 0.002977
FDM train: iteration: 1500, fdm_loss: 0.003805
FDM train: iteration: 2000, fdm_loss: 0.004607
FDM train: iteration: 2500, fdm_loss: 0.004602
FDM train: iteration: 3000, fdm_loss: 0.003868
FDM train: iteration: 3500, fdm_loss: 0.002857
FDM train: iteration: 4000, fdm_loss: 0.003822
FDM train: iteration: 4500, fdm_loss: 0.003960
FDM train: iteration: 5000, fdm_loss: 0.003713

Background Trial: 1, reward: 229.24178324427012
Background Trial: 2, reward: -174.11822430948718
Background Trial: 3, reward: -402.4341249279532
Background Trial: 4, reward: -306.87564594140713
Background Trial: 5, reward: 17.385648795473188
Background Trial: 6, reward: -34.639967734369904
Background Trial: 7, reward: 241.77568024346354
Background Trial: 8, reward: 265.0585915433108
Background Trial: 9, reward: -187.20742874306043
Iteration: 28, average_reward: -39.09040975886224, policy_loss: 0.464410, fdm_loss: 0.002714


episode_reward: 284.8
Background Trial: 1, reward: 257.54935087379056
Background Trial: 2, reward: 263.0373573509204
Background Trial: 3, reward: 9.435930899220779
Background Trial: 4, reward: -286.9669216397283
Background Trial: 5, reward: -363.61440756567515
Background Trial: 6, reward: -102.95727186892682
Background Trial: 7, reward: 238.7105852858991
Background Trial: 8, reward: 243.03129691511305
Background Trial: 9, reward: 239.02280292182465
Iteration: 29, average_reward: 55.24985813027092, policy_loss: 0.485099, fdm_loss: 0.006260


episode_reward:  18.5Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.002285
FDM train: iteration: 1000, fdm_loss: 0.004050
FDM train: iteration: 1500, fdm_loss: 0.003618
FDM train: iteration: 2000, fdm_loss: 0.005129
FDM train: iteration: 2500, fdm_loss: 0.004317
FDM train: iteration: 3000, fdm_loss: 0.003964
FDM train: iteration: 3500, fdm_loss: 0.004248
FDM train: iteration: 4000, fdm_loss: 0.003707
FDM train: iteration: 4500, fdm_loss: 0.004930
FDM train: iteration: 5000, fdm_loss: 0.002919

Background Trial: 1, reward: -23.864941143559477
Background Trial: 2, reward: -5.319645375156895
Background Trial: 3, reward: 40.57631915352724
Background Trial: 4, reward: -13.566303141923754
Background Trial: 5, reward: -26.400636168164652
Background Trial: 6, reward: -44.61038478925313
Background Trial: 7, reward: 18.625410642172
Background Trial: 8, reward: -26.09253033004687
Background Trial: 9, reward: -78.98741925241674
Iteration: 30, average_reward: -17.737792267202476, policy_loss: 0.579856, fdm_loss: 0.004337


episode_reward:  14.6
Background Trial: 1, reward: -347.2305001620132
Background Trial: 2, reward: -435.9454195473617
Background Trial: 3, reward: -8.770921421960026
Background Trial: 4, reward: -469.68963493341136
Background Trial: 5, reward: 26.823584972829266
Background Trial: 6, reward: 46.04135582961203
Background Trial: 7, reward: 51.72107330453872
Background Trial: 8, reward: 217.65330918432923
Background Trial: 9, reward: 26.147862631306964
Iteration: 31, average_reward: -99.24992112690335, policy_loss: 0.493012, fdm_loss: 0.003419


episode_reward: 221.3Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.002729
FDM train: iteration: 1000, fdm_loss: 0.005505
FDM train: iteration: 1500, fdm_loss: 0.003624
FDM train: iteration: 2000, fdm_loss: 0.003375
FDM train: iteration: 2500, fdm_loss: 0.003382
FDM train: iteration: 3000, fdm_loss: 0.004180
FDM train: iteration: 3500, fdm_loss: 0.003801
FDM train: iteration: 4000, fdm_loss: 0.006867
FDM train: iteration: 4500, fdm_loss: 0.003002
FDM train: iteration: 5000, fdm_loss: 0.003984

Background Trial: 1, reward: -612.8918093222586
Background Trial: 2, reward: -504.88636674277325
Background Trial: 3, reward: -522.4017593754645
Background Trial: 4, reward: -480.4576074451229
Background Trial: 5, reward: -591.6995689775006
Background Trial: 6, reward: -693.9302821969314
Background Trial: 7, reward: -550.6952385208263
Background Trial: 8, reward: -524.810477101646
Background Trial: 9, reward: -600.1720977010731
Iteration: 32, average_reward: -564.6605785981773, policy_loss: 0.780285, fdm_loss: 0.003770


episode_reward:  33.3
Background Trial: 1, reward: 11.994204225059974
Background Trial: 2, reward: 193.76639667026205
Background Trial: 3, reward: 30.84217012266177
Background Trial: 4, reward: -456.0842207416221
Background Trial: 5, reward: -191.7471722540185
Background Trial: 6, reward: 33.47061228861236
Background Trial: 7, reward: 80.29167071895739
Background Trial: 8, reward: 32.392017804316254
Background Trial: 9, reward: 15.783302670531313
Iteration: 33, average_reward: -27.69900205502661, policy_loss: 0.621708, fdm_loss: 0.004165


episode_reward: 257.2Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.002567
FDM train: iteration: 1000, fdm_loss: 0.004805
FDM train: iteration: 1500, fdm_loss: 0.004495
FDM train: iteration: 2000, fdm_loss: 0.006137
FDM train: iteration: 2500, fdm_loss: 0.003434
FDM train: iteration: 3000, fdm_loss: 0.004120
FDM train: iteration: 3500, fdm_loss: 0.003870
FDM train: iteration: 4000, fdm_loss: 0.004388
FDM train: iteration: 4500, fdm_loss: 0.004711
FDM train: iteration: 5000, fdm_loss: 0.003492

Background Trial: 1, reward: 1.6629119880473127
Background Trial: 2, reward: 6.229252713199799
Background Trial: 3, reward: -414.72996076795704
Background Trial: 4, reward: 306.2748310643111
Background Trial: 5, reward: -503.2107808242588
Background Trial: 6, reward: 222.1219427994263
Background Trial: 7, reward: -8.522985838229275
Background Trial: 8, reward: 251.28281320960582
Background Trial: 9, reward: 222.72472754122052
Iteration: 34, average_reward: 9.314750209485076, policy_loss: 0.559479, fdm_loss: 0.003297


episode_reward: 270.9
Background Trial: 1, reward: 24.819770204315233
Background Trial: 2, reward: 261.2995764503735
Background Trial: 3, reward: 29.908000307556364
Background Trial: 4, reward: 198.60328648036418
Background Trial: 5, reward: 194.56890083076087
Background Trial: 6, reward: 41.35861570117993
Background Trial: 7, reward: 50.88662718940779
Background Trial: 8, reward: 22.58712244452893
Background Trial: 9, reward: 51.020913287887765
Iteration: 35, average_reward: 97.2280903218194, policy_loss: 0.554115, fdm_loss: 0.003489


episode_reward: -328.2Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.004051
FDM train: iteration: 1000, fdm_loss: 0.002955
FDM train: iteration: 1500, fdm_loss: 0.005213
FDM train: iteration: 2000, fdm_loss: 0.003569
FDM train: iteration: 2500, fdm_loss: 0.002826
FDM train: iteration: 3000, fdm_loss: 0.004376
FDM train: iteration: 3500, fdm_loss: 0.004855
FDM train: iteration: 4000, fdm_loss: 0.004795
FDM train: iteration: 4500, fdm_loss: 0.003482
FDM train: iteration: 5000, fdm_loss: 0.003452

Background Trial: 1, reward: -19.35961507056507
Background Trial: 2, reward: -17.25223747724047
Background Trial: 3, reward: 232.09228959003502
Background Trial: 4, reward: -38.79919913149679
Background Trial: 5, reward: -216.70541977350985
Background Trial: 6, reward: 44.411138874918834
Background Trial: 7, reward: 55.27369945681747
Background Trial: 8, reward: 201.78371347975113
Background Trial: 9, reward: -59.23759783999489
Iteration: 36, average_reward: 20.245196900968377, policy_loss: 0.467399, fdm_loss: 0.004700


episode_reward: 232.4
Background Trial: 1, reward: -49.103223052549794
Background Trial: 2, reward: 230.6268358412916
Background Trial: 3, reward: 46.17892040466842
Background Trial: 4, reward: -32.275364741795556
Background Trial: 5, reward: 20.38200644387574
Background Trial: 6, reward: 220.93793130066663
Background Trial: 7, reward: 29.700373217077356
Background Trial: 8, reward: -81.37565266937389
Background Trial: 9, reward: 5.439455794177164
Iteration: 37, average_reward: 43.390142504226404, policy_loss: 0.473355, fdm_loss: 0.004799


episode_reward: 236.6Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.002656
FDM train: iteration: 1000, fdm_loss: 0.003811
FDM train: iteration: 1500, fdm_loss: 0.002401
FDM train: iteration: 2000, fdm_loss: 0.005002
FDM train: iteration: 2500, fdm_loss: 0.005095
FDM train: iteration: 3000, fdm_loss: 0.004174
FDM train: iteration: 3500, fdm_loss: 0.004012
FDM train: iteration: 4000, fdm_loss: 0.002653
FDM train: iteration: 4500, fdm_loss: 0.002881
FDM train: iteration: 5000, fdm_loss: 0.004395

Background Trial: 1, reward: 157.9866225081181
Background Trial: 2, reward: -178.25003777698936
Background Trial: 3, reward: 204.93962484051372
Background Trial: 4, reward: 250.61243011985525
Background Trial: 5, reward: 44.13472157283999
Background Trial: 6, reward: 43.83695063454235
Background Trial: 7, reward: -3.88302725745622
Background Trial: 8, reward: 250.44564074928294
Background Trial: 9, reward: 3.6429243780777085
Iteration: 38, average_reward: 85.94064997430938, policy_loss: 0.570557, fdm_loss: 0.004120


episode_reward:   1.4
Background Trial: 1, reward: -82.33424578816204
Background Trial: 2, reward: 33.13370338434396
Background Trial: 3, reward: 264.0322186833039
Background Trial: 4, reward: -7.640403461357465
Background Trial: 5, reward: 57.05285222078038
Background Trial: 6, reward: 237.27635399455698
Background Trial: 7, reward: 196.97721925629136
Background Trial: 8, reward: -7.3621947692548275
Background Trial: 9, reward: 295.0977054013205
Iteration: 39, average_reward: 109.58146765798031, policy_loss: 0.533705, fdm_loss: 0.003678


episode_reward: -219.5Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.004391
FDM train: iteration: 1000, fdm_loss: 0.003895
FDM train: iteration: 1500, fdm_loss: 0.003809
FDM train: iteration: 2000, fdm_loss: 0.003831
FDM train: iteration: 2500, fdm_loss: 0.004286
FDM train: iteration: 3000, fdm_loss: 0.002977
FDM train: iteration: 3500, fdm_loss: 0.003653
FDM train: iteration: 4000, fdm_loss: 0.002736
FDM train: iteration: 4500, fdm_loss: 0.004894
FDM train: iteration: 5000, fdm_loss: 0.005890

Background Trial: 1, reward: 18.694712071464608
Background Trial: 2, reward: -31.167618310414497
Background Trial: 3, reward: 292.42264142310603
Background Trial: 4, reward: 14.482638770763629
Background Trial: 5, reward: 5.280723499385346
Background Trial: 6, reward: 36.855592912761864
Background Trial: 7, reward: 277.8427177666453
Background Trial: 8, reward: -145.59842249218627
Background Trial: 9, reward: -13.379448104432583
Iteration: 40, average_reward: 50.603726393010376, policy_loss: 0.568329, fdm_loss: 0.005057


episode_reward: 217.5
Background Trial: 1, reward: -3.671767190443518
Background Trial: 2, reward: 45.40814181769389
Background Trial: 3, reward: 6.300999691602868
Background Trial: 4, reward: -248.13760179223252
Background Trial: 5, reward: -471.5022378339005
Background Trial: 6, reward: 48.752382353303176
Background Trial: 7, reward: -27.30414802646989
Background Trial: 8, reward: 16.440726656734313
Background Trial: 9, reward: 7.07022840344348
Iteration: 41, average_reward: -69.62703065780764, policy_loss: 0.597598, fdm_loss: 0.004109


episode_reward: 240.7Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.003951
FDM train: iteration: 1000, fdm_loss: 0.005896
FDM train: iteration: 1500, fdm_loss: 0.004861
FDM train: iteration: 2000, fdm_loss: 0.002956
FDM train: iteration: 2500, fdm_loss: 0.004934
FDM train: iteration: 3000, fdm_loss: 0.003495
FDM train: iteration: 3500, fdm_loss: 0.003380
FDM train: iteration: 4000, fdm_loss: 0.004165
FDM train: iteration: 4500, fdm_loss: 0.003657
FDM train: iteration: 5000, fdm_loss: 0.003580

Background Trial: 1, reward: -286.29040730469796
Background Trial: 2, reward: -37.17867717319736
Background Trial: 3, reward: -157.1558746885721
Background Trial: 4, reward: 286.53433473688324
Background Trial: 5, reward: -52.70266343455919
Background Trial: 6, reward: 36.127702478307754
Background Trial: 7, reward: 31.988953820842767
Background Trial: 8, reward: -289.9522185366869
Background Trial: 9, reward: -221.7116813885439
Iteration: 42, average_reward: -76.70450349891374, policy_loss: 0.504899, fdm_loss: 0.003923


episode_reward: 224.2
Background Trial: 1, reward: -335.6028646903126
Background Trial: 2, reward: 269.8774459319083
Background Trial: 3, reward: 273.6021794772361
Background Trial: 4, reward: 3.151535537113247
Background Trial: 5, reward: 68.49538255657572
Background Trial: 6, reward: 285.4313803357221
Background Trial: 7, reward: 60.200313472141374
Background Trial: 8, reward: 275.94670887380346
Background Trial: 9, reward: -299.2409856521143
Iteration: 43, average_reward: 66.87345509356372, policy_loss: 0.547673, fdm_loss: 0.005070


episode_reward:  26.9Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.005005
FDM train: iteration: 1000, fdm_loss: 0.003933
FDM train: iteration: 1500, fdm_loss: 0.004133
FDM train: iteration: 2000, fdm_loss: 0.003511
FDM train: iteration: 2500, fdm_loss: 0.002121
FDM train: iteration: 3000, fdm_loss: 0.002876
FDM train: iteration: 3500, fdm_loss: 0.003783
FDM train: iteration: 4000, fdm_loss: 0.003865
FDM train: iteration: 4500, fdm_loss: 0.003685
FDM train: iteration: 5000, fdm_loss: 0.003289

Background Trial: 1, reward: 270.32820808091253
Background Trial: 2, reward: -142.26479926437943
Background Trial: 3, reward: -4.722014346138295
Background Trial: 4, reward: 18.419382275929962
Background Trial: 5, reward: 237.12383923303562
Background Trial: 6, reward: 235.01645484256053
Background Trial: 7, reward: 225.90785549810167
Background Trial: 8, reward: 293.7545420318629
Background Trial: 9, reward: 35.0350523577695
Iteration: 44, average_reward: 129.84428007885057, policy_loss: 0.476873, fdm_loss: 0.005490


episode_reward: 217.1
Background Trial: 1, reward: -348.3457441023605
Background Trial: 2, reward: 216.76250901477087
Background Trial: 3, reward: 206.88959019340018
Background Trial: 4, reward: -322.20077568455036
Background Trial: 5, reward: -44.69810563131804
Background Trial: 6, reward: -12.784106065282572
Background Trial: 7, reward: -468.5042042737037
Background Trial: 8, reward: 235.54326226766216
Background Trial: 9, reward: 229.4358710791996
Iteration: 45, average_reward: -34.211300355798045, policy_loss: 0.521799, fdm_loss: 0.003906


episode_reward:  58.2Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.003458
FDM train: iteration: 1000, fdm_loss: 0.003753
FDM train: iteration: 1500, fdm_loss: 0.004860
FDM train: iteration: 2000, fdm_loss: 0.003528
FDM train: iteration: 2500, fdm_loss: 0.003141
FDM train: iteration: 3000, fdm_loss: 0.004608
FDM train: iteration: 3500, fdm_loss: 0.003617
FDM train: iteration: 4000, fdm_loss: 0.005036
FDM train: iteration: 4500, fdm_loss: 0.003965
FDM train: iteration: 5000, fdm_loss: 0.004892

Background Trial: 1, reward: 244.0345525571608
Background Trial: 2, reward: 29.439264381020905
Background Trial: 3, reward: -20.182276436158986
Background Trial: 4, reward: -551.9217848320409
Background Trial: 5, reward: 58.28607230452164
Background Trial: 6, reward: 3.025256078319373
Background Trial: 7, reward: -349.63875743734627
Background Trial: 8, reward: 213.47087402069144
Background Trial: 9, reward: 16.55387907741526
Iteration: 46, average_reward: -39.65921336515741, policy_loss: 0.560597, fdm_loss: 0.004687


episode_reward:  -4.0
Background Trial: 1, reward: 14.520329368953
Background Trial: 2, reward: 15.671677017397627
Background Trial: 3, reward: 55.09719321360785
Background Trial: 4, reward: 5.43771050264111
Background Trial: 5, reward: -10.182541535870143
Background Trial: 6, reward: 65.90178703760108
Background Trial: 7, reward: -307.60525596566924
Background Trial: 8, reward: 32.654324424554204
Background Trial: 9, reward: 65.43631098051537
Iteration: 47, average_reward: -7.007607217363238, policy_loss: 0.583958, fdm_loss: 0.004319


episode_reward:   5.5Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.003694
FDM train: iteration: 1000, fdm_loss: 0.004968
FDM train: iteration: 1500, fdm_loss: 0.004025
FDM train: iteration: 2000, fdm_loss: 0.002768
FDM train: iteration: 2500, fdm_loss: 0.004035
FDM train: iteration: 3000, fdm_loss: 0.006334
FDM train: iteration: 3500, fdm_loss: 0.005113
FDM train: iteration: 4000, fdm_loss: 0.004031
FDM train: iteration: 4500, fdm_loss: 0.003634
FDM train: iteration: 5000, fdm_loss: 0.003397

Background Trial: 1, reward: -47.06331777750855
Background Trial: 2, reward: -4.149935450569544
Background Trial: 3, reward: 30.213122275058538
Background Trial: 4, reward: 13.956201094395382
Background Trial: 5, reward: 8.647136959404563
Background Trial: 6, reward: -12.403933376808567
Background Trial: 7, reward: -18.882127005366726
Background Trial: 8, reward: -2.1760889083067667
Background Trial: 9, reward: 8.534909314899991
Iteration: 48, average_reward: -2.5915592083112977, policy_loss: 0.804660, fdm_loss: 0.004937


episode_reward:   7.2
Background Trial: 1, reward: -37.650271951920296
Background Trial: 2, reward: 7.363686216838801
Background Trial: 3, reward: -22.853317522114367
Background Trial: 4, reward: 16.26407298477733
Background Trial: 5, reward: -28.890744813022437
Background Trial: 6, reward: 0.52021347332375
Background Trial: 7, reward: 26.016029673800176
Background Trial: 8, reward: -13.638252861781766
Background Trial: 9, reward: 0.9802466198783009
Iteration: 49, average_reward: -5.765370908913389, policy_loss: 0.589967, fdm_loss: 0.003720


episode_reward: -54.0Collecting dynamics training data from exploration policy 1000
FDM train: iteration: 500, fdm_loss: 0.005155
FDM train: iteration: 1000, fdm_loss: 0.004393
FDM train: iteration: 1500, fdm_loss: 0.004960
FDM train: iteration: 2000, fdm_loss: 0.005297
FDM train: iteration: 2500, fdm_loss: 0.004232
FDM train: iteration: 3000, fdm_loss: 0.004743
FDM train: iteration: 3500, fdm_loss: 0.004336
FDM train: iteration: 4000, fdm_loss: 0.005136
FDM train: iteration: 4500, fdm_loss: 0.004853
FDM train: iteration: 5000, fdm_loss: 0.003333

Background Trial: 1, reward: -340.4694177230821
Background Trial: 2, reward: -326.24356802941753
Background Trial: 3, reward: -261.38630640940244
Background Trial: 4, reward: -50.99889906115424
Background Trial: 5, reward: -238.75314606451988
Background Trial: 6, reward: -290.9739190667385
Background Trial: 7, reward: -235.0079251103407
Background Trial: 8, reward: -248.769163567155
Background Trial: 9, reward: -291.31584407382735
Iteration: 50, average_reward: -253.7686876784042, policy_loss: 0.564991, fdm_loss: 0.003687

