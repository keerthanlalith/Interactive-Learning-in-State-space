Collecting idm training data 1000
Collecting idm training data 2000
Collecting idm training data 3000
Collecting idm training data 4000
Collecting idm training data 5000
Collecting idm training data 6000
Collecting idm training data 7000
Collecting idm training data 8000
Collecting idm training data 9000
Collecting idm training data 10000
Collecting idm training data 11000
Collecting idm training data 12000
Collecting idm training data 13000
Collecting idm training data 14000
Collecting idm training data 15000
Collecting idm training data 16000
Collecting idm training data 17000
Collecting idm training data 18000
Collecting idm training data 19000
Collecting idm training data 20000
IDM train: iteration: 500, idm_loss: 0.337780
IDM train: iteration: 1000, idm_loss: 0.322349
IDM train: iteration: 1500, idm_loss: 0.366418
IDM train: iteration: 2000, idm_loss: 0.150430
IDM train: iteration: 2500, idm_loss: 0.166172
IDM train: iteration: 3000, idm_loss: 0.189810
IDM train: iteration: 3500, idm_loss: 0.133268
IDM train: iteration: 4000, idm_loss: 0.096344
IDM train: iteration: 4500, idm_loss: 0.071098
IDM train: iteration: 5000, idm_loss: 0.091745
IDM train: iteration: 5500, idm_loss: 0.076957
IDM train: iteration: 6000, idm_loss: 0.071305
IDM train: iteration: 6500, idm_loss: 0.074295
IDM train: iteration: 7000, idm_loss: 0.096387

iteration: 1, total_reward: -139.74386674566244, policy_loss: 0.184903, idm_loss: 0.071017


iteration: 2, total_reward: -110.79876208417144, policy_loss: 0.162356, idm_loss: 0.119246


iteration: 3, total_reward: 4.931999858770894, policy_loss: 0.152368, idm_loss: 0.033589


iteration: 4, total_reward: -287.477559743495, policy_loss: 0.131108, idm_loss: 0.072311

Policy train: iteration: 500, policy_loss: 0.063280
Policy train: iteration: 1000, policy_loss: 0.077475
Policy train: iteration: 1500, policy_loss: 0.062465
Policy train: iteration: 2000, policy_loss: 0.120098
Policy train: iteration: 2500, policy_loss: 0.061132
Policy train: iteration: 3000, policy_loss: 0.051577
Policy train: iteration: 3500, policy_loss: 0.052239
Policy train: iteration: 4000, policy_loss: 0.047031
Policy train: iteration: 4500, policy_loss: 0.043674
Policy train: iteration: 5000, policy_loss: 0.063895
Policy train: iteration: 5500, policy_loss: 0.050257
Policy train: iteration: 6000, policy_loss: 0.032083
Policy train: iteration: 6500, policy_loss: 0.044034
Policy train: iteration: 7000, policy_loss: 0.049041

iteration: 5, total_reward: -118.4512542204622, policy_loss: 0.191901, idm_loss: 0.107072

Collecting idm training data 1000
Collecting idm training data 2000
Collecting idm training data 3000
Collecting idm training data 4000
Collecting idm training data 5000
Collecting idm training data 6000
Collecting idm training data 7000
Collecting idm training data 8000
Collecting idm training data 9000
Collecting idm training data 10000
Collecting idm training data 11000
Collecting idm training data 12000
Collecting idm training data 13000
Collecting idm training data 14000
Collecting idm training data 15000
Collecting idm training data 16000
Collecting idm training data 17000
Collecting idm training data 18000
Collecting idm training data 19000
Collecting idm training data 20000
IDM train: iteration: 500, idm_loss: 0.120380
IDM train: iteration: 1000, idm_loss: 0.076189
IDM train: iteration: 1500, idm_loss: 0.143705
IDM train: iteration: 2000, idm_loss: 0.141438
IDM train: iteration: 2500, idm_loss: 0.070770
IDM train: iteration: 3000, idm_loss: 0.074020
IDM train: iteration: 3500, idm_loss: 0.128638
IDM train: iteration: 4000, idm_loss: 0.085610
IDM train: iteration: 4500, idm_loss: 0.086930
IDM train: iteration: 5000, idm_loss: 0.087823
IDM train: iteration: 5500, idm_loss: 0.077466
IDM train: iteration: 6000, idm_loss: 0.068495
IDM train: iteration: 6500, idm_loss: 0.098913
IDM train: iteration: 7000, idm_loss: 0.098670

iteration: 6, total_reward: 6.005186792383583, policy_loss: 0.267685, idm_loss: 0.119898


iteration: 7, total_reward: -233.4499564816455, policy_loss: 0.313556, idm_loss: 0.214806


iteration: 8, total_reward: -222.15469640655616, policy_loss: 0.356899, idm_loss: 0.136023


iteration: 9, total_reward: -38.31820990126859, policy_loss: 0.315969, idm_loss: 0.142701

Policy train: iteration: 500, policy_loss: 0.069078
Policy train: iteration: 1000, policy_loss: 0.093884
Policy train: iteration: 1500, policy_loss: 0.061856
Policy train: iteration: 2000, policy_loss: 0.034043
Policy train: iteration: 2500, policy_loss: 0.051220
Policy train: iteration: 3000, policy_loss: 0.052691
Policy train: iteration: 3500, policy_loss: 0.060813
Policy train: iteration: 4000, policy_loss: 0.078709
Policy train: iteration: 4500, policy_loss: 0.036561
Policy train: iteration: 5000, policy_loss: 0.047956
Policy train: iteration: 5500, policy_loss: 0.039730
Policy train: iteration: 6000, policy_loss: 0.034147
Policy train: iteration: 6500, policy_loss: 0.026914
Policy train: iteration: 7000, policy_loss: 0.030605

iteration: 10, total_reward: -125.24636962083947, policy_loss: 0.295232, idm_loss: 0.111792

Collecting idm training data 1000
Collecting idm training data 2000
Collecting idm training data 3000
Collecting idm training data 4000
Collecting idm training data 5000
Collecting idm training data 6000
Collecting idm training data 7000
Collecting idm training data 8000
Collecting idm training data 9000
Collecting idm training data 10000
Collecting idm training data 11000
Collecting idm training data 12000
Collecting idm training data 13000
Collecting idm training data 14000
