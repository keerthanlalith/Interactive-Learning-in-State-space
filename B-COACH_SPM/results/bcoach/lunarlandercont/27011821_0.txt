Collecting idm training data 1000
Collecting idm training data 2000
Collecting idm training data 3000
Collecting idm training data 4000
Collecting idm training data 5000
Collecting idm training data 6000
Collecting idm training data 7000
Collecting idm training data 8000
Collecting idm training data 9000
Collecting idm training data 10000
Collecting idm training data 11000
Collecting idm training data 12000
Collecting idm training data 13000
Collecting idm training data 14000
Collecting idm training data 15000
Collecting idm training data 16000
Collecting idm training data 17000
Collecting idm training data 18000
Collecting idm training data 19000
Collecting idm training data 20000
IDM train: iteration: 500, idm_loss: 0.335867
IDM train: iteration: 1000, idm_loss: 0.321124
IDM train: iteration: 1500, idm_loss: 0.316334
IDM train: iteration: 2000, idm_loss: 0.152777
IDM train: iteration: 2500, idm_loss: 0.184904
IDM train: iteration: 3000, idm_loss: 0.216194
IDM train: iteration: 3500, idm_loss: 0.219712
IDM train: iteration: 4000, idm_loss: 0.202131
IDM train: iteration: 4500, idm_loss: 0.152981
IDM train: iteration: 5000, idm_loss: 0.176486
IDM train: iteration: 5500, idm_loss: 0.093751
IDM train: iteration: 6000, idm_loss: 0.133171
IDM train: iteration: 6500, idm_loss: 0.085101
IDM train: iteration: 7000, idm_loss: 0.105831

iteration: 1, total_reward: -262.4718856177635, policy_loss: 0.125252, idm_loss: 0.104737

Policy train: iteration: 500, policy_loss: 0.085237
Policy train: iteration: 1000, policy_loss: 0.088304
Policy train: iteration: 1500, policy_loss: 0.071881
Policy train: iteration: 2000, policy_loss: 0.044890
Policy train: iteration: 2500, policy_loss: 0.041250
Policy train: iteration: 3000, policy_loss: 0.048288
Policy train: iteration: 3500, policy_loss: 0.054099
Policy train: iteration: 4000, policy_loss: 0.031039
Policy train: iteration: 4500, policy_loss: 0.051892
Policy train: iteration: 5000, policy_loss: 0.045673
Policy train: iteration: 5500, policy_loss: 0.062467
Policy train: iteration: 6000, policy_loss: 0.037879
Policy train: iteration: 6500, policy_loss: 0.071888
Policy train: iteration: 7000, policy_loss: 0.049782

iteration: 2, total_reward: -166.76066358836982, policy_loss: 0.362202, idm_loss: 0.230753


iteration: 3, total_reward: -169.70262474744828, policy_loss: 0.389946, idm_loss: 0.236416

Policy train: iteration: 500, policy_loss: 0.058652
Policy train: iteration: 1000, policy_loss: 0.022431
Policy train: iteration: 1500, policy_loss: 0.036379
Policy train: iteration: 2000, policy_loss: 0.055300
Policy train: iteration: 2500, policy_loss: 0.023663
Policy train: iteration: 3000, policy_loss: 0.046368
Policy train: iteration: 3500, policy_loss: 0.034883
Policy train: iteration: 4000, policy_loss: 0.034776
Policy train: iteration: 4500, policy_loss: 0.052916
Policy train: iteration: 5000, policy_loss: 0.039453
Policy train: iteration: 5500, policy_loss: 0.044251
Policy train: iteration: 6000, policy_loss: 0.044277
Policy train: iteration: 6500, policy_loss: 0.024702
Policy train: iteration: 7000, policy_loss: 0.038174

iteration: 4, total_reward: -161.68862634711584, policy_loss: 0.427802, idm_loss: 0.099311


iteration: 5, total_reward: -158.15690079185725, policy_loss: 0.374441, idm_loss: 0.135708

Policy train: iteration: 500, policy_loss: 0.026510
Policy train: iteration: 1000, policy_loss: 0.042586
Policy train: iteration: 1500, policy_loss: 0.035956
Policy train: iteration: 2000, policy_loss: 0.027721
Policy train: iteration: 2500, policy_loss: 0.042591
Policy train: iteration: 3000, policy_loss: 0.031600
Policy train: iteration: 3500, policy_loss: 0.028119
Policy train: iteration: 4000, policy_loss: 0.033047
Policy train: iteration: 4500, policy_loss: 0.058630
Policy train: iteration: 5000, policy_loss: 0.040573
Policy train: iteration: 5500, policy_loss: 0.032438
Policy train: iteration: 6000, policy_loss: 0.026174
Policy train: iteration: 6500, policy_loss: 0.029452
Policy train: iteration: 7000, policy_loss: 0.028320

iteration: 6, total_reward: -28.983836477163678, policy_loss: 0.382384, idm_loss: 0.117849


iteration: 7, total_reward: -43.93301555229335, policy_loss: 0.409766, idm_loss: 0.187442

Policy train: iteration: 500, policy_loss: 0.039282
Policy train: iteration: 1000, policy_loss: 0.025207
Policy train: iteration: 1500, policy_loss: 0.024630
Policy train: iteration: 2000, policy_loss: 0.021314
Policy train: iteration: 2500, policy_loss: 0.028267
Policy train: iteration: 3000, policy_loss: 0.026225
Policy train: iteration: 3500, policy_loss: 0.035480
Policy train: iteration: 4000, policy_loss: 0.042495
Policy train: iteration: 4500, policy_loss: 0.029270
Policy train: iteration: 5000, policy_loss: 0.026370
Policy train: iteration: 5500, policy_loss: 0.038061
Policy train: iteration: 6000, policy_loss: 0.021455
Policy train: iteration: 6500, policy_loss: 0.022101
Policy train: iteration: 7000, policy_loss: 0.015701

iteration: 8, total_reward: -211.52833846063345, policy_loss: 0.359266, idm_loss: 0.214012


iteration: 9, total_reward: -23.338593644212153, policy_loss: 0.224548, idm_loss: 0.106938

Policy train: iteration: 500, policy_loss: 0.032051
Policy train: iteration: 1000, policy_loss: 0.046204
Policy train: iteration: 1500, policy_loss: 0.020006
Policy train: iteration: 2000, policy_loss: 0.030341
Policy train: iteration: 2500, policy_loss: 0.030135
Policy train: iteration: 3000, policy_loss: 0.021570
Policy train: iteration: 3500, policy_loss: 0.039048
Policy train: iteration: 4000, policy_loss: 0.028510
Policy train: iteration: 4500, policy_loss: 0.027889
Policy train: iteration: 5000, policy_loss: 0.024760
Policy train: iteration: 5500, policy_loss: 0.031988
Policy train: iteration: 6000, policy_loss: 0.025662
Policy train: iteration: 6500, policy_loss: 0.018237
Policy train: iteration: 7000, policy_loss: 0.029770

iteration: 10, total_reward: 21.83226869913244, policy_loss: 0.143882, idm_loss: 0.142723


iteration: 11, total_reward: -25.943178100036477, policy_loss: 0.141840, idm_loss: 0.104353

Policy train: iteration: 500, policy_loss: 0.029139
Policy train: iteration: 1000, policy_loss: 0.029618
Policy train: iteration: 1500, policy_loss: 0.037492
Policy train: iteration: 2000, policy_loss: 0.043642
Policy train: iteration: 2500, policy_loss: 0.031400
Policy train: iteration: 3000, policy_loss: 0.025158
Policy train: iteration: 3500, policy_loss: 0.039094
Policy train: iteration: 4000, policy_loss: 0.033749
Policy train: iteration: 4500, policy_loss: 0.020086
Policy train: iteration: 5000, policy_loss: 0.030016
Policy train: iteration: 5500, policy_loss: 0.026773
Policy train: iteration: 6000, policy_loss: 0.023769
Policy train: iteration: 6500, policy_loss: 0.020931
Policy train: iteration: 7000, policy_loss: 0.033462

iteration: 12, total_reward: -15.52331145961908, policy_loss: 0.113182, idm_loss: 0.131652


iteration: 13, total_reward: -14.505855339856808, policy_loss: 0.318180, idm_loss: 0.166094

Policy train: iteration: 500, policy_loss: 0.031916
Policy train: iteration: 1000, policy_loss: 0.041458
Policy train: iteration: 1500, policy_loss: 0.027825
Policy train: iteration: 2000, policy_loss: 0.019744
Policy train: iteration: 2500, policy_loss: 0.038652
Policy train: iteration: 3000, policy_loss: 0.028218
Policy train: iteration: 3500, policy_loss: 0.030469
Policy train: iteration: 4000, policy_loss: 0.036226
Policy train: iteration: 4500, policy_loss: 0.028396
Policy train: iteration: 5000, policy_loss: 0.018882
Policy train: iteration: 5500, policy_loss: 0.029213
Policy train: iteration: 6000, policy_loss: 0.035501
Policy train: iteration: 6500, policy_loss: 0.030360
Policy train: iteration: 7000, policy_loss: 0.018573

iteration: 14, total_reward: 19.105693114687327, policy_loss: 0.194646, idm_loss: 0.240409


iteration: 15, total_reward: 16.239428785017353, policy_loss: 0.168815, idm_loss: 0.137995

Policy train: iteration: 500, policy_loss: 0.024761
Policy train: iteration: 1000, policy_loss: 0.035190
Policy train: iteration: 1500, policy_loss: 0.029196
Policy train: iteration: 2000, policy_loss: 0.047422
Policy train: iteration: 2500, policy_loss: 0.025488
Policy train: iteration: 3000, policy_loss: 0.031294
Policy train: iteration: 3500, policy_loss: 0.039087
Policy train: iteration: 4000, policy_loss: 0.035820
Policy train: iteration: 4500, policy_loss: 0.032554
Policy train: iteration: 5000, policy_loss: 0.022488
Policy train: iteration: 5500, policy_loss: 0.032565
Policy train: iteration: 6000, policy_loss: 0.043991
Policy train: iteration: 6500, policy_loss: 0.035536
Policy train: iteration: 7000, policy_loss: 0.025767

iteration: 16, total_reward: 6.314986680120086, policy_loss: 0.095610, idm_loss: 0.138114


iteration: 17, total_reward: -333.39463601941713, policy_loss: 0.168820, idm_loss: 0.072354

Policy train: iteration: 500, policy_loss: 0.034478
Policy train: iteration: 1000, policy_loss: 0.033812
Policy train: iteration: 1500, policy_loss: 0.035955
Policy train: iteration: 2000, policy_loss: 0.026850
Policy train: iteration: 2500, policy_loss: 0.032840
Policy train: iteration: 3000, policy_loss: 0.029525
Policy train: iteration: 3500, policy_loss: 0.020371
Policy train: iteration: 4000, policy_loss: 0.042848
Policy train: iteration: 4500, policy_loss: 0.019755
Policy train: iteration: 5000, policy_loss: 0.052847
Policy train: iteration: 5500, policy_loss: 0.021094
Policy train: iteration: 6000, policy_loss: 0.038850
Policy train: iteration: 6500, policy_loss: 0.024995
Policy train: iteration: 7000, policy_loss: 0.041678

iteration: 18, total_reward: -411.9233595198309, policy_loss: 0.152097, idm_loss: 0.166624


iteration: 19, total_reward: 31.368652671210327, policy_loss: 0.340576, idm_loss: 0.123965

Policy train: iteration: 500, policy_loss: 0.036653
Policy train: iteration: 1000, policy_loss: 0.019438
Policy train: iteration: 1500, policy_loss: 0.030394
Policy train: iteration: 2000, policy_loss: 0.032484
Policy train: iteration: 2500, policy_loss: 0.012440
Policy train: iteration: 3000, policy_loss: 0.038187
Policy train: iteration: 3500, policy_loss: 0.023565
Policy train: iteration: 4000, policy_loss: 0.041087
Policy train: iteration: 4500, policy_loss: 0.020445
Policy train: iteration: 5000, policy_loss: 0.017913
Policy train: iteration: 5500, policy_loss: 0.031174
Policy train: iteration: 6000, policy_loss: 0.029682
Policy train: iteration: 6500, policy_loss: 0.034473
Policy train: iteration: 7000, policy_loss: 0.030083

iteration: 20, total_reward: -453.05884391571834, policy_loss: 0.181145, idm_loss: 0.135016


iteration: 21, total_reward: 265.2703381671811, policy_loss: 0.219037, idm_loss: 0.094883

Policy train: iteration: 500, policy_loss: 0.022349
Policy train: iteration: 1000, policy_loss: 0.029534
Policy train: iteration: 1500, policy_loss: 0.018008
Policy train: iteration: 2000, policy_loss: 0.022453
Policy train: iteration: 2500, policy_loss: 0.036915
Policy train: iteration: 3000, policy_loss: 0.022248
Policy train: iteration: 3500, policy_loss: 0.025122
Policy train: iteration: 4000, policy_loss: 0.021543
Policy train: iteration: 4500, policy_loss: 0.037491
Policy train: iteration: 5000, policy_loss: 0.028907
Policy train: iteration: 5500, policy_loss: 0.037760
Policy train: iteration: 6000, policy_loss: 0.017158
Policy train: iteration: 6500, policy_loss: 0.034667
Policy train: iteration: 7000, policy_loss: 0.034484

iteration: 22, total_reward: 24.691687616021866, policy_loss: 0.190669, idm_loss: 0.075485


iteration: 23, total_reward: 225.84199669658912, policy_loss: 0.334596, idm_loss: 0.117801

Policy train: iteration: 500, policy_loss: 0.015586
Policy train: iteration: 1000, policy_loss: 0.015017
Policy train: iteration: 1500, policy_loss: 0.026330
Policy train: iteration: 2000, policy_loss: 0.040841
Policy train: iteration: 2500, policy_loss: 0.021900
Policy train: iteration: 3000, policy_loss: 0.018802
Policy train: iteration: 3500, policy_loss: 0.030406
Policy train: iteration: 4000, policy_loss: 0.032825
Policy train: iteration: 4500, policy_loss: 0.016124
Policy train: iteration: 5000, policy_loss: 0.018486
