Collecting idm training data 1000
IDM train: iteration: 500, idm_loss: 0.017423
IDM train: iteration: 1000, idm_loss: 0.002843

iteration: 1, total_reward: 10.0, policy_loss: 0.692912, idm_loss: 0.002733


iteration: 2, total_reward: 11.0, policy_loss: 0.691602, idm_loss: 0.003109


iteration: 3, total_reward: 11.0, policy_loss: 0.693767, idm_loss: 0.003169


iteration: 4, total_reward: 9.0, policy_loss: 0.697404, idm_loss: 0.002748


iteration: 5, total_reward: 11.0, policy_loss: 0.702363, idm_loss: 0.002885


iteration: 6, total_reward: 10.0, policy_loss: 0.718456, idm_loss: 0.002966


iteration: 7, total_reward: 11.0, policy_loss: 0.725651, idm_loss: 0.002609


iteration: 8, total_reward: 10.0, policy_loss: 0.787798, idm_loss: 0.002895


iteration: 9, total_reward: 10.0, policy_loss: 0.841787, idm_loss: 0.002664


iteration: 10, total_reward: 10.0, policy_loss: 0.884531, idm_loss: 0.002665


iteration: 11, total_reward: 10.0, policy_loss: 0.910387, idm_loss: 0.003011


iteration: 12, total_reward: 10.0, policy_loss: 1.002782, idm_loss: 0.002995


iteration: 13, total_reward: 10.0, policy_loss: 0.987241, idm_loss: 0.003041


iteration: 14, total_reward: 10.0, policy_loss: 0.974749, idm_loss: 0.002967


iteration: 15, total_reward: 10.0, policy_loss: 0.987725, idm_loss: 0.003045


iteration: 16, total_reward: 10.0, policy_loss: 0.987244, idm_loss: 0.003074


iteration: 17, total_reward: 8.0, policy_loss: 1.063907, idm_loss: 0.002826

