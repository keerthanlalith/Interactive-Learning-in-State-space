Collecting idm training data 1000
Collecting idm training data 2000
Collecting idm training data 3000
Collecting idm training data 4000
Collecting idm training data 5000
Collecting idm training data 6000
Collecting idm training data 7000
Collecting idm training data 8000
Collecting idm training data 9000
Collecting idm training data 10000
IDM train: iteration: 500, idm_loss: 0.310358
IDM train: iteration: 1000, idm_loss: 0.341960
IDM train: iteration: 1500, idm_loss: 0.199929
IDM train: iteration: 2000, idm_loss: 0.232114
IDM train: iteration: 2500, idm_loss: 0.210713
IDM train: iteration: 3000, idm_loss: 0.198262
IDM train: iteration: 3500, idm_loss: 0.195218
IDM train: iteration: 4000, idm_loss: 0.203439
IDM train: iteration: 4500, idm_loss: 0.205948
IDM train: iteration: 5000, idm_loss: 0.181088
Policy train: iteration: 500, policy_loss: 0.041117
Policy train: iteration: 1000, policy_loss: 0.028094
Policy train: iteration: 1500, policy_loss: 0.029067
Policy train: iteration: 2000, policy_loss: 0.027704
Policy train: iteration: 2500, policy_loss: 0.026922
Policy train: iteration: 3000, policy_loss: 0.024573
Policy train: iteration: 3500, policy_loss: 0.024945
Policy train: iteration: 4000, policy_loss: 0.023920
Policy train: iteration: 4500, policy_loss: 0.023254
Policy train: iteration: 5000, policy_loss: 0.026277
IDM train: iteration: 500, idm_loss: 0.129940
IDM train: iteration: 1000, idm_loss: 0.126570
IDM train: iteration: 1500, idm_loss: 0.097471
IDM train: iteration: 2000, idm_loss: 0.080140
IDM train: iteration: 2500, idm_loss: 0.085833
IDM train: iteration: 3000, idm_loss: 0.090562
IDM train: iteration: 3500, idm_loss: 0.092216
IDM train: iteration: 4000, idm_loss: 0.076935
IDM train: iteration: 4500, idm_loss: 0.113517
IDM train: iteration: 5000, idm_loss: 0.095374

iteration: 1, total_reward: -39.93593895711012, policy_loss: 0.081326, idm_loss: 0.059014

Policy train: iteration: 500, policy_loss: 0.048186
Policy train: iteration: 1000, policy_loss: 0.044363
Policy train: iteration: 1500, policy_loss: 0.036774
Policy train: iteration: 2000, policy_loss: 0.044355
Policy train: iteration: 2500, policy_loss: 0.040098
Policy train: iteration: 3000, policy_loss: 0.040338
Policy train: iteration: 3500, policy_loss: 0.040946
Policy train: iteration: 4000, policy_loss: 0.038952
Policy train: iteration: 4500, policy_loss: 0.045740
Policy train: iteration: 5000, policy_loss: 0.040931
IDM train: iteration: 500, idm_loss: 0.084570
IDM train: iteration: 1000, idm_loss: 0.089353
IDM train: iteration: 1500, idm_loss: 0.060939
IDM train: iteration: 2000, idm_loss: 0.090170
IDM train: iteration: 2500, idm_loss: 0.071622
IDM train: iteration: 3000, idm_loss: 0.083865
IDM train: iteration: 3500, idm_loss: 0.076002
IDM train: iteration: 4000, idm_loss: 0.096655
IDM train: iteration: 4500, idm_loss: 0.077732
IDM train: iteration: 5000, idm_loss: 0.083918

iteration: 2, total_reward: 263.32077144113396, policy_loss: 0.049082, idm_loss: 0.082225

Policy train: iteration: 500, policy_loss: 0.044026
Policy train: iteration: 1000, policy_loss: 0.036136
Policy train: iteration: 1500, policy_loss: 0.038213
Policy train: iteration: 2000, policy_loss: 0.041676
Policy train: iteration: 2500, policy_loss: 0.042918
Policy train: iteration: 3000, policy_loss: 0.035761
Policy train: iteration: 3500, policy_loss: 0.038136
Policy train: iteration: 4000, policy_loss: 0.042063
Policy train: iteration: 4500, policy_loss: 0.036693
Policy train: iteration: 5000, policy_loss: 0.035691
IDM train: iteration: 500, idm_loss: 0.111153
IDM train: iteration: 1000, idm_loss: 0.091691
IDM train: iteration: 1500, idm_loss: 0.097909
IDM train: iteration: 2000, idm_loss: 0.108693
IDM train: iteration: 2500, idm_loss: 0.108349
IDM train: iteration: 3000, idm_loss: 0.103401
IDM train: iteration: 3500, idm_loss: 0.078450
IDM train: iteration: 4000, idm_loss: 0.063942
IDM train: iteration: 4500, idm_loss: 0.074764
IDM train: iteration: 5000, idm_loss: 0.078071

iteration: 3, total_reward: 242.59237570187588, policy_loss: 0.062645, idm_loss: 0.055939

Policy train: iteration: 500, policy_loss: 0.039514
Policy train: iteration: 1000, policy_loss: 0.036297
Policy train: iteration: 1500, policy_loss: 0.038782
