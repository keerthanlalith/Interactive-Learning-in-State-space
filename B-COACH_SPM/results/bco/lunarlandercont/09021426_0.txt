Collecting idm training data 1000
Collecting idm training data 2000
Collecting idm training data 3000
Collecting idm training data 4000
Collecting idm training data 5000
Collecting idm training data 6000
Collecting idm training data 7000
Collecting idm training data 8000
Collecting idm training data 9000
Collecting idm training data 10000
IDM train: iteration: 500, idm_loss: 0.002062
IDM train: iteration: 1000, idm_loss: 0.002306
IDM train: iteration: 1500, idm_loss: 0.021083
IDM train: iteration: 2000, idm_loss: 0.003234
IDM train: iteration: 2500, idm_loss: 0.008239
IDM train: iteration: 3000, idm_loss: 0.006996
IDM train: iteration: 3500, idm_loss: 0.016745
IDM train: iteration: 4000, idm_loss: 0.041273
IDM train: iteration: 4500, idm_loss: 0.099747
IDM train: iteration: 5000, idm_loss: 0.107703
Policy train: iteration: 500, policy_loss: 0.054360
Policy train: iteration: 1000, policy_loss: 0.043509
Policy train: iteration: 1500, policy_loss: 0.043895
Policy train: iteration: 2000, policy_loss: 0.040186
Policy train: iteration: 2500, policy_loss: 0.038865
Policy train: iteration: 3000, policy_loss: 0.038775
Policy train: iteration: 3500, policy_loss: 0.037892
Policy train: iteration: 4000, policy_loss: 0.034066
Policy train: iteration: 4500, policy_loss: 0.037828
Policy train: iteration: 5000, policy_loss: 0.031329
IDM train: iteration: 500, idm_loss: 0.111132
IDM train: iteration: 1000, idm_loss: 0.102189
IDM train: iteration: 1500, idm_loss: 0.081104
IDM train: iteration: 2000, idm_loss: 0.088580
IDM train: iteration: 2500, idm_loss: 0.076290
IDM train: iteration: 3000, idm_loss: 0.073127
IDM train: iteration: 3500, idm_loss: 0.117083
IDM train: iteration: 4000, idm_loss: 0.115455
IDM train: iteration: 4500, idm_loss: 0.062896
IDM train: iteration: 5000, idm_loss: 0.081196

iteration: 1, total_reward: 42.41259394439416, policy_loss: 0.066246, idm_loss: 0.092647

Policy train: iteration: 500, policy_loss: 0.040982
Policy train: iteration: 1000, policy_loss: 0.040990
Policy train: iteration: 1500, policy_loss: 0.038422
Policy train: iteration: 2000, policy_loss: 0.046613
Policy train: iteration: 2500, policy_loss: 0.044028
Policy train: iteration: 3000, policy_loss: 0.038699
Policy train: iteration: 3500, policy_loss: 0.036605
