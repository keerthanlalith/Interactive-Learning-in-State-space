Policy train: iteration: 500, policy_loss: 0.413627
Policy train: iteration: 1000, policy_loss: 0.392073
Policy train: iteration: 1500, policy_loss: 0.376024
Policy train: iteration: 2000, policy_loss: 0.333942
Policy train: iteration: 2500, policy_loss: 0.330660
Policy train: iteration: 3000, policy_loss: 0.377362
Policy train: iteration: 3500, policy_loss: 0.330843
Policy train: iteration: 4000, policy_loss: 0.323998
Policy train: iteration: 4500, policy_loss: 0.354259
Policy train: iteration: 5000, policy_loss: 0.366416
Policy train: iteration: 5500, policy_loss: 0.328668
Policy train: iteration: 6000, policy_loss: 0.321435
Policy train: iteration: 6500, policy_loss: 0.333246
Policy train: iteration: 7000, policy_loss: 0.298086
Policy train: iteration: 7500, policy_loss: 0.315589
Policy train: iteration: 8000, policy_loss: 0.290879
Policy train: iteration: 8500, policy_loss: 0.311511
Policy train: iteration: 9000, policy_loss: 0.309832

Background Trial: 1, reward: -83.1124176309792
Background Trial: 2, reward: -51.44873042449035
Background Trial: 3, reward: -50.45199639890662
Background Trial: 4, reward: -28.835378119990864
Background Trial: 5, reward: -336.2157817172115
Background Trial: 6, reward: -38.46404520829254
Background Trial: 7, reward: -37.74986237816623
Background Trial: 8, reward: -360.7316134234859
Background Trial: 9, reward: -67.42414973495892
Iteration: 1, average_reward: -117.15933055960912

Policy train: iteration: 500, policy_loss: 0.339339
Policy train: iteration: 1000, policy_loss: 0.328912
Policy train: iteration: 1500, policy_loss: 0.339757
Policy train: iteration: 2000, policy_loss: 0.315501
Policy train: iteration: 2500, policy_loss: 0.324281
Policy train: iteration: 3000, policy_loss: 0.284713
Policy train: iteration: 3500, policy_loss: 0.284340
Policy train: iteration: 4000, policy_loss: 0.289362
Policy train: iteration: 4500, policy_loss: 0.302458
Policy train: iteration: 5000, policy_loss: 0.273823
Policy train: iteration: 5500, policy_loss: 0.320168
Policy train: iteration: 6000, policy_loss: 0.279034
Policy train: iteration: 6500, policy_loss: 0.284969
Policy train: iteration: 7000, policy_loss: 0.288530
Policy train: iteration: 7500, policy_loss: 0.314228
Policy train: iteration: 8000, policy_loss: 0.284714
Policy train: iteration: 8500, policy_loss: 0.316418
Policy train: iteration: 9000, policy_loss: 0.287554

Background Trial: 1, reward: -76.78428614987409
Background Trial: 2, reward: -110.0648247803543
Background Trial: 3, reward: -75.36087531967725
Background Trial: 4, reward: -80.05587280486425
Background Trial: 5, reward: -189.4908315528617
Background Trial: 6, reward: -141.24961527144058
Background Trial: 7, reward: -226.51653454005668
Background Trial: 8, reward: -214.15057880356449
Background Trial: 9, reward: -104.83095232357442
Iteration: 2, average_reward: -135.38937461625196

Policy train: iteration: 500, policy_loss: 0.283870
Policy train: iteration: 1000, policy_loss: 0.259433
Policy train: iteration: 1500, policy_loss: 0.268152
Policy train: iteration: 2000, policy_loss: 0.263358
Policy train: iteration: 2500, policy_loss: 0.288337
Policy train: iteration: 3000, policy_loss: 0.279839
Policy train: iteration: 3500, policy_loss: 0.289785
Policy train: iteration: 4000, policy_loss: 0.273587
Policy train: iteration: 4500, policy_loss: 0.273883
Policy train: iteration: 5000, policy_loss: 0.257451
Policy train: iteration: 5500, policy_loss: 0.267976
Policy train: iteration: 6000, policy_loss: 0.267293
Policy train: iteration: 6500, policy_loss: 0.291757
Policy train: iteration: 7000, policy_loss: 0.293912
Policy train: iteration: 7500, policy_loss: 0.242999
Policy train: iteration: 8000, policy_loss: 0.276846
Policy train: iteration: 8500, policy_loss: 0.274127
Policy train: iteration: 9000, policy_loss: 0.275630

Background Trial: 1, reward: -84.7461727477762
Background Trial: 2, reward: -16.067715467229434
Background Trial: 3, reward: -40.366650353394554
Background Trial: 4, reward: -246.4842625045119
Background Trial: 5, reward: -222.68924018118656
Background Trial: 6, reward: -240.9211539532243
Background Trial: 7, reward: -47.36662615712747
Background Trial: 8, reward: -158.85892100092457
Background Trial: 9, reward: -87.86855580778257
Iteration: 3, average_reward: -127.26325535257304

Policy train: iteration: 500, policy_loss: 0.286132
Policy train: iteration: 1000, policy_loss: 0.283325
Policy train: iteration: 1500, policy_loss: 0.260280
Policy train: iteration: 2000, policy_loss: 0.273657
Policy train: iteration: 2500, policy_loss: 0.287640
Policy train: iteration: 3000, policy_loss: 0.270197
Policy train: iteration: 3500, policy_loss: 0.287684
Policy train: iteration: 4000, policy_loss: 0.292520
Policy train: iteration: 4500, policy_loss: 0.253666
Policy train: iteration: 5000, policy_loss: 0.269935
Policy train: iteration: 5500, policy_loss: 0.244989
Policy train: iteration: 6000, policy_loss: 0.249936
Policy train: iteration: 6500, policy_loss: 0.276274
Policy train: iteration: 7000, policy_loss: 0.269633
Policy train: iteration: 7500, policy_loss: 0.267071
Policy train: iteration: 8000, policy_loss: 0.261758
Policy train: iteration: 8500, policy_loss: 0.261439
Policy train: iteration: 9000, policy_loss: 0.253774

Background Trial: 1, reward: -58.791191665278724
Background Trial: 2, reward: -275.576784970229
Background Trial: 3, reward: 8.122166819788816
Background Trial: 4, reward: -276.40024564280253
Background Trial: 5, reward: -60.43961133662542
Background Trial: 6, reward: -27.01432948771395
Background Trial: 7, reward: -298.29676956844077
Background Trial: 8, reward: -82.42485263585601
Background Trial: 9, reward: -76.2869190703352
Iteration: 4, average_reward: -127.45650417305475

Policy train: iteration: 500, policy_loss: 0.260326
Policy train: iteration: 1000, policy_loss: 0.215557
Policy train: iteration: 1500, policy_loss: 0.265139
Policy train: iteration: 2000, policy_loss: 0.251933
Policy train: iteration: 2500, policy_loss: 0.272595
Policy train: iteration: 3000, policy_loss: 0.264905
Policy train: iteration: 3500, policy_loss: 0.244114
Policy train: iteration: 4000, policy_loss: 0.237628
Policy train: iteration: 4500, policy_loss: 0.251667
Policy train: iteration: 5000, policy_loss: 0.244031
Policy train: iteration: 5500, policy_loss: 0.269700
Policy train: iteration: 6000, policy_loss: 0.233954
Policy train: iteration: 6500, policy_loss: 0.262058
Policy train: iteration: 7000, policy_loss: 0.241015
Policy train: iteration: 7500, policy_loss: 0.242752
Policy train: iteration: 8000, policy_loss: 0.206181
Policy train: iteration: 8500, policy_loss: 0.230545
Policy train: iteration: 9000, policy_loss: 0.236795

Background Trial: 1, reward: -110.1339558228641
Background Trial: 2, reward: -114.42092009930703
Background Trial: 3, reward: -171.4134426647478
Background Trial: 4, reward: -66.67892907104365
Background Trial: 5, reward: -51.88031660196561
Background Trial: 6, reward: -298.3885782870734
Background Trial: 7, reward: -73.79567839351776
Background Trial: 8, reward: -49.04789443191324
Background Trial: 9, reward: -110.36768241216373
Iteration: 5, average_reward: -116.23637753162181

Policy train: iteration: 500, policy_loss: 0.253947
Policy train: iteration: 1000, policy_loss: 0.245848
Policy train: iteration: 1500, policy_loss: 0.257075
Policy train: iteration: 2000, policy_loss: 0.263308
Policy train: iteration: 2500, policy_loss: 0.237714
Policy train: iteration: 3000, policy_loss: 0.228484
Policy train: iteration: 3500, policy_loss: 0.243798
Policy train: iteration: 4000, policy_loss: 0.249869
Policy train: iteration: 4500, policy_loss: 0.256796
Policy train: iteration: 5000, policy_loss: 0.224087
Policy train: iteration: 5500, policy_loss: 0.249740
Policy train: iteration: 6000, policy_loss: 0.238233
Policy train: iteration: 6500, policy_loss: 0.232545
Policy train: iteration: 7000, policy_loss: 0.262300
Policy train: iteration: 7500, policy_loss: 0.227091
Policy train: iteration: 8000, policy_loss: 0.273427
Policy train: iteration: 8500, policy_loss: 0.250857
Policy train: iteration: 9000, policy_loss: 0.228696

Background Trial: 1, reward: -159.6529986958297
Background Trial: 2, reward: -261.9466303079693
Background Trial: 3, reward: -92.26291682968474
Background Trial: 4, reward: -119.55214538492058
Background Trial: 5, reward: -61.29445115755971
Background Trial: 6, reward: -76.11306783230417
Background Trial: 7, reward: -81.38201048341547
Background Trial: 8, reward: -91.28239452430684
Background Trial: 9, reward: -147.29090719307212
Iteration: 6, average_reward: -121.19750248989584

Policy train: iteration: 500, policy_loss: 0.251533
Policy train: iteration: 1000, policy_loss: 0.256359
Policy train: iteration: 1500, policy_loss: 0.247630
Policy train: iteration: 2000, policy_loss: 0.224917
Policy train: iteration: 2500, policy_loss: 0.260135
Policy train: iteration: 3000, policy_loss: 0.225602
Policy train: iteration: 3500, policy_loss: 0.237770
Policy train: iteration: 4000, policy_loss: 0.227069
Policy train: iteration: 4500, policy_loss: 0.246079
Policy train: iteration: 5000, policy_loss: 0.232903
Policy train: iteration: 5500, policy_loss: 0.214842
Policy train: iteration: 6000, policy_loss: 0.246471
Policy train: iteration: 6500, policy_loss: 0.209645
Policy train: iteration: 7000, policy_loss: 0.232188
Policy train: iteration: 7500, policy_loss: 0.225407
Policy train: iteration: 8000, policy_loss: 0.202344
Policy train: iteration: 8500, policy_loss: 0.227683
Policy train: iteration: 9000, policy_loss: 0.213271

Background Trial: 1, reward: -211.99481897422686
Background Trial: 2, reward: -99.8921671177401
Background Trial: 3, reward: -118.5298271883963
Background Trial: 4, reward: -150.17273416019614
Background Trial: 5, reward: -32.72159814554561
Background Trial: 6, reward: -47.28264460259214
Background Trial: 7, reward: -28.08239293626616
Background Trial: 8, reward: -100.20168666107841
Background Trial: 9, reward: -93.7766627054702
Iteration: 7, average_reward: -98.0727258323902

Policy train: iteration: 500, policy_loss: 0.225309
Policy train: iteration: 1000, policy_loss: 0.237172
Policy train: iteration: 1500, policy_loss: 0.227968
Policy train: iteration: 2000, policy_loss: 0.237755
Policy train: iteration: 2500, policy_loss: 0.230718
Policy train: iteration: 3000, policy_loss: 0.244548
Policy train: iteration: 3500, policy_loss: 0.242753
Policy train: iteration: 4000, policy_loss: 0.223089
Policy train: iteration: 4500, policy_loss: 0.233675
Policy train: iteration: 5000, policy_loss: 0.215310
Policy train: iteration: 5500, policy_loss: 0.213115
Policy train: iteration: 6000, policy_loss: 0.205989
Policy train: iteration: 6500, policy_loss: 0.236901
Policy train: iteration: 7000, policy_loss: 0.220657
Policy train: iteration: 7500, policy_loss: 0.231421
Policy train: iteration: 8000, policy_loss: 0.217056
Policy train: iteration: 8500, policy_loss: 0.190947
Policy train: iteration: 9000, policy_loss: 0.211720

Background Trial: 1, reward: -216.15492129080613
Background Trial: 2, reward: -316.68097555860936
Background Trial: 3, reward: -119.34451292759852
Background Trial: 4, reward: -181.18850386608835
Background Trial: 5, reward: -549.4246290638339
Background Trial: 6, reward: -13.789345659616217
Background Trial: 7, reward: -131.8865573970302
Background Trial: 8, reward: -235.94185843308844
Background Trial: 9, reward: -105.30908026444783
Iteration: 8, average_reward: -207.7467093845688

Policy train: iteration: 500, policy_loss: 0.215670
Policy train: iteration: 1000, policy_loss: 0.245627
Policy train: iteration: 1500, policy_loss: 0.251867
Policy train: iteration: 2000, policy_loss: 0.205567
Policy train: iteration: 2500, policy_loss: 0.252156
Policy train: iteration: 3000, policy_loss: 0.205470
Policy train: iteration: 3500, policy_loss: 0.225393
Policy train: iteration: 4000, policy_loss: 0.236238
Policy train: iteration: 4500, policy_loss: 0.216894
Policy train: iteration: 5000, policy_loss: 0.229981
Policy train: iteration: 5500, policy_loss: 0.228715
Policy train: iteration: 6000, policy_loss: 0.202103
Policy train: iteration: 6500, policy_loss: 0.245266
Policy train: iteration: 7000, policy_loss: 0.243073
Policy train: iteration: 7500, policy_loss: 0.243810
Policy train: iteration: 8000, policy_loss: 0.237077
Policy train: iteration: 8500, policy_loss: 0.213886
Policy train: iteration: 9000, policy_loss: 0.206628

Background Trial: 1, reward: -131.54875338574323
Background Trial: 2, reward: -57.65678712752328
Background Trial: 3, reward: -136.18246274516565
Background Trial: 4, reward: -87.26593460609048
Background Trial: 5, reward: -76.2925888919701
Background Trial: 6, reward: -76.72995650538974
Background Trial: 7, reward: -412.027110767199
Background Trial: 8, reward: -201.43070962289323
Background Trial: 9, reward: -209.65908369233023
Iteration: 9, average_reward: -154.31037637158943

Policy train: iteration: 500, policy_loss: 0.217806
Policy train: iteration: 1000, policy_loss: 0.249622
Policy train: iteration: 1500, policy_loss: 0.220786
Policy train: iteration: 2000, policy_loss: 0.203422
Policy train: iteration: 2500, policy_loss: 0.226263
Policy train: iteration: 3000, policy_loss: 0.227975
Policy train: iteration: 3500, policy_loss: 0.240327
Policy train: iteration: 4000, policy_loss: 0.202885
Policy train: iteration: 4500, policy_loss: 0.212222
Policy train: iteration: 5000, policy_loss: 0.217250
Policy train: iteration: 5500, policy_loss: 0.203461
Policy train: iteration: 6000, policy_loss: 0.197287
Policy train: iteration: 6500, policy_loss: 0.190620
Policy train: iteration: 7000, policy_loss: 0.197847
Policy train: iteration: 7500, policy_loss: 0.204224
Policy train: iteration: 8000, policy_loss: 0.219495
Policy train: iteration: 8500, policy_loss: 0.205942
Policy train: iteration: 9000, policy_loss: 0.211004

Background Trial: 1, reward: -166.52144395052835
Background Trial: 2, reward: -332.84528344980095
Background Trial: 3, reward: -127.50174638861839
Background Trial: 4, reward: -90.79658919302616
Background Trial: 5, reward: -102.4460503732856
Background Trial: 6, reward: -144.451337014326
Background Trial: 7, reward: -122.18367797678873
Background Trial: 8, reward: -220.42675513244836
Background Trial: 9, reward: -0.23162837787320711
Iteration: 10, average_reward: -145.2671679840773

Policy train: iteration: 500, policy_loss: 0.191728
Policy train: iteration: 1000, policy_loss: 0.196737
Policy train: iteration: 1500, policy_loss: 0.219026
Policy train: iteration: 2000, policy_loss: 0.195482
Policy train: iteration: 2500, policy_loss: 0.237886
Policy train: iteration: 3000, policy_loss: 0.185083
Policy train: iteration: 3500, policy_loss: 0.206137
Policy train: iteration: 4000, policy_loss: 0.232620
Policy train: iteration: 4500, policy_loss: 0.207918
Policy train: iteration: 5000, policy_loss: 0.203564
Policy train: iteration: 5500, policy_loss: 0.218065
Policy train: iteration: 6000, policy_loss: 0.212953
Policy train: iteration: 6500, policy_loss: 0.219918
Policy train: iteration: 7000, policy_loss: 0.174086
Policy train: iteration: 7500, policy_loss: 0.207966
Policy train: iteration: 8000, policy_loss: 0.176727
Policy train: iteration: 8500, policy_loss: 0.183603
Policy train: iteration: 9000, policy_loss: 0.222619

Background Trial: 1, reward: -137.03743539698598
Background Trial: 2, reward: -25.774618912940056
Background Trial: 3, reward: -54.69747531816547
Background Trial: 4, reward: -123.1143689777283
Background Trial: 5, reward: -3.0602641209798236
Background Trial: 6, reward: -91.58897243801499
Background Trial: 7, reward: -11.94287010235422
Background Trial: 8, reward: -123.31373183009595
Background Trial: 9, reward: -193.83172404638077
Iteration: 11, average_reward: -84.92905123818284

Policy train: iteration: 500, policy_loss: 0.188725
Policy train: iteration: 1000, policy_loss: 0.181146
Policy train: iteration: 1500, policy_loss: 0.192949
Policy train: iteration: 2000, policy_loss: 0.220277
Policy train: iteration: 2500, policy_loss: 0.196504
Policy train: iteration: 3000, policy_loss: 0.212867
Policy train: iteration: 3500, policy_loss: 0.200947
Policy train: iteration: 4000, policy_loss: 0.186263
Policy train: iteration: 4500, policy_loss: 0.213109
Policy train: iteration: 5000, policy_loss: 0.240074
Policy train: iteration: 5500, policy_loss: 0.187501
Policy train: iteration: 6000, policy_loss: 0.205147
Policy train: iteration: 6500, policy_loss: 0.225080
Policy train: iteration: 7000, policy_loss: 0.196794
Policy train: iteration: 7500, policy_loss: 0.219799
Policy train: iteration: 8000, policy_loss: 0.224156
Policy train: iteration: 8500, policy_loss: 0.201061
Policy train: iteration: 9000, policy_loss: 0.209875

Background Trial: 1, reward: -314.0447838882125
Background Trial: 2, reward: -52.869219246304304
Background Trial: 3, reward: -149.90799935127504
Background Trial: 4, reward: -278.7925314834654
Background Trial: 5, reward: -73.40558979790273
Background Trial: 6, reward: 258.94844737624896
Background Trial: 7, reward: -153.4279681929167
Background Trial: 8, reward: -297.3032003397349
Background Trial: 9, reward: -120.7535060817058
Iteration: 12, average_reward: -131.28403900058538

Policy train: iteration: 500, policy_loss: 0.198870
Policy train: iteration: 1000, policy_loss: 0.224696
Policy train: iteration: 1500, policy_loss: 0.184048
Policy train: iteration: 2000, policy_loss: 0.207955
Policy train: iteration: 2500, policy_loss: 0.195037
Policy train: iteration: 3000, policy_loss: 0.252409
Policy train: iteration: 3500, policy_loss: 0.201649
Policy train: iteration: 4000, policy_loss: 0.196621
Policy train: iteration: 4500, policy_loss: 0.204231
Policy train: iteration: 5000, policy_loss: 0.206348
Policy train: iteration: 5500, policy_loss: 0.173134
Policy train: iteration: 6000, policy_loss: 0.188310
Policy train: iteration: 6500, policy_loss: 0.202407
Policy train: iteration: 7000, policy_loss: 0.197350
Policy train: iteration: 7500, policy_loss: 0.201051
Policy train: iteration: 8000, policy_loss: 0.196955
Policy train: iteration: 8500, policy_loss: 0.206576
Policy train: iteration: 9000, policy_loss: 0.194705

Background Trial: 1, reward: -208.85519688862087
Background Trial: 2, reward: -150.6688623558873
Background Trial: 3, reward: -242.41720083425452
Background Trial: 4, reward: -106.97869047032012
Background Trial: 5, reward: -132.87227957679983
Background Trial: 6, reward: -477.11738637173033
Background Trial: 7, reward: -120.68536099058502
Background Trial: 8, reward: -166.49047710156964
Background Trial: 9, reward: -86.77833443217469
Iteration: 13, average_reward: -188.0959765579936

Policy train: iteration: 500, policy_loss: 0.171603
Policy train: iteration: 1000, policy_loss: 0.204808
Policy train: iteration: 1500, policy_loss: 0.179300
Policy train: iteration: 2000, policy_loss: 0.211378
Policy train: iteration: 2500, policy_loss: 0.195577
Policy train: iteration: 3000, policy_loss: 0.182105
Policy train: iteration: 3500, policy_loss: 0.229239
Policy train: iteration: 4000, policy_loss: 0.189222
Policy train: iteration: 4500, policy_loss: 0.204695
Policy train: iteration: 5000, policy_loss: 0.207828
Policy train: iteration: 5500, policy_loss: 0.192196
Policy train: iteration: 6000, policy_loss: 0.215966
Policy train: iteration: 6500, policy_loss: 0.193660
Policy train: iteration: 7000, policy_loss: 0.191917
Policy train: iteration: 7500, policy_loss: 0.207416
Policy train: iteration: 8000, policy_loss: 0.205961
Policy train: iteration: 8500, policy_loss: 0.186929
Policy train: iteration: 9000, policy_loss: 0.200371

Background Trial: 1, reward: -161.04512137797704
Background Trial: 2, reward: -52.23802053329342
Background Trial: 3, reward: -71.86727088135976
Background Trial: 4, reward: -227.78835015310645
Background Trial: 5, reward: -290.5697882715841
Background Trial: 6, reward: -22.164461881087917
Background Trial: 7, reward: -5.015128873817446
Background Trial: 8, reward: -142.61974707391477
Background Trial: 9, reward: -413.8630594169361
Iteration: 14, average_reward: -154.13010538478636

Policy train: iteration: 500, policy_loss: 0.221011
Policy train: iteration: 1000, policy_loss: 0.177570
Policy train: iteration: 1500, policy_loss: 0.198956
Policy train: iteration: 2000, policy_loss: 0.196949
Policy train: iteration: 2500, policy_loss: 0.175199
Policy train: iteration: 3000, policy_loss: 0.205414
Policy train: iteration: 3500, policy_loss: 0.211405
Policy train: iteration: 4000, policy_loss: 0.180278
Policy train: iteration: 4500, policy_loss: 0.176637
Policy train: iteration: 5000, policy_loss: 0.196623
Policy train: iteration: 5500, policy_loss: 0.202016
Policy train: iteration: 6000, policy_loss: 0.189218
Policy train: iteration: 6500, policy_loss: 0.179835
Policy train: iteration: 7000, policy_loss: 0.184267
Policy train: iteration: 7500, policy_loss: 0.183790
Policy train: iteration: 8000, policy_loss: 0.169265
Policy train: iteration: 8500, policy_loss: 0.168281
Policy train: iteration: 9000, policy_loss: 0.188653

Background Trial: 1, reward: -98.67978058660348
Background Trial: 2, reward: -146.64278335775805
Background Trial: 3, reward: -124.50562626661053
Background Trial: 4, reward: 15.551134961235633
Background Trial: 5, reward: -93.82719326423324
Background Trial: 6, reward: -30.47302472786339
Background Trial: 7, reward: -198.02669896516545
Background Trial: 8, reward: -125.22256217260028
Background Trial: 9, reward: 8.198231036665689
Iteration: 15, average_reward: -88.18092259365925

Policy train: iteration: 500, policy_loss: 0.198675
Policy train: iteration: 1000, policy_loss: 0.180665
Policy train: iteration: 1500, policy_loss: 0.166379
Policy train: iteration: 2000, policy_loss: 0.210275
Policy train: iteration: 2500, policy_loss: 0.176150
Policy train: iteration: 3000, policy_loss: 0.224290
Policy train: iteration: 3500, policy_loss: 0.183826
Policy train: iteration: 4000, policy_loss: 0.209860
Policy train: iteration: 4500, policy_loss: 0.185265
Policy train: iteration: 5000, policy_loss: 0.193913
Policy train: iteration: 5500, policy_loss: 0.199013
Policy train: iteration: 6000, policy_loss: 0.210800
Policy train: iteration: 6500, policy_loss: 0.170346
Policy train: iteration: 7000, policy_loss: 0.197042
Policy train: iteration: 7500, policy_loss: 0.142929
Policy train: iteration: 8000, policy_loss: 0.199992
Policy train: iteration: 8500, policy_loss: 0.157782
Policy train: iteration: 9000, policy_loss: 0.185227

Background Trial: 1, reward: -167.8564108153087
Background Trial: 2, reward: -31.439216588769852
Background Trial: 3, reward: -174.21769401571828
Background Trial: 4, reward: -54.069499734063605
Background Trial: 5, reward: -150.16759444865406
Background Trial: 6, reward: -292.3376584555473
Background Trial: 7, reward: 20.13204445340699
Background Trial: 8, reward: -10.05377679579719
Background Trial: 9, reward: -98.13706012390784
Iteration: 16, average_reward: -106.46076294715108

Policy train: iteration: 500, policy_loss: 0.203431
Policy train: iteration: 1000, policy_loss: 0.215353
Policy train: iteration: 1500, policy_loss: 0.200005
Policy train: iteration: 2000, policy_loss: 0.193283
Policy train: iteration: 2500, policy_loss: 0.153487
Policy train: iteration: 3000, policy_loss: 0.187891
Policy train: iteration: 3500, policy_loss: 0.194381
Policy train: iteration: 4000, policy_loss: 0.232018
Policy train: iteration: 4500, policy_loss: 0.194291
Policy train: iteration: 5000, policy_loss: 0.185720
Policy train: iteration: 5500, policy_loss: 0.198671
Policy train: iteration: 6000, policy_loss: 0.191459
Policy train: iteration: 6500, policy_loss: 0.180910
Policy train: iteration: 7000, policy_loss: 0.180851
Policy train: iteration: 7500, policy_loss: 0.163023
Policy train: iteration: 8000, policy_loss: 0.178662
Policy train: iteration: 8500, policy_loss: 0.172129
Policy train: iteration: 9000, policy_loss: 0.192252

Background Trial: 1, reward: -138.282690260181
Background Trial: 2, reward: -139.21518803365382
Background Trial: 3, reward: -15.999433446101307
Background Trial: 4, reward: -207.88748768754238
Background Trial: 5, reward: -245.3683963793278
Background Trial: 6, reward: -135.57934759721832
Background Trial: 7, reward: -124.9156135539942
Background Trial: 8, reward: -145.00345303245564
Background Trial: 9, reward: -117.74348620343955
Iteration: 17, average_reward: -141.1105662437682

Policy train: iteration: 500, policy_loss: 0.187592
Policy train: iteration: 1000, policy_loss: 0.189803
Policy train: iteration: 1500, policy_loss: 0.251098
Policy train: iteration: 2000, policy_loss: 0.196011
Policy train: iteration: 2500, policy_loss: 0.182847
Policy train: iteration: 3000, policy_loss: 0.192332
Policy train: iteration: 3500, policy_loss: 0.161831
Policy train: iteration: 4000, policy_loss: 0.166348
Policy train: iteration: 4500, policy_loss: 0.198593
Policy train: iteration: 5000, policy_loss: 0.166423
Policy train: iteration: 5500, policy_loss: 0.195351
Policy train: iteration: 6000, policy_loss: 0.172872
Policy train: iteration: 6500, policy_loss: 0.192210
Policy train: iteration: 7000, policy_loss: 0.204949
Policy train: iteration: 7500, policy_loss: 0.185653
Policy train: iteration: 8000, policy_loss: 0.162622
Policy train: iteration: 8500, policy_loss: 0.158211
Policy train: iteration: 9000, policy_loss: 0.182670

Background Trial: 1, reward: -259.84540998305624
Background Trial: 2, reward: -3.329869797987101
Background Trial: 3, reward: -311.19756240976574
Background Trial: 4, reward: -11.95467334716453
Background Trial: 5, reward: -22.124829571960333
Background Trial: 6, reward: -53.78107943252293
Background Trial: 7, reward: -275.715126282308
Background Trial: 8, reward: -30.231296304915887
Background Trial: 9, reward: -201.88743083202058
Iteration: 18, average_reward: -130.00747532907792

Policy train: iteration: 500, policy_loss: 0.187281
Policy train: iteration: 1000, policy_loss: 0.183797
Policy train: iteration: 1500, policy_loss: 0.191022
Policy train: iteration: 2000, policy_loss: 0.192116
Policy train: iteration: 2500, policy_loss: 0.192833
Policy train: iteration: 3000, policy_loss: 0.188719
Policy train: iteration: 3500, policy_loss: 0.151348
Policy train: iteration: 4000, policy_loss: 0.161698
Policy train: iteration: 4500, policy_loss: 0.218856
Policy train: iteration: 5000, policy_loss: 0.177725
Policy train: iteration: 5500, policy_loss: 0.180796
Policy train: iteration: 6000, policy_loss: 0.177158
Policy train: iteration: 6500, policy_loss: 0.192698
Policy train: iteration: 7000, policy_loss: 0.160538
Policy train: iteration: 7500, policy_loss: 0.182522
Policy train: iteration: 8000, policy_loss: 0.175882
Policy train: iteration: 8500, policy_loss: 0.180558
Policy train: iteration: 9000, policy_loss: 0.182531

Background Trial: 1, reward: -53.41368606786424
Background Trial: 2, reward: -56.92238176446344
Background Trial: 3, reward: -161.57316392868108
Background Trial: 4, reward: 3.665390321911488
Background Trial: 5, reward: -28.24106059571264
Background Trial: 6, reward: -244.80885952394664
Background Trial: 7, reward: -3.0161028256412123
Background Trial: 8, reward: -47.13666350688517
Background Trial: 9, reward: -32.629031479302625
Iteration: 19, average_reward: -69.34172881895395

Policy train: iteration: 500, policy_loss: 0.186787
Policy train: iteration: 1000, policy_loss: 0.169690
Policy train: iteration: 1500, policy_loss: 0.155657
Policy train: iteration: 2000, policy_loss: 0.201656
Policy train: iteration: 2500, policy_loss: 0.172090
Policy train: iteration: 3000, policy_loss: 0.164035
Policy train: iteration: 3500, policy_loss: 0.200586
Policy train: iteration: 4000, policy_loss: 0.174784
Policy train: iteration: 4500, policy_loss: 0.176680
Policy train: iteration: 5000, policy_loss: 0.189350
Policy train: iteration: 5500, policy_loss: 0.192941
Policy train: iteration: 6000, policy_loss: 0.157121
Policy train: iteration: 6500, policy_loss: 0.158881
Policy train: iteration: 7000, policy_loss: 0.170915
Policy train: iteration: 7500, policy_loss: 0.178754
Policy train: iteration: 8000, policy_loss: 0.154243
Policy train: iteration: 8500, policy_loss: 0.203526
Policy train: iteration: 9000, policy_loss: 0.178035

Background Trial: 1, reward: -378.0439567888564
Background Trial: 2, reward: -38.722690663402076
Background Trial: 3, reward: -68.0735500623868
Background Trial: 4, reward: -113.49443710477641
Background Trial: 5, reward: -254.4695211166148
Background Trial: 6, reward: -93.51206590471557
Background Trial: 7, reward: 32.844942280474356
Background Trial: 8, reward: -249.3221033082715
Background Trial: 9, reward: -114.07673941646314
Iteration: 20, average_reward: -141.87445800944582

Policy train: iteration: 500, policy_loss: 0.200181
Policy train: iteration: 1000, policy_loss: 0.162100
Policy train: iteration: 1500, policy_loss: 0.192306
Policy train: iteration: 2000, policy_loss: 0.157281
Policy train: iteration: 2500, policy_loss: 0.182198
Policy train: iteration: 3000, policy_loss: 0.228634
Policy train: iteration: 3500, policy_loss: 0.170335
Policy train: iteration: 4000, policy_loss: 0.141885
Policy train: iteration: 4500, policy_loss: 0.169900
Policy train: iteration: 5000, policy_loss: 0.163738
Policy train: iteration: 5500, policy_loss: 0.202646
Policy train: iteration: 6000, policy_loss: 0.175512
Policy train: iteration: 6500, policy_loss: 0.171268
Policy train: iteration: 7000, policy_loss: 0.164981
Policy train: iteration: 7500, policy_loss: 0.160067
Policy train: iteration: 8000, policy_loss: 0.165004
Policy train: iteration: 8500, policy_loss: 0.165021
Policy train: iteration: 9000, policy_loss: 0.183779

Background Trial: 1, reward: -385.7351157944515
Background Trial: 2, reward: -103.44133884243578
Background Trial: 3, reward: -34.082219086584985
Background Trial: 4, reward: -826.9291989239468
Background Trial: 5, reward: -253.09969553330203
Background Trial: 6, reward: -84.93796338444153
Background Trial: 7, reward: -391.5392790637211
Background Trial: 8, reward: -29.673261971408834
Background Trial: 9, reward: -135.50866660615503
Iteration: 21, average_reward: -249.43852657849416

Policy train: iteration: 500, policy_loss: 0.194263
Policy train: iteration: 1000, policy_loss: 0.163587
Policy train: iteration: 1500, policy_loss: 0.157708
Policy train: iteration: 2000, policy_loss: 0.177984
Policy train: iteration: 2500, policy_loss: 0.166877
Policy train: iteration: 3000, policy_loss: 0.145103
Policy train: iteration: 3500, policy_loss: 0.153206
Policy train: iteration: 4000, policy_loss: 0.184617
Policy train: iteration: 4500, policy_loss: 0.171383
Policy train: iteration: 5000, policy_loss: 0.169820
Policy train: iteration: 5500, policy_loss: 0.160511
Policy train: iteration: 6000, policy_loss: 0.204398
Policy train: iteration: 6500, policy_loss: 0.181598
Policy train: iteration: 7000, policy_loss: 0.189999
Policy train: iteration: 7500, policy_loss: 0.151983
Policy train: iteration: 8000, policy_loss: 0.218909
Policy train: iteration: 8500, policy_loss: 0.168641
Policy train: iteration: 9000, policy_loss: 0.199196

Background Trial: 1, reward: -33.90840698392897
Background Trial: 2, reward: -131.13261846034237
Background Trial: 3, reward: -44.49690837322835
Background Trial: 4, reward: -140.1538386498355
Background Trial: 5, reward: -146.9658015403818
Background Trial: 6, reward: -135.6751127938476
Background Trial: 7, reward: -308.1314150736596
Background Trial: 8, reward: -149.54916058930846
Background Trial: 9, reward: -124.04723205512632
Iteration: 22, average_reward: -134.89561050218433

Policy train: iteration: 500, policy_loss: 0.157162
Policy train: iteration: 1000, policy_loss: 0.195665
Policy train: iteration: 1500, policy_loss: 0.165251
Policy train: iteration: 2000, policy_loss: 0.176772
Policy train: iteration: 2500, policy_loss: 0.150043
Policy train: iteration: 3000, policy_loss: 0.170489
Policy train: iteration: 3500, policy_loss: 0.182835
Policy train: iteration: 4000, policy_loss: 0.174758
Policy train: iteration: 4500, policy_loss: 0.223330
Policy train: iteration: 5000, policy_loss: 0.153156
Policy train: iteration: 5500, policy_loss: 0.168519
Policy train: iteration: 6000, policy_loss: 0.171079
Policy train: iteration: 6500, policy_loss: 0.174335
Policy train: iteration: 7000, policy_loss: 0.185734
Policy train: iteration: 7500, policy_loss: 0.178294
Policy train: iteration: 8000, policy_loss: 0.197048
Policy train: iteration: 8500, policy_loss: 0.132428
Policy train: iteration: 9000, policy_loss: 0.172904

Background Trial: 1, reward: -12.403951517733589
Background Trial: 2, reward: -7.360762586864382
Background Trial: 3, reward: -141.63611570662823
Background Trial: 4, reward: -22.784698350139465
Background Trial: 5, reward: -1.7848434546258858
Background Trial: 6, reward: -32.94367719866533
Background Trial: 7, reward: -99.73813337848222
Background Trial: 8, reward: -205.62304839076256
Background Trial: 9, reward: -6.821920571182844
Iteration: 23, average_reward: -59.01079457278718

Policy train: iteration: 500, policy_loss: 0.163060
Policy train: iteration: 1000, policy_loss: 0.177237
Policy train: iteration: 1500, policy_loss: 0.179844
Policy train: iteration: 2000, policy_loss: 0.172256
Policy train: iteration: 2500, policy_loss: 0.169180
Policy train: iteration: 3000, policy_loss: 0.173996
Policy train: iteration: 3500, policy_loss: 0.153318
Policy train: iteration: 4000, policy_loss: 0.171319
Policy train: iteration: 4500, policy_loss: 0.161993
Policy train: iteration: 5000, policy_loss: 0.171683
Policy train: iteration: 5500, policy_loss: 0.178650
Policy train: iteration: 6000, policy_loss: 0.151727
Policy train: iteration: 6500, policy_loss: 0.182953
Policy train: iteration: 7000, policy_loss: 0.160968
Policy train: iteration: 7500, policy_loss: 0.197340
Policy train: iteration: 8000, policy_loss: 0.168085
Policy train: iteration: 8500, policy_loss: 0.174296
Policy train: iteration: 9000, policy_loss: 0.173182

Background Trial: 1, reward: -187.48860020052763
Background Trial: 2, reward: -144.84612335584083
Background Trial: 3, reward: -160.2499156575575
Background Trial: 4, reward: 6.7541755355773745
Background Trial: 5, reward: 47.575670081953746
Background Trial: 6, reward: -32.29992764459081
Background Trial: 7, reward: -23.021352024928646
Background Trial: 8, reward: -13.857347109525634
Background Trial: 9, reward: -206.7950430012861
Iteration: 24, average_reward: -79.35871815296956

Policy train: iteration: 500, policy_loss: 0.197672
Policy train: iteration: 1000, policy_loss: 0.191130
Policy train: iteration: 1500, policy_loss: 0.147623
Policy train: iteration: 2000, policy_loss: 0.195599
Policy train: iteration: 2500, policy_loss: 0.153497
Policy train: iteration: 3000, policy_loss: 0.177179
Policy train: iteration: 3500, policy_loss: 0.195290
Policy train: iteration: 4000, policy_loss: 0.169938
Policy train: iteration: 4500, policy_loss: 0.180109
Policy train: iteration: 5000, policy_loss: 0.144480
Policy train: iteration: 5500, policy_loss: 0.165609
Policy train: iteration: 6000, policy_loss: 0.170002
Policy train: iteration: 6500, policy_loss: 0.153757
Policy train: iteration: 7000, policy_loss: 0.187361
Policy train: iteration: 7500, policy_loss: 0.161157
Policy train: iteration: 8000, policy_loss: 0.175726
Policy train: iteration: 8500, policy_loss: 0.173964
Policy train: iteration: 9000, policy_loss: 0.160227

Background Trial: 1, reward: -114.37099524697626
Background Trial: 2, reward: 0.48098928804415664
Background Trial: 3, reward: -115.80125087737042
Background Trial: 4, reward: -138.88962221726229
Background Trial: 5, reward: -72.84908443566131
Background Trial: 6, reward: -35.212077803946556
Background Trial: 7, reward: -228.5091190563079
Background Trial: 8, reward: -269.28552529090746
Background Trial: 9, reward: -100.73351246708188
Iteration: 25, average_reward: -119.46335534527442

Policy train: iteration: 500, policy_loss: 0.156326
Policy train: iteration: 1000, policy_loss: 0.186975
Policy train: iteration: 1500, policy_loss: 0.179966
Policy train: iteration: 2000, policy_loss: 0.184769
Policy train: iteration: 2500, policy_loss: 0.150936
Policy train: iteration: 3000, policy_loss: 0.152398
Policy train: iteration: 3500, policy_loss: 0.186966
Policy train: iteration: 4000, policy_loss: 0.158658
Policy train: iteration: 4500, policy_loss: 0.178589
Policy train: iteration: 5000, policy_loss: 0.163716
Policy train: iteration: 5500, policy_loss: 0.178987
Policy train: iteration: 6000, policy_loss: 0.168678
Policy train: iteration: 6500, policy_loss: 0.167021
Policy train: iteration: 7000, policy_loss: 0.170443
Policy train: iteration: 7500, policy_loss: 0.177457
Policy train: iteration: 8000, policy_loss: 0.157410
Policy train: iteration: 8500, policy_loss: 0.155858
Policy train: iteration: 9000, policy_loss: 0.135894

Background Trial: 1, reward: -92.55429432247182
Background Trial: 2, reward: -156.27942727912693
Background Trial: 3, reward: 27.42717957789388
Background Trial: 4, reward: -181.89152689738586
Background Trial: 5, reward: -283.4547793175027
Background Trial: 6, reward: -279.22462389653816
Background Trial: 7, reward: -319.94858295879334
Background Trial: 8, reward: -480.57987372172994
Background Trial: 9, reward: -109.25363541418596
Iteration: 26, average_reward: -208.4177293588712

Policy train: iteration: 500, policy_loss: 0.149964
Policy train: iteration: 1000, policy_loss: 0.143187
Policy train: iteration: 1500, policy_loss: 0.165170
Policy train: iteration: 2000, policy_loss: 0.202938
Policy train: iteration: 2500, policy_loss: 0.162498
Policy train: iteration: 3000, policy_loss: 0.160463
Policy train: iteration: 3500, policy_loss: 0.152633
Policy train: iteration: 4000, policy_loss: 0.143657
Policy train: iteration: 4500, policy_loss: 0.184679
Policy train: iteration: 5000, policy_loss: 0.164473
Policy train: iteration: 5500, policy_loss: 0.191157
Policy train: iteration: 6000, policy_loss: 0.135929
Policy train: iteration: 6500, policy_loss: 0.168737
Policy train: iteration: 7000, policy_loss: 0.161975
Policy train: iteration: 7500, policy_loss: 0.162475
Policy train: iteration: 8000, policy_loss: 0.164459
Policy train: iteration: 8500, policy_loss: 0.150027
Policy train: iteration: 9000, policy_loss: 0.158229

Background Trial: 1, reward: -37.965245857028
Background Trial: 2, reward: -233.55139226014282
Background Trial: 3, reward: -156.3495400760223
Background Trial: 4, reward: -141.00657505127296
Background Trial: 5, reward: -111.27461775972198
Background Trial: 6, reward: -137.84779810690557
Background Trial: 7, reward: -128.38961112661417
Background Trial: 8, reward: -492.6851480212325
Background Trial: 9, reward: -117.8582706491174
Iteration: 27, average_reward: -172.9920221008953

Policy train: iteration: 500, policy_loss: 0.202554
Policy train: iteration: 1000, policy_loss: 0.142136
Policy train: iteration: 1500, policy_loss: 0.157829
Policy train: iteration: 2000, policy_loss: 0.151656
Policy train: iteration: 2500, policy_loss: 0.202206
Policy train: iteration: 3000, policy_loss: 0.152326
Policy train: iteration: 3500, policy_loss: 0.185342
Policy train: iteration: 4000, policy_loss: 0.177316
Policy train: iteration: 4500, policy_loss: 0.203651
Policy train: iteration: 5000, policy_loss: 0.140316
Policy train: iteration: 5500, policy_loss: 0.152177
Policy train: iteration: 6000, policy_loss: 0.180542
Policy train: iteration: 6500, policy_loss: 0.147006
Policy train: iteration: 7000, policy_loss: 0.153171
Policy train: iteration: 7500, policy_loss: 0.133435
Policy train: iteration: 8000, policy_loss: 0.162127
Policy train: iteration: 8500, policy_loss: 0.154425
Policy train: iteration: 9000, policy_loss: 0.164851

Background Trial: 1, reward: -151.12012793310583
Background Trial: 2, reward: -62.63502654253218
Background Trial: 3, reward: -247.99774090278808
Background Trial: 4, reward: -125.76448473452992
Background Trial: 5, reward: -38.82736588211651
Background Trial: 6, reward: -13.849413612838191
Background Trial: 7, reward: -423.61196144060955
Background Trial: 8, reward: -31.202107210334333
Background Trial: 9, reward: 10.2519810297036
Iteration: 28, average_reward: -120.52847191435012

Policy train: iteration: 500, policy_loss: 0.169188
Policy train: iteration: 1000, policy_loss: 0.180605
Policy train: iteration: 1500, policy_loss: 0.147433
Policy train: iteration: 2000, policy_loss: 0.142194
Policy train: iteration: 2500, policy_loss: 0.167882
Policy train: iteration: 3000, policy_loss: 0.161131
Policy train: iteration: 3500, policy_loss: 0.174033
Policy train: iteration: 4000, policy_loss: 0.145859
Policy train: iteration: 4500, policy_loss: 0.135646
Policy train: iteration: 5000, policy_loss: 0.144758
Policy train: iteration: 5500, policy_loss: 0.145616
Policy train: iteration: 6000, policy_loss: 0.152031
Policy train: iteration: 6500, policy_loss: 0.180534
Policy train: iteration: 7000, policy_loss: 0.172018
Policy train: iteration: 7500, policy_loss: 0.146195
Policy train: iteration: 8000, policy_loss: 0.141442
Policy train: iteration: 8500, policy_loss: 0.155500
Policy train: iteration: 9000, policy_loss: 0.139047

Background Trial: 1, reward: -538.6134622045774
Background Trial: 2, reward: -290.8080587410392
Background Trial: 3, reward: -101.66025404746807
Background Trial: 4, reward: -110.03652681220285
Background Trial: 5, reward: -259.8645315637269
Background Trial: 6, reward: -147.8248851172711
Background Trial: 7, reward: -76.64600330292515
Background Trial: 8, reward: -120.74155514104459
Background Trial: 9, reward: -21.531703512085926
Iteration: 29, average_reward: -185.3029978269268

Policy train: iteration: 500, policy_loss: 0.152954
Policy train: iteration: 1000, policy_loss: 0.161358
Policy train: iteration: 1500, policy_loss: 0.122801
Policy train: iteration: 2000, policy_loss: 0.164759
Policy train: iteration: 2500, policy_loss: 0.137733
Policy train: iteration: 3000, policy_loss: 0.180736
Policy train: iteration: 3500, policy_loss: 0.177533
Policy train: iteration: 4000, policy_loss: 0.169212
Policy train: iteration: 4500, policy_loss: 0.158850
Policy train: iteration: 5000, policy_loss: 0.148218
Policy train: iteration: 5500, policy_loss: 0.150443
Policy train: iteration: 6000, policy_loss: 0.140203
Policy train: iteration: 6500, policy_loss: 0.160033
Policy train: iteration: 7000, policy_loss: 0.159471
Policy train: iteration: 7500, policy_loss: 0.138472
Policy train: iteration: 8000, policy_loss: 0.123587
Policy train: iteration: 8500, policy_loss: 0.157755
Policy train: iteration: 9000, policy_loss: 0.146441

Background Trial: 1, reward: -35.50062967972973
Background Trial: 2, reward: -109.92332532246905
Background Trial: 3, reward: -45.29865189671088
Background Trial: 4, reward: -137.55376256477712
Background Trial: 5, reward: -97.03079333016798
Background Trial: 6, reward: -27.06134161464945
Background Trial: 7, reward: -97.47123922867884
Background Trial: 8, reward: -150.9634408956604
Background Trial: 9, reward: 3.7524974382132683
Iteration: 30, average_reward: -77.4500763438478

Policy train: iteration: 500, policy_loss: 0.168591
Policy train: iteration: 1000, policy_loss: 0.137060
Policy train: iteration: 1500, policy_loss: 0.156956
Policy train: iteration: 2000, policy_loss: 0.145010
Policy train: iteration: 2500, policy_loss: 0.202249
Policy train: iteration: 3000, policy_loss: 0.197380
Policy train: iteration: 3500, policy_loss: 0.158455
Policy train: iteration: 4000, policy_loss: 0.146971
Policy train: iteration: 4500, policy_loss: 0.135645
Policy train: iteration: 5000, policy_loss: 0.138645
Policy train: iteration: 5500, policy_loss: 0.181736
Policy train: iteration: 6000, policy_loss: 0.124382
Policy train: iteration: 6500, policy_loss: 0.175167
Policy train: iteration: 7000, policy_loss: 0.168802
Policy train: iteration: 7500, policy_loss: 0.155695
Policy train: iteration: 8000, policy_loss: 0.148475
Policy train: iteration: 8500, policy_loss: 0.154848
Policy train: iteration: 9000, policy_loss: 0.153145

Background Trial: 1, reward: -141.16999069168568
Background Trial: 2, reward: -419.65117013837306
Background Trial: 3, reward: -880.0921195528305
Background Trial: 4, reward: -99.76846001627078
Background Trial: 5, reward: -151.11706963607784
Background Trial: 6, reward: -71.46190650222594
Background Trial: 7, reward: -304.2316682959139
Background Trial: 8, reward: -82.73465516853133
Background Trial: 9, reward: -17.66252194574858
Iteration: 31, average_reward: -240.87661799418422

Policy train: iteration: 500, policy_loss: 0.149349
Policy train: iteration: 1000, policy_loss: 0.170330
Policy train: iteration: 1500, policy_loss: 0.144363
Policy train: iteration: 2000, policy_loss: 0.143774
Policy train: iteration: 2500, policy_loss: 0.155114
Policy train: iteration: 3000, policy_loss: 0.147252
Policy train: iteration: 3500, policy_loss: 0.180379
Policy train: iteration: 4000, policy_loss: 0.170387
Policy train: iteration: 4500, policy_loss: 0.159942
Policy train: iteration: 5000, policy_loss: 0.154245
Policy train: iteration: 5500, policy_loss: 0.119809
Policy train: iteration: 6000, policy_loss: 0.153445
Policy train: iteration: 6500, policy_loss: 0.134537
Policy train: iteration: 7000, policy_loss: 0.162188
Policy train: iteration: 7500, policy_loss: 0.158289
Policy train: iteration: 8000, policy_loss: 0.175014
Policy train: iteration: 8500, policy_loss: 0.148817
Policy train: iteration: 9000, policy_loss: 0.149866

Background Trial: 1, reward: 11.041927864876087
Background Trial: 2, reward: -63.836284898390495
Background Trial: 3, reward: -75.26881160308788
Background Trial: 4, reward: -566.1633767008777
Background Trial: 5, reward: -256.1660438145158
Background Trial: 6, reward: 282.5905412297641
Background Trial: 7, reward: -46.47916707475083
Background Trial: 8, reward: -213.6597759181232
Background Trial: 9, reward: -100.00190368372255
Iteration: 32, average_reward: -114.2158771776476

Policy train: iteration: 500, policy_loss: 0.146918
Policy train: iteration: 1000, policy_loss: 0.165195
Policy train: iteration: 1500, policy_loss: 0.155838
Policy train: iteration: 2000, policy_loss: 0.155169
Policy train: iteration: 2500, policy_loss: 0.151248
Policy train: iteration: 3000, policy_loss: 0.158178
Policy train: iteration: 3500, policy_loss: 0.181228
Policy train: iteration: 4000, policy_loss: 0.117820
Policy train: iteration: 4500, policy_loss: 0.136067
Policy train: iteration: 5000, policy_loss: 0.187690
Policy train: iteration: 5500, policy_loss: 0.153474
Policy train: iteration: 6000, policy_loss: 0.152947
Policy train: iteration: 6500, policy_loss: 0.154495
Policy train: iteration: 7000, policy_loss: 0.158475
Policy train: iteration: 7500, policy_loss: 0.154538
Policy train: iteration: 8000, policy_loss: 0.155798
Policy train: iteration: 8500, policy_loss: 0.136599
Policy train: iteration: 9000, policy_loss: 0.125186

Background Trial: 1, reward: 9.165573902010465
Background Trial: 2, reward: -9.128809384160718
Background Trial: 3, reward: -369.8239796618593
Background Trial: 4, reward: -35.161824937707124
Background Trial: 5, reward: 21.921430022716862
Background Trial: 6, reward: -81.07273113170476
Background Trial: 7, reward: -176.30110760038932
Background Trial: 8, reward: -18.494678214899125
Background Trial: 9, reward: -78.5251874411564
Iteration: 33, average_reward: -81.93570160523883

Policy train: iteration: 500, policy_loss: 0.137632
Policy train: iteration: 1000, policy_loss: 0.163446
Policy train: iteration: 1500, policy_loss: 0.144779
Policy train: iteration: 2000, policy_loss: 0.169164
Policy train: iteration: 2500, policy_loss: 0.164694
Policy train: iteration: 3000, policy_loss: 0.143534
Policy train: iteration: 3500, policy_loss: 0.144653
Policy train: iteration: 4000, policy_loss: 0.144609
Policy train: iteration: 4500, policy_loss: 0.134228
Policy train: iteration: 5000, policy_loss: 0.133661
Policy train: iteration: 5500, policy_loss: 0.185041
Policy train: iteration: 6000, policy_loss: 0.156210
Policy train: iteration: 6500, policy_loss: 0.150688
Policy train: iteration: 7000, policy_loss: 0.164441
Policy train: iteration: 7500, policy_loss: 0.195186
Policy train: iteration: 8000, policy_loss: 0.141898
Policy train: iteration: 8500, policy_loss: 0.161190
Policy train: iteration: 9000, policy_loss: 0.181523

Background Trial: 1, reward: 223.88430569184703
Background Trial: 2, reward: -136.33667460826086
Background Trial: 3, reward: 27.891190611569655
Background Trial: 4, reward: -103.03125336864596
Background Trial: 5, reward: -116.34530001544394
Background Trial: 6, reward: -129.01120898862425
Background Trial: 7, reward: -44.863142364737016
Background Trial: 8, reward: -265.4867198216082
Background Trial: 9, reward: -15.248972386256682
Iteration: 34, average_reward: -62.060863916684475

Policy train: iteration: 500, policy_loss: 0.176609
Policy train: iteration: 1000, policy_loss: 0.165917
Policy train: iteration: 1500, policy_loss: 0.127225
Policy train: iteration: 2000, policy_loss: 0.139181
Policy train: iteration: 2500, policy_loss: 0.162396
Policy train: iteration: 3000, policy_loss: 0.148509
Policy train: iteration: 3500, policy_loss: 0.144340
Policy train: iteration: 4000, policy_loss: 0.150286
Policy train: iteration: 4500, policy_loss: 0.135541
Policy train: iteration: 5000, policy_loss: 0.160538
Policy train: iteration: 5500, policy_loss: 0.140682
Policy train: iteration: 6000, policy_loss: 0.191536
Policy train: iteration: 6500, policy_loss: 0.165570
Policy train: iteration: 7000, policy_loss: 0.159583
Policy train: iteration: 7500, policy_loss: 0.167483
Policy train: iteration: 8000, policy_loss: 0.124568
Policy train: iteration: 8500, policy_loss: 0.222850
Policy train: iteration: 9000, policy_loss: 0.176268

Background Trial: 1, reward: -741.503741757629
Background Trial: 2, reward: -613.8200164788789
Background Trial: 3, reward: -31.425944116190948
Background Trial: 4, reward: -276.1229812628334
Background Trial: 5, reward: -30.810875129170114
Background Trial: 6, reward: -241.09660695886166
Background Trial: 7, reward: -48.57209063770718
Background Trial: 8, reward: -39.36500782168699
Background Trial: 9, reward: -185.32311441840395
Iteration: 35, average_reward: -245.33781984237353

Policy train: iteration: 500, policy_loss: 0.187010
Policy train: iteration: 1000, policy_loss: 0.128060
Policy train: iteration: 1500, policy_loss: 0.177341
Policy train: iteration: 2000, policy_loss: 0.209789
Policy train: iteration: 2500, policy_loss: 0.155210
Policy train: iteration: 3000, policy_loss: 0.129346
Policy train: iteration: 3500, policy_loss: 0.203640
Policy train: iteration: 4000, policy_loss: 0.144966
Policy train: iteration: 4500, policy_loss: 0.141463
Policy train: iteration: 5000, policy_loss: 0.158738
Policy train: iteration: 5500, policy_loss: 0.187280
Policy train: iteration: 6000, policy_loss: 0.166326
Policy train: iteration: 6500, policy_loss: 0.121218
Policy train: iteration: 7000, policy_loss: 0.155424
Policy train: iteration: 7500, policy_loss: 0.131011
Policy train: iteration: 8000, policy_loss: 0.153505
Policy train: iteration: 8500, policy_loss: 0.134591
Policy train: iteration: 9000, policy_loss: 0.151342

Background Trial: 1, reward: -9.936129661888543
Background Trial: 2, reward: -118.05478514161277
Background Trial: 3, reward: -126.27448306916726
Background Trial: 4, reward: -17.65009262709762
Background Trial: 5, reward: -97.9529350781167
Background Trial: 6, reward: -162.63799833570266
Background Trial: 7, reward: -220.16285719948513
Background Trial: 8, reward: -77.70355779690999
Background Trial: 9, reward: -107.27300884992366
Iteration: 36, average_reward: -104.18287197332269

Policy train: iteration: 500, policy_loss: 0.160099
Policy train: iteration: 1000, policy_loss: 0.169932
Policy train: iteration: 1500, policy_loss: 0.142864
Policy train: iteration: 2000, policy_loss: 0.140957
Policy train: iteration: 2500, policy_loss: 0.127975
Policy train: iteration: 3000, policy_loss: 0.126642
Policy train: iteration: 3500, policy_loss: 0.144017
Policy train: iteration: 4000, policy_loss: 0.152453
Policy train: iteration: 4500, policy_loss: 0.125206
Policy train: iteration: 5000, policy_loss: 0.137028
Policy train: iteration: 5500, policy_loss: 0.157789
Policy train: iteration: 6000, policy_loss: 0.140946
Policy train: iteration: 6500, policy_loss: 0.170498
Policy train: iteration: 7000, policy_loss: 0.175241
Policy train: iteration: 7500, policy_loss: 0.164468
Policy train: iteration: 8000, policy_loss: 0.172858
Policy train: iteration: 8500, policy_loss: 0.157626
Policy train: iteration: 9000, policy_loss: 0.139964

Background Trial: 1, reward: -47.849267142771446
Background Trial: 2, reward: -185.64940418396066
Background Trial: 3, reward: -228.05543385874458
Background Trial: 4, reward: -104.93928389127421
Background Trial: 5, reward: 5.480797359178098
Background Trial: 6, reward: -100.17162925659417
Background Trial: 7, reward: -128.6949305101677
Background Trial: 8, reward: -70.01174453738568
Background Trial: 9, reward: -48.33097700777912
Iteration: 37, average_reward: -100.91354144772215

Policy train: iteration: 500, policy_loss: 0.160567
Policy train: iteration: 1000, policy_loss: 0.159846
Policy train: iteration: 1500, policy_loss: 0.133898
Policy train: iteration: 2000, policy_loss: 0.152811
Policy train: iteration: 2500, policy_loss: 0.152226
Policy train: iteration: 3000, policy_loss: 0.123190
Policy train: iteration: 3500, policy_loss: 0.154560
Policy train: iteration: 4000, policy_loss: 0.143594
Policy train: iteration: 4500, policy_loss: 0.174212
Policy train: iteration: 5000, policy_loss: 0.165803
Policy train: iteration: 5500, policy_loss: 0.149151
Policy train: iteration: 6000, policy_loss: 0.148965
Policy train: iteration: 6500, policy_loss: 0.147074
Policy train: iteration: 7000, policy_loss: 0.144828
Policy train: iteration: 7500, policy_loss: 0.147360
Policy train: iteration: 8000, policy_loss: 0.128118
Policy train: iteration: 8500, policy_loss: 0.164307
Policy train: iteration: 9000, policy_loss: 0.151858

Background Trial: 1, reward: -34.87702871386598
Background Trial: 2, reward: 8.254386141332915
Background Trial: 3, reward: -31.32500848050249
Background Trial: 4, reward: -136.41329180646045
Background Trial: 5, reward: -175.02543094223157
Background Trial: 6, reward: -38.19401315785749
Background Trial: 7, reward: -43.489801895127144
Background Trial: 8, reward: 7.861282661221026
Background Trial: 9, reward: 249.63284507853567
Iteration: 38, average_reward: -21.50845123499506

Policy train: iteration: 500, policy_loss: 0.145348
Policy train: iteration: 1000, policy_loss: 0.170092
Policy train: iteration: 1500, policy_loss: 0.128750
Policy train: iteration: 2000, policy_loss: 0.147101
Policy train: iteration: 2500, policy_loss: 0.156182
Policy train: iteration: 3000, policy_loss: 0.127696
Policy train: iteration: 3500, policy_loss: 0.135262
Policy train: iteration: 4000, policy_loss: 0.197203
Policy train: iteration: 4500, policy_loss: 0.144487
Policy train: iteration: 5000, policy_loss: 0.137748
Policy train: iteration: 5500, policy_loss: 0.145841
Policy train: iteration: 6000, policy_loss: 0.128399
Policy train: iteration: 6500, policy_loss: 0.141837
Policy train: iteration: 7000, policy_loss: 0.126458
Policy train: iteration: 7500, policy_loss: 0.181376
Policy train: iteration: 8000, policy_loss: 0.177263
Policy train: iteration: 8500, policy_loss: 0.154986
Policy train: iteration: 9000, policy_loss: 0.206546

Background Trial: 1, reward: -5.8632951464391425
Background Trial: 2, reward: -156.4516278718732
Background Trial: 3, reward: -125.05060115566422
Background Trial: 4, reward: -44.36342911397009
Background Trial: 5, reward: -108.6985297057766
Background Trial: 6, reward: 270.5916258933098
Background Trial: 7, reward: 235.23274870315993
Background Trial: 8, reward: -104.77623359823164
Background Trial: 9, reward: -24.990238599143552
Iteration: 39, average_reward: -7.152175621625416

Policy train: iteration: 500, policy_loss: 0.154360
Policy train: iteration: 1000, policy_loss: 0.135604
Policy train: iteration: 1500, policy_loss: 0.126657
Policy train: iteration: 2000, policy_loss: 0.137810
Policy train: iteration: 2500, policy_loss: 0.210854
Policy train: iteration: 3000, policy_loss: 0.129168
Policy train: iteration: 3500, policy_loss: 0.148568
Policy train: iteration: 4000, policy_loss: 0.126520
Policy train: iteration: 4500, policy_loss: 0.153277
Policy train: iteration: 5000, policy_loss: 0.149187
Policy train: iteration: 5500, policy_loss: 0.160930
Policy train: iteration: 6000, policy_loss: 0.134907
Policy train: iteration: 6500, policy_loss: 0.119487
Policy train: iteration: 7000, policy_loss: 0.145207
Policy train: iteration: 7500, policy_loss: 0.141798
Policy train: iteration: 8000, policy_loss: 0.156953
Policy train: iteration: 8500, policy_loss: 0.153850
Policy train: iteration: 9000, policy_loss: 0.198581

Background Trial: 1, reward: -48.654637729647845
Background Trial: 2, reward: -64.39467647714343
Background Trial: 3, reward: -79.0545274461804
Background Trial: 4, reward: 11.019445137014827
Background Trial: 5, reward: -107.67727164621337
Background Trial: 6, reward: -124.1805171343945
Background Trial: 7, reward: -288.60889295792026
Background Trial: 8, reward: -117.15158014530999
Background Trial: 9, reward: -27.48417133517856
Iteration: 40, average_reward: -94.0207588594415

Policy train: iteration: 500, policy_loss: 0.143923
Policy train: iteration: 1000, policy_loss: 0.145763
Policy train: iteration: 1500, policy_loss: 0.132418
Policy train: iteration: 2000, policy_loss: 0.144856
Policy train: iteration: 2500, policy_loss: 0.133527
Policy train: iteration: 3000, policy_loss: 0.164562
Policy train: iteration: 3500, policy_loss: 0.133327
Policy train: iteration: 4000, policy_loss: 0.133665
Policy train: iteration: 4500, policy_loss: 0.123109
Policy train: iteration: 5000, policy_loss: 0.137626
Policy train: iteration: 5500, policy_loss: 0.135182
Policy train: iteration: 6000, policy_loss: 0.149914
Policy train: iteration: 6500, policy_loss: 0.158937
Policy train: iteration: 7000, policy_loss: 0.171075
Policy train: iteration: 7500, policy_loss: 0.133726
Policy train: iteration: 8000, policy_loss: 0.111090
Policy train: iteration: 8500, policy_loss: 0.153356
Policy train: iteration: 9000, policy_loss: 0.126596

Background Trial: 1, reward: -10.312655005037584
Background Trial: 2, reward: -20.336604556197486
Background Trial: 3, reward: -32.18582898976847
Background Trial: 4, reward: -488.56507009535574
Background Trial: 5, reward: -24.77194216682031
Background Trial: 6, reward: -730.0165294921743
Background Trial: 7, reward: -53.93233680965026
Background Trial: 8, reward: -29.634873437901902
Background Trial: 9, reward: -186.13015363855823
Iteration: 41, average_reward: -175.09844379905158

Policy train: iteration: 500, policy_loss: 0.133681
Policy train: iteration: 1000, policy_loss: 0.147916
Policy train: iteration: 1500, policy_loss: 0.133960
Policy train: iteration: 2000, policy_loss: 0.145068
Policy train: iteration: 2500, policy_loss: 0.133814
Policy train: iteration: 3000, policy_loss: 0.127173
Policy train: iteration: 3500, policy_loss: 0.133339
Policy train: iteration: 4000, policy_loss: 0.136290
Policy train: iteration: 4500, policy_loss: 0.148690
Policy train: iteration: 5000, policy_loss: 0.153346
Policy train: iteration: 5500, policy_loss: 0.158741
Policy train: iteration: 6000, policy_loss: 0.172891
Policy train: iteration: 6500, policy_loss: 0.143968
Policy train: iteration: 7000, policy_loss: 0.164336
Policy train: iteration: 7500, policy_loss: 0.132628
Policy train: iteration: 8000, policy_loss: 0.122872
Policy train: iteration: 8500, policy_loss: 0.139101
Policy train: iteration: 9000, policy_loss: 0.146840

Background Trial: 1, reward: -4.308417253441078
Background Trial: 2, reward: -147.22317318027754
Background Trial: 3, reward: -60.13262310648625
Background Trial: 4, reward: 226.46980387853694
Background Trial: 5, reward: -29.204351186464365
Background Trial: 6, reward: -177.99994095823473
Background Trial: 7, reward: -4.66340329361698
Background Trial: 8, reward: -5.550924180766472
Background Trial: 9, reward: -146.11697365431795
Iteration: 42, average_reward: -38.74777810389649

Policy train: iteration: 500, policy_loss: 0.134223
Policy train: iteration: 1000, policy_loss: 0.145378
Policy train: iteration: 1500, policy_loss: 0.162688
Policy train: iteration: 2000, policy_loss: 0.133966
Policy train: iteration: 2500, policy_loss: 0.142495
Policy train: iteration: 3000, policy_loss: 0.125020
Policy train: iteration: 3500, policy_loss: 0.120355
Policy train: iteration: 4000, policy_loss: 0.140566
Policy train: iteration: 4500, policy_loss: 0.150362
Policy train: iteration: 5000, policy_loss: 0.131584
Policy train: iteration: 5500, policy_loss: 0.157306
Policy train: iteration: 6000, policy_loss: 0.143627
Policy train: iteration: 6500, policy_loss: 0.136812
Policy train: iteration: 7000, policy_loss: 0.103470
Policy train: iteration: 7500, policy_loss: 0.135231
Policy train: iteration: 8000, policy_loss: 0.135815
Policy train: iteration: 8500, policy_loss: 0.158070
Policy train: iteration: 9000, policy_loss: 0.158801

Background Trial: 1, reward: 0.4819780716856741
Background Trial: 2, reward: 1.1571159004094085
Background Trial: 3, reward: -147.47715271951813
Background Trial: 4, reward: -158.67465840851256
Background Trial: 5, reward: -103.94343015311765
Background Trial: 6, reward: -125.4787491187906
Background Trial: 7, reward: -26.72261878935612
Background Trial: 8, reward: -2.3126082825762495
Background Trial: 9, reward: -335.8439140853776
Iteration: 43, average_reward: -99.86822639835042

Policy train: iteration: 500, policy_loss: 0.136171
Policy train: iteration: 1000, policy_loss: 0.118372
Policy train: iteration: 1500, policy_loss: 0.159046
Policy train: iteration: 2000, policy_loss: 0.132491
Policy train: iteration: 2500, policy_loss: 0.121680
Policy train: iteration: 3000, policy_loss: 0.131369
Policy train: iteration: 3500, policy_loss: 0.140579
Policy train: iteration: 4000, policy_loss: 0.105395
Policy train: iteration: 4500, policy_loss: 0.145685
Policy train: iteration: 5000, policy_loss: 0.146691
Policy train: iteration: 5500, policy_loss: 0.144660
Policy train: iteration: 6000, policy_loss: 0.137546
Policy train: iteration: 6500, policy_loss: 0.113885
Policy train: iteration: 7000, policy_loss: 0.151598
Policy train: iteration: 7500, policy_loss: 0.147387
Policy train: iteration: 8000, policy_loss: 0.144338
Policy train: iteration: 8500, policy_loss: 0.154713
Policy train: iteration: 9000, policy_loss: 0.121448

Background Trial: 1, reward: -108.31017235458103
Background Trial: 2, reward: -122.10720456693993
Background Trial: 3, reward: -156.8408037262235
Background Trial: 4, reward: -102.96387661094917
Background Trial: 5, reward: -132.7087194778092
Background Trial: 6, reward: 8.848195890199364
Background Trial: 7, reward: 22.109730703568715
Background Trial: 8, reward: -97.64600923907649
Background Trial: 9, reward: -86.41888791339944
Iteration: 44, average_reward: -86.22641636613452

Policy train: iteration: 500, policy_loss: 0.125281
Policy train: iteration: 1000, policy_loss: 0.152917
Policy train: iteration: 1500, policy_loss: 0.127758
Policy train: iteration: 2000, policy_loss: 0.169867
Policy train: iteration: 2500, policy_loss: 0.152045
Policy train: iteration: 3000, policy_loss: 0.110690
Policy train: iteration: 3500, policy_loss: 0.140638
Policy train: iteration: 4000, policy_loss: 0.156917
Policy train: iteration: 4500, policy_loss: 0.116008
Policy train: iteration: 5000, policy_loss: 0.123680
Policy train: iteration: 5500, policy_loss: 0.123829
Policy train: iteration: 6000, policy_loss: 0.139812
Policy train: iteration: 6500, policy_loss: 0.120352
Policy train: iteration: 7000, policy_loss: 0.140741
Policy train: iteration: 7500, policy_loss: 0.127625
Policy train: iteration: 8000, policy_loss: 0.147621
Policy train: iteration: 8500, policy_loss: 0.132133
Policy train: iteration: 9000, policy_loss: 0.153827

Background Trial: 1, reward: 13.142449105242093
Background Trial: 2, reward: -306.18978035630107
Background Trial: 3, reward: -18.884647383672643
Background Trial: 4, reward: -48.0136184498838
Background Trial: 5, reward: -29.732228186404114
Background Trial: 6, reward: -47.351841503442955
Background Trial: 7, reward: -5.233953110082979
Background Trial: 8, reward: -514.959014681988
Background Trial: 9, reward: -51.88453069234211
Iteration: 45, average_reward: -112.1230183620973

Policy train: iteration: 500, policy_loss: 0.141396
Policy train: iteration: 1000, policy_loss: 0.128657
Policy train: iteration: 1500, policy_loss: 0.121189
Policy train: iteration: 2000, policy_loss: 0.175053
Policy train: iteration: 2500, policy_loss: 0.139286
Policy train: iteration: 3000, policy_loss: 0.150941
Policy train: iteration: 3500, policy_loss: 0.181537
Policy train: iteration: 4000, policy_loss: 0.116610
Policy train: iteration: 4500, policy_loss: 0.153592
Policy train: iteration: 5000, policy_loss: 0.155242
Policy train: iteration: 5500, policy_loss: 0.142385
Policy train: iteration: 6000, policy_loss: 0.150456
Policy train: iteration: 6500, policy_loss: 0.146336
Policy train: iteration: 7000, policy_loss: 0.123161
Policy train: iteration: 7500, policy_loss: 0.144821
Policy train: iteration: 8000, policy_loss: 0.112774
Policy train: iteration: 8500, policy_loss: 0.155931
Policy train: iteration: 9000, policy_loss: 0.154954

Background Trial: 1, reward: -21.51233255343986
Background Trial: 2, reward: -6.391034425828437
Background Trial: 3, reward: -19.694252683842222
Background Trial: 4, reward: -80.32757810520434
Background Trial: 5, reward: -112.68712965618943
Background Trial: 6, reward: -247.73537003233196
Background Trial: 7, reward: -5.698638545313003
Background Trial: 8, reward: -174.6741492027705
Background Trial: 9, reward: 33.7389516135118
Iteration: 46, average_reward: -70.55350373237866

Policy train: iteration: 500, policy_loss: 0.134340
Policy train: iteration: 1000, policy_loss: 0.136655
Policy train: iteration: 1500, policy_loss: 0.126755
Policy train: iteration: 2000, policy_loss: 0.133969
Policy train: iteration: 2500, policy_loss: 0.155864
Policy train: iteration: 3000, policy_loss: 0.124206
Policy train: iteration: 3500, policy_loss: 0.148029
Policy train: iteration: 4000, policy_loss: 0.142145
Policy train: iteration: 4500, policy_loss: 0.153528
Policy train: iteration: 5000, policy_loss: 0.138701
Policy train: iteration: 5500, policy_loss: 0.125803
Policy train: iteration: 6000, policy_loss: 0.197965
Policy train: iteration: 6500, policy_loss: 0.131985
Policy train: iteration: 7000, policy_loss: 0.143068
Policy train: iteration: 7500, policy_loss: 0.112935
Policy train: iteration: 8000, policy_loss: 0.145053
Policy train: iteration: 8500, policy_loss: 0.156942
Policy train: iteration: 9000, policy_loss: 0.154730

Background Trial: 1, reward: 211.6926570165528
Background Trial: 2, reward: -124.61349097861478
Background Trial: 3, reward: -3.424832955897571
Background Trial: 4, reward: -67.12671918826675
Background Trial: 5, reward: -48.87641788596111
Background Trial: 6, reward: -92.22198945817067
Background Trial: 7, reward: -80.92556651383555
Background Trial: 8, reward: -88.46737432439005
Background Trial: 9, reward: -92.17653688746952
Iteration: 47, average_reward: -42.90447457511702

Policy train: iteration: 500, policy_loss: 0.154866
Policy train: iteration: 1000, policy_loss: 0.143488
Policy train: iteration: 1500, policy_loss: 0.142838
Policy train: iteration: 2000, policy_loss: 0.156016
Policy train: iteration: 2500, policy_loss: 0.155834
Policy train: iteration: 3000, policy_loss: 0.127265
Policy train: iteration: 3500, policy_loss: 0.153068
Policy train: iteration: 4000, policy_loss: 0.160669
Policy train: iteration: 4500, policy_loss: 0.184394
Policy train: iteration: 5000, policy_loss: 0.133304
Policy train: iteration: 5500, policy_loss: 0.123855
Policy train: iteration: 6000, policy_loss: 0.110470
Policy train: iteration: 6500, policy_loss: 0.115556
Policy train: iteration: 7000, policy_loss: 0.137342
Policy train: iteration: 7500, policy_loss: 0.140472
Policy train: iteration: 8000, policy_loss: 0.133593
Policy train: iteration: 8500, policy_loss: 0.163799
Policy train: iteration: 9000, policy_loss: 0.178092

Background Trial: 1, reward: -189.95916700889177
Background Trial: 2, reward: -5.532896924393981
Background Trial: 3, reward: -123.88863809505082
Background Trial: 4, reward: -27.60519700470782
Background Trial: 5, reward: -83.72204489080383
Background Trial: 6, reward: -73.48069739088248
Background Trial: 7, reward: -83.20714348761845
Background Trial: 8, reward: -50.348054571102544
Background Trial: 9, reward: -122.5719527972857
Iteration: 48, average_reward: -84.47953246341528

Policy train: iteration: 500, policy_loss: 0.134239
Policy train: iteration: 1000, policy_loss: 0.157516
Policy train: iteration: 1500, policy_loss: 0.169343
Policy train: iteration: 2000, policy_loss: 0.181496
Policy train: iteration: 2500, policy_loss: 0.141636
Policy train: iteration: 3000, policy_loss: 0.135660
Policy train: iteration: 3500, policy_loss: 0.144967
Policy train: iteration: 4000, policy_loss: 0.125885
Policy train: iteration: 4500, policy_loss: 0.135322
Policy train: iteration: 5000, policy_loss: 0.112342
Policy train: iteration: 5500, policy_loss: 0.147204
Policy train: iteration: 6000, policy_loss: 0.123494
Policy train: iteration: 6500, policy_loss: 0.185585
Policy train: iteration: 7000, policy_loss: 0.134418
Policy train: iteration: 7500, policy_loss: 0.167939
Policy train: iteration: 8000, policy_loss: 0.116119
Policy train: iteration: 8500, policy_loss: 0.140858
Policy train: iteration: 9000, policy_loss: 0.138834

Background Trial: 1, reward: -111.09666592636866
Background Trial: 2, reward: -71.96525069401298
Background Trial: 3, reward: -38.79690885620546
Background Trial: 4, reward: -19.140881131384717
Background Trial: 5, reward: -51.385267815491204
Background Trial: 6, reward: -12.156399785403906
Background Trial: 7, reward: -26.713199136674277
Background Trial: 8, reward: -38.134560341544045
Background Trial: 9, reward: -649.5900482094968
Iteration: 49, average_reward: -113.21990909962022

Policy train: iteration: 500, policy_loss: 0.124901
Policy train: iteration: 1000, policy_loss: 0.131170
Policy train: iteration: 1500, policy_loss: 0.148391
Policy train: iteration: 2000, policy_loss: 0.145755
Policy train: iteration: 2500, policy_loss: 0.163696
Policy train: iteration: 3000, policy_loss: 0.132372
Policy train: iteration: 3500, policy_loss: 0.164543
Policy train: iteration: 4000, policy_loss: 0.155683
Policy train: iteration: 4500, policy_loss: 0.121170
Policy train: iteration: 5000, policy_loss: 0.127142
Policy train: iteration: 5500, policy_loss: 0.127910
Policy train: iteration: 6000, policy_loss: 0.136775
Policy train: iteration: 6500, policy_loss: 0.131758
Policy train: iteration: 7000, policy_loss: 0.118773
Policy train: iteration: 7500, policy_loss: 0.155913
Policy train: iteration: 8000, policy_loss: 0.119446
Policy train: iteration: 8500, policy_loss: 0.115820
Policy train: iteration: 9000, policy_loss: 0.145781

Background Trial: 1, reward: 215.81974040044759
Background Trial: 2, reward: -91.17525887646954
Background Trial: 3, reward: -55.72014847448941
Background Trial: 4, reward: -42.62434697963215
Background Trial: 5, reward: -118.20765248612031
Background Trial: 6, reward: -307.26285635291833
Background Trial: 7, reward: -37.75806421330948
Background Trial: 8, reward: -66.37582120323397
Background Trial: 9, reward: -22.68073226466356
Iteration: 50, average_reward: -58.442793383376575

Policy train: iteration: 500, policy_loss: 0.126973
Policy train: iteration: 1000, policy_loss: 0.137531
Policy train: iteration: 1500, policy_loss: 0.146078
Policy train: iteration: 2000, policy_loss: 0.171503
Policy train: iteration: 2500, policy_loss: 0.131824
Policy train: iteration: 3000, policy_loss: 0.133987
Policy train: iteration: 3500, policy_loss: 0.107997
Policy train: iteration: 4000, policy_loss: 0.148925
Policy train: iteration: 4500, policy_loss: 0.125570
Policy train: iteration: 5000, policy_loss: 0.147114
Policy train: iteration: 5500, policy_loss: 0.153172
Policy train: iteration: 6000, policy_loss: 0.139701
Policy train: iteration: 6500, policy_loss: 0.136126
Policy train: iteration: 7000, policy_loss: 0.125395
Policy train: iteration: 7500, policy_loss: 0.130356
Policy train: iteration: 8000, policy_loss: 0.128291
Policy train: iteration: 8500, policy_loss: 0.139544
Policy train: iteration: 9000, policy_loss: 0.123618

Background Trial: 1, reward: -100.39888812652181
Background Trial: 2, reward: -130.5727011413896
Background Trial: 3, reward: 37.22893449745109
Background Trial: 4, reward: -260.97875320821106
Background Trial: 5, reward: 7.457224179738418
Background Trial: 6, reward: -73.96367282357849
Background Trial: 7, reward: -678.5608164614082
Background Trial: 8, reward: 197.7299463209116
Background Trial: 9, reward: -47.303050358267186
Iteration: 51, average_reward: -116.595753013475

Policy train: iteration: 500, policy_loss: 0.128136
Policy train: iteration: 1000, policy_loss: 0.139592
Policy train: iteration: 1500, policy_loss: 0.138978
Policy train: iteration: 2000, policy_loss: 0.120824
Policy train: iteration: 2500, policy_loss: 0.144849
Policy train: iteration: 3000, policy_loss: 0.149877
Policy train: iteration: 3500, policy_loss: 0.102297
Policy train: iteration: 4000, policy_loss: 0.133383
Policy train: iteration: 4500, policy_loss: 0.127647
Policy train: iteration: 5000, policy_loss: 0.130357
Policy train: iteration: 5500, policy_loss: 0.136771
Policy train: iteration: 6000, policy_loss: 0.136569
Policy train: iteration: 6500, policy_loss: 0.128344
Policy train: iteration: 7000, policy_loss: 0.139181
Policy train: iteration: 7500, policy_loss: 0.154714
Policy train: iteration: 8000, policy_loss: 0.140641
Policy train: iteration: 8500, policy_loss: 0.141953
Policy train: iteration: 9000, policy_loss: 0.151896

Background Trial: 1, reward: 6.025476271252288
Background Trial: 2, reward: -413.0807117977523
Background Trial: 3, reward: -653.5364136026978
Background Trial: 4, reward: -348.2045978617095
Background Trial: 5, reward: -6.9133352400102694
Background Trial: 6, reward: -319.8882049539217
Background Trial: 7, reward: -1.301283200740741
Background Trial: 8, reward: -27.222852181233478
Background Trial: 9, reward: -107.71488321818394
Iteration: 52, average_reward: -207.98186730944414

Policy train: iteration: 500, policy_loss: 0.107452
Policy train: iteration: 1000, policy_loss: 0.142544
Policy train: iteration: 1500, policy_loss: 0.135583
Policy train: iteration: 2000, policy_loss: 0.134032
Policy train: iteration: 2500, policy_loss: 0.144126
Policy train: iteration: 3000, policy_loss: 0.169711
Policy train: iteration: 3500, policy_loss: 0.140193
Policy train: iteration: 4000, policy_loss: 0.130845
Policy train: iteration: 4500, policy_loss: 0.157075
Policy train: iteration: 5000, policy_loss: 0.115233
Policy train: iteration: 5500, policy_loss: 0.142266
Policy train: iteration: 6000, policy_loss: 0.130527
Policy train: iteration: 6500, policy_loss: 0.139560
Policy train: iteration: 7000, policy_loss: 0.148337
Policy train: iteration: 7500, policy_loss: 0.130316
Policy train: iteration: 8000, policy_loss: 0.143399
Policy train: iteration: 8500, policy_loss: 0.109704
Policy train: iteration: 9000, policy_loss: 0.116982

Background Trial: 1, reward: -34.17283348594847
Background Trial: 2, reward: -91.51981451881335
Background Trial: 3, reward: -175.14630771131706
Background Trial: 4, reward: -43.62144278318999
Background Trial: 5, reward: -43.78469615250863
Background Trial: 6, reward: -133.94452527654178
Background Trial: 7, reward: -93.09543213723353
Background Trial: 8, reward: -414.57757792424536
Background Trial: 9, reward: -190.66851475641818
Iteration: 53, average_reward: -135.61457163846848

Policy train: iteration: 500, policy_loss: 0.179709
Policy train: iteration: 1000, policy_loss: 0.121123
Policy train: iteration: 1500, policy_loss: 0.133468
Policy train: iteration: 2000, policy_loss: 0.149132
Policy train: iteration: 2500, policy_loss: 0.116281
Policy train: iteration: 3000, policy_loss: 0.142110
Policy train: iteration: 3500, policy_loss: 0.131968
Policy train: iteration: 4000, policy_loss: 0.142776
Policy train: iteration: 4500, policy_loss: 0.150813
Policy train: iteration: 5000, policy_loss: 0.140493
Policy train: iteration: 5500, policy_loss: 0.132614
Policy train: iteration: 6000, policy_loss: 0.152696
Policy train: iteration: 6500, policy_loss: 0.119920
Policy train: iteration: 7000, policy_loss: 0.134874
Policy train: iteration: 7500, policy_loss: 0.129044
Policy train: iteration: 8000, policy_loss: 0.135627
Policy train: iteration: 8500, policy_loss: 0.125730
Policy train: iteration: 9000, policy_loss: 0.125735

Background Trial: 1, reward: -24.673753923922433
Background Trial: 2, reward: -118.07176718970963
Background Trial: 3, reward: -14.892969997420693
Background Trial: 4, reward: -40.592581595076865
Background Trial: 5, reward: -113.99063818547405
Background Trial: 6, reward: -36.633465542351615
Background Trial: 7, reward: -234.68664041138453
Background Trial: 8, reward: -634.8583500916751
Background Trial: 9, reward: -223.61107073654796
Iteration: 54, average_reward: -160.2234708526181

Policy train: iteration: 500, policy_loss: 0.166192
Policy train: iteration: 1000, policy_loss: 0.118220
Policy train: iteration: 1500, policy_loss: 0.147047
Policy train: iteration: 2000, policy_loss: 0.132021
Policy train: iteration: 2500, policy_loss: 0.141849
Policy train: iteration: 3000, policy_loss: 0.135182
Policy train: iteration: 3500, policy_loss: 0.142842
Policy train: iteration: 4000, policy_loss: 0.140393
Policy train: iteration: 4500, policy_loss: 0.141521
Policy train: iteration: 5000, policy_loss: 0.113104
Policy train: iteration: 5500, policy_loss: 0.137421
Policy train: iteration: 6000, policy_loss: 0.111653
Policy train: iteration: 6500, policy_loss: 0.158645
Policy train: iteration: 7000, policy_loss: 0.154861
Policy train: iteration: 7500, policy_loss: 0.129171
Policy train: iteration: 8000, policy_loss: 0.143442
Policy train: iteration: 8500, policy_loss: 0.119170
Policy train: iteration: 9000, policy_loss: 0.152599

Background Trial: 1, reward: -69.62438194304809
Background Trial: 2, reward: -102.02492114883066
Background Trial: 3, reward: -11.880409838350431
Background Trial: 4, reward: -31.00757181537159
Background Trial: 5, reward: -13.579462175299255
Background Trial: 6, reward: -124.08649765243283
Background Trial: 7, reward: -120.67431957265629
Background Trial: 8, reward: 38.88063985597353
Background Trial: 9, reward: -283.1484969455998
Iteration: 55, average_reward: -79.68282458173505

Policy train: iteration: 500, policy_loss: 0.129150
Policy train: iteration: 1000, policy_loss: 0.158713
Policy train: iteration: 1500, policy_loss: 0.135556
Policy train: iteration: 2000, policy_loss: 0.117494
Policy train: iteration: 2500, policy_loss: 0.115212
Policy train: iteration: 3000, policy_loss: 0.093967
Policy train: iteration: 3500, policy_loss: 0.171217
Policy train: iteration: 4000, policy_loss: 0.142330
Policy train: iteration: 4500, policy_loss: 0.106167
Policy train: iteration: 5000, policy_loss: 0.104674
Policy train: iteration: 5500, policy_loss: 0.144127
Policy train: iteration: 6000, policy_loss: 0.114083
Policy train: iteration: 6500, policy_loss: 0.115412
Policy train: iteration: 7000, policy_loss: 0.129365
Policy train: iteration: 7500, policy_loss: 0.157029
Policy train: iteration: 8000, policy_loss: 0.126310
Policy train: iteration: 8500, policy_loss: 0.168703
Policy train: iteration: 9000, policy_loss: 0.105682

Background Trial: 1, reward: -9.83973305299834
Background Trial: 2, reward: -73.60344949290935
Background Trial: 3, reward: -235.0258944537854
Background Trial: 4, reward: -34.99177964654271
Background Trial: 5, reward: 61.373100249337824
Background Trial: 6, reward: -51.94486471150236
Background Trial: 7, reward: 235.4161909845499
Background Trial: 8, reward: 18.139193630134642
Background Trial: 9, reward: -18.53408019699218
Iteration: 56, average_reward: -12.112368521189776

Policy train: iteration: 500, policy_loss: 0.133824
Policy train: iteration: 1000, policy_loss: 0.129242
Policy train: iteration: 1500, policy_loss: 0.141350
Policy train: iteration: 2000, policy_loss: 0.112166
Policy train: iteration: 2500, policy_loss: 0.101461
Policy train: iteration: 3000, policy_loss: 0.134184
Policy train: iteration: 3500, policy_loss: 0.153294
Policy train: iteration: 4000, policy_loss: 0.132582
Policy train: iteration: 4500, policy_loss: 0.157344
Policy train: iteration: 5000, policy_loss: 0.140863
Policy train: iteration: 5500, policy_loss: 0.142603
Policy train: iteration: 6000, policy_loss: 0.146856
Policy train: iteration: 6500, policy_loss: 0.133887
Policy train: iteration: 7000, policy_loss: 0.157650
Policy train: iteration: 7500, policy_loss: 0.121867
Policy train: iteration: 8000, policy_loss: 0.129321
Policy train: iteration: 8500, policy_loss: 0.131413
Policy train: iteration: 9000, policy_loss: 0.118410

Background Trial: 1, reward: 6.302743375435341
Background Trial: 2, reward: -454.1163684115872
Background Trial: 3, reward: -29.318293784599803
Background Trial: 4, reward: -197.11236156526252
Background Trial: 5, reward: -186.45965620872056
Background Trial: 6, reward: -304.62356049555626
Background Trial: 7, reward: -113.33111923869116
Background Trial: 8, reward: -207.5583954781786
Background Trial: 9, reward: -157.9280382799622
Iteration: 57, average_reward: -182.6827833430137

Policy train: iteration: 500, policy_loss: 0.142401
Policy train: iteration: 1000, policy_loss: 0.103465
Policy train: iteration: 1500, policy_loss: 0.123448
Policy train: iteration: 2000, policy_loss: 0.120780
Policy train: iteration: 2500, policy_loss: 0.107111
Policy train: iteration: 3000, policy_loss: 0.129543
Policy train: iteration: 3500, policy_loss: 0.146630
Policy train: iteration: 4000, policy_loss: 0.141374
Policy train: iteration: 4500, policy_loss: 0.139860
Policy train: iteration: 5000, policy_loss: 0.117511
Policy train: iteration: 5500, policy_loss: 0.114849
Policy train: iteration: 6000, policy_loss: 0.133806
Policy train: iteration: 6500, policy_loss: 0.109813
Policy train: iteration: 7000, policy_loss: 0.151127
Policy train: iteration: 7500, policy_loss: 0.137824
Policy train: iteration: 8000, policy_loss: 0.135357
Policy train: iteration: 8500, policy_loss: 0.164672
Policy train: iteration: 9000, policy_loss: 0.135406

Background Trial: 1, reward: -46.793754942362284
Background Trial: 2, reward: -34.88658961528269
Background Trial: 3, reward: -628.3007295654588
Background Trial: 4, reward: 33.046441101937916
Background Trial: 5, reward: -360.2569021940035
Background Trial: 6, reward: -46.959594245602574
Background Trial: 7, reward: 10.159971517413027
Background Trial: 8, reward: -74.56089908443425
Background Trial: 9, reward: -310.22699872011304
Iteration: 58, average_reward: -162.08656174976736

Policy train: iteration: 500, policy_loss: 0.108560
Policy train: iteration: 1000, policy_loss: 0.120058
Policy train: iteration: 1500, policy_loss: 0.107195
Policy train: iteration: 2000, policy_loss: 0.125822
Policy train: iteration: 2500, policy_loss: 0.122559
Policy train: iteration: 3000, policy_loss: 0.144597
Policy train: iteration: 3500, policy_loss: 0.110717
Policy train: iteration: 4000, policy_loss: 0.122918
Policy train: iteration: 4500, policy_loss: 0.142977
Policy train: iteration: 5000, policy_loss: 0.128310
Policy train: iteration: 5500, policy_loss: 0.104680
Policy train: iteration: 6000, policy_loss: 0.144619
Policy train: iteration: 6500, policy_loss: 0.130029
Policy train: iteration: 7000, policy_loss: 0.126423
Policy train: iteration: 7500, policy_loss: 0.120635
Policy train: iteration: 8000, policy_loss: 0.124469
Policy train: iteration: 8500, policy_loss: 0.114904
Policy train: iteration: 9000, policy_loss: 0.103377

Background Trial: 1, reward: -33.368680539437065
Background Trial: 2, reward: -65.80148606593717
Background Trial: 3, reward: -35.999300173233806
Background Trial: 4, reward: -207.47718572773454
Background Trial: 5, reward: 11.987815125740553
Background Trial: 6, reward: -203.68988208376146
Background Trial: 7, reward: -109.71154101102367
Background Trial: 8, reward: -55.329987506508246
Background Trial: 9, reward: -222.46630911207552
Iteration: 59, average_reward: -102.42850634377454

Policy train: iteration: 500, policy_loss: 0.103115
Policy train: iteration: 1000, policy_loss: 0.092840
Policy train: iteration: 1500, policy_loss: 0.187277
Policy train: iteration: 2000, policy_loss: 0.133331
Policy train: iteration: 2500, policy_loss: 0.122125
Policy train: iteration: 3000, policy_loss: 0.149758
Policy train: iteration: 3500, policy_loss: 0.125924
Policy train: iteration: 4000, policy_loss: 0.128910
Policy train: iteration: 4500, policy_loss: 0.160938
Policy train: iteration: 5000, policy_loss: 0.151880
Policy train: iteration: 5500, policy_loss: 0.125075
Policy train: iteration: 6000, policy_loss: 0.102728
Policy train: iteration: 6500, policy_loss: 0.130729
Policy train: iteration: 7000, policy_loss: 0.126176
Policy train: iteration: 7500, policy_loss: 0.143993
Policy train: iteration: 8000, policy_loss: 0.118823
Policy train: iteration: 8500, policy_loss: 0.162725
Policy train: iteration: 9000, policy_loss: 0.105726

Background Trial: 1, reward: 259.2118400061177
Background Trial: 2, reward: -306.74305760120336
Background Trial: 3, reward: -225.52241922120513
Background Trial: 4, reward: -40.72219286329269
Background Trial: 5, reward: -40.7964983275797
Background Trial: 6, reward: -110.12975561085393
Background Trial: 7, reward: 12.584081943342582
Background Trial: 8, reward: -5.864143973011778
Background Trial: 9, reward: -43.616000976229905
Iteration: 60, average_reward: -55.733127402657345

Policy train: iteration: 500, policy_loss: 0.132312
Policy train: iteration: 1000, policy_loss: 0.125359
Policy train: iteration: 1500, policy_loss: 0.132090
Policy train: iteration: 2000, policy_loss: 0.137728
Policy train: iteration: 2500, policy_loss: 0.122703
Policy train: iteration: 3000, policy_loss: 0.127361
Policy train: iteration: 3500, policy_loss: 0.124595
Policy train: iteration: 4000, policy_loss: 0.128763
Policy train: iteration: 4500, policy_loss: 0.135342
Policy train: iteration: 5000, policy_loss: 0.113037
Policy train: iteration: 5500, policy_loss: 0.109679
Policy train: iteration: 6000, policy_loss: 0.123607
Policy train: iteration: 6500, policy_loss: 0.143084
Policy train: iteration: 7000, policy_loss: 0.126281
Policy train: iteration: 7500, policy_loss: 0.086766
Policy train: iteration: 8000, policy_loss: 0.111938
Policy train: iteration: 8500, policy_loss: 0.136110
Policy train: iteration: 9000, policy_loss: 0.131221

Background Trial: 1, reward: -84.45230364045992
Background Trial: 2, reward: -391.58289272841654
Background Trial: 3, reward: -81.64478733409905
Background Trial: 4, reward: -149.42228753443433
Background Trial: 5, reward: -399.6655736761745
Background Trial: 6, reward: -62.32949936448281
Background Trial: 7, reward: -4.656468532211946
Background Trial: 8, reward: -441.09098262790116
Background Trial: 9, reward: -68.25406670694524
Iteration: 61, average_reward: -187.0109846827917

Policy train: iteration: 500, policy_loss: 0.115411
Policy train: iteration: 1000, policy_loss: 0.119905
Policy train: iteration: 1500, policy_loss: 0.133381
Policy train: iteration: 2000, policy_loss: 0.119619
Policy train: iteration: 2500, policy_loss: 0.111370
Policy train: iteration: 3000, policy_loss: 0.129835
Policy train: iteration: 3500, policy_loss: 0.137932
Policy train: iteration: 4000, policy_loss: 0.149715
Policy train: iteration: 4500, policy_loss: 0.163085
Policy train: iteration: 5000, policy_loss: 0.136611
Policy train: iteration: 5500, policy_loss: 0.159370
Policy train: iteration: 6000, policy_loss: 0.123832
Policy train: iteration: 6500, policy_loss: 0.121875
Policy train: iteration: 7000, policy_loss: 0.107582
Policy train: iteration: 7500, policy_loss: 0.132372
Policy train: iteration: 8000, policy_loss: 0.144334
Policy train: iteration: 8500, policy_loss: 0.150410
Policy train: iteration: 9000, policy_loss: 0.130510

Background Trial: 1, reward: -173.71976077171607
Background Trial: 2, reward: -9.22939920118165
Background Trial: 3, reward: -482.3150796854659
Background Trial: 4, reward: -138.92588302593956
Background Trial: 5, reward: -223.8957348395572
Background Trial: 6, reward: -50.26308685707525
Background Trial: 7, reward: -6.807702295112904
Background Trial: 8, reward: 1.4726235389096587
Background Trial: 9, reward: -49.939680762505574
Iteration: 62, average_reward: -125.9581893221827

Policy train: iteration: 500, policy_loss: 0.133585
Policy train: iteration: 1000, policy_loss: 0.128129
Policy train: iteration: 1500, policy_loss: 0.129875
Policy train: iteration: 2000, policy_loss: 0.109516
Policy train: iteration: 2500, policy_loss: 0.138393
Policy train: iteration: 3000, policy_loss: 0.133063
Policy train: iteration: 3500, policy_loss: 0.142148
Policy train: iteration: 4000, policy_loss: 0.104573
Policy train: iteration: 4500, policy_loss: 0.143696
Policy train: iteration: 5000, policy_loss: 0.138721
Policy train: iteration: 5500, policy_loss: 0.095018
Policy train: iteration: 6000, policy_loss: 0.108705
Policy train: iteration: 6500, policy_loss: 0.122278
Policy train: iteration: 7000, policy_loss: 0.108650
Policy train: iteration: 7500, policy_loss: 0.120281
Policy train: iteration: 8000, policy_loss: 0.189888
Policy train: iteration: 8500, policy_loss: 0.117118
Policy train: iteration: 9000, policy_loss: 0.154642

Background Trial: 1, reward: 263.64347034582397
Background Trial: 2, reward: 20.302925100913143
Background Trial: 3, reward: -94.87462505639027
Background Trial: 4, reward: -211.86728825000628
Background Trial: 5, reward: 7.489755410723518
Background Trial: 6, reward: -40.247367094992484
Background Trial: 7, reward: 273.75541275090416
Background Trial: 8, reward: -20.768027056275017
Background Trial: 9, reward: -96.24169225147038
Iteration: 63, average_reward: 11.243618211025598

Policy train: iteration: 500, policy_loss: 0.117076
Policy train: iteration: 1000, policy_loss: 0.123900
Policy train: iteration: 1500, policy_loss: 0.125511
Policy train: iteration: 2000, policy_loss: 0.135392
Policy train: iteration: 2500, policy_loss: 0.091935
Policy train: iteration: 3000, policy_loss: 0.116720
Policy train: iteration: 3500, policy_loss: 0.186222
Policy train: iteration: 4000, policy_loss: 0.122390
Policy train: iteration: 4500, policy_loss: 0.121586
Policy train: iteration: 5000, policy_loss: 0.138472
Policy train: iteration: 5500, policy_loss: 0.148367
Policy train: iteration: 6000, policy_loss: 0.137468
Policy train: iteration: 6500, policy_loss: 0.101139
Policy train: iteration: 7000, policy_loss: 0.140782
Policy train: iteration: 7500, policy_loss: 0.131164
Policy train: iteration: 8000, policy_loss: 0.117278
Policy train: iteration: 8500, policy_loss: 0.134028
Policy train: iteration: 9000, policy_loss: 0.121669

Background Trial: 1, reward: -678.5972400067657
Background Trial: 2, reward: -126.73251305295562
Background Trial: 3, reward: -414.8651493593135
Background Trial: 4, reward: -27.164562435783807
Background Trial: 5, reward: -25.74709932711218
Background Trial: 6, reward: -28.455460242203756
Background Trial: 7, reward: -574.0310733155643
Background Trial: 8, reward: -218.33028209652574
Background Trial: 9, reward: -38.020035423349626
Iteration: 64, average_reward: -236.88260169550827

Policy train: iteration: 500, policy_loss: 0.118026
Policy train: iteration: 1000, policy_loss: 0.120848
Policy train: iteration: 1500, policy_loss: 0.108624
Policy train: iteration: 2000, policy_loss: 0.119233
Policy train: iteration: 2500, policy_loss: 0.140549
Policy train: iteration: 3000, policy_loss: 0.126230
Policy train: iteration: 3500, policy_loss: 0.144211
Policy train: iteration: 4000, policy_loss: 0.116198
Policy train: iteration: 4500, policy_loss: 0.123726
Policy train: iteration: 5000, policy_loss: 0.115381
Policy train: iteration: 5500, policy_loss: 0.118865
Policy train: iteration: 6000, policy_loss: 0.157874
Policy train: iteration: 6500, policy_loss: 0.128183
Policy train: iteration: 7000, policy_loss: 0.099782
Policy train: iteration: 7500, policy_loss: 0.125226
Policy train: iteration: 8000, policy_loss: 0.142109
Policy train: iteration: 8500, policy_loss: 0.117557
Policy train: iteration: 9000, policy_loss: 0.136548

Background Trial: 1, reward: -51.76091823342905
Background Trial: 2, reward: -52.326711252898235
Background Trial: 3, reward: -80.4101930664417
Background Trial: 4, reward: -741.0978121939147
Background Trial: 5, reward: 14.402773219599197
Background Trial: 6, reward: 51.34403403812405
Background Trial: 7, reward: -381.3634144393279
Background Trial: 8, reward: -254.00825636550053
Background Trial: 9, reward: 237.75974856243553
Iteration: 65, average_reward: -139.71786108126147

Policy train: iteration: 500, policy_loss: 0.102010
Policy train: iteration: 1000, policy_loss: 0.121791
Policy train: iteration: 1500, policy_loss: 0.139682
Policy train: iteration: 2000, policy_loss: 0.108659
Policy train: iteration: 2500, policy_loss: 0.126550
Policy train: iteration: 3000, policy_loss: 0.115339
Policy train: iteration: 3500, policy_loss: 0.127296
Policy train: iteration: 4000, policy_loss: 0.129871
Policy train: iteration: 4500, policy_loss: 0.124775
Policy train: iteration: 5000, policy_loss: 0.104923
Policy train: iteration: 5500, policy_loss: 0.122405
Policy train: iteration: 6000, policy_loss: 0.105038
Policy train: iteration: 6500, policy_loss: 0.105259
Policy train: iteration: 7000, policy_loss: 0.154345
Policy train: iteration: 7500, policy_loss: 0.142708
Policy train: iteration: 8000, policy_loss: 0.116557
Policy train: iteration: 8500, policy_loss: 0.135076
Policy train: iteration: 9000, policy_loss: 0.131045

Background Trial: 1, reward: -247.5099355736686
Background Trial: 2, reward: -357.455281092761
Background Trial: 3, reward: -468.9739231250083
Background Trial: 4, reward: -203.3843241983337
Background Trial: 5, reward: -147.5867150650535
Background Trial: 6, reward: 8.658557413922665
Background Trial: 7, reward: -118.47218950541918
Background Trial: 8, reward: 2.767792286023081
Background Trial: 9, reward: -36.00732732551707
Iteration: 66, average_reward: -174.21814957620174

Policy train: iteration: 500, policy_loss: 0.146732
Policy train: iteration: 1000, policy_loss: 0.122884
Policy train: iteration: 1500, policy_loss: 0.117148
Policy train: iteration: 2000, policy_loss: 0.155500
Policy train: iteration: 2500, policy_loss: 0.100917
Policy train: iteration: 3000, policy_loss: 0.144188
Policy train: iteration: 3500, policy_loss: 0.127582
Policy train: iteration: 4000, policy_loss: 0.126599
Policy train: iteration: 4500, policy_loss: 0.136007
Policy train: iteration: 5000, policy_loss: 0.107047
Policy train: iteration: 5500, policy_loss: 0.128677
Policy train: iteration: 6000, policy_loss: 0.147789
Policy train: iteration: 6500, policy_loss: 0.107959
Policy train: iteration: 7000, policy_loss: 0.126528
Policy train: iteration: 7500, policy_loss: 0.120847
Policy train: iteration: 8000, policy_loss: 0.103758
Policy train: iteration: 8500, policy_loss: 0.182503
Policy train: iteration: 9000, policy_loss: 0.107675

Background Trial: 1, reward: -47.81819754170745
Background Trial: 2, reward: -275.5837501133516
Background Trial: 3, reward: -52.495954506355886
Background Trial: 4, reward: -126.55021735443394
Background Trial: 5, reward: -0.19971930033811702
Background Trial: 6, reward: -634.8187292144366
Background Trial: 7, reward: -31.026160501482522
Background Trial: 8, reward: 258.37626784043664
Background Trial: 9, reward: -636.2306211648552
Iteration: 67, average_reward: -171.81634242850274

Policy train: iteration: 500, policy_loss: 0.125596
Policy train: iteration: 1000, policy_loss: 0.119453
Policy train: iteration: 1500, policy_loss: 0.102708
Policy train: iteration: 2000, policy_loss: 0.123729
Policy train: iteration: 2500, policy_loss: 0.128893
Policy train: iteration: 3000, policy_loss: 0.118026
Policy train: iteration: 3500, policy_loss: 0.155030
Policy train: iteration: 4000, policy_loss: 0.136563
Policy train: iteration: 4500, policy_loss: 0.119708
Policy train: iteration: 5000, policy_loss: 0.141553
Policy train: iteration: 5500, policy_loss: 0.134146
Policy train: iteration: 6000, policy_loss: 0.135989
Policy train: iteration: 6500, policy_loss: 0.121375
Policy train: iteration: 7000, policy_loss: 0.142516
Policy train: iteration: 7500, policy_loss: 0.162728
Policy train: iteration: 8000, policy_loss: 0.134711
Policy train: iteration: 8500, policy_loss: 0.136119
Policy train: iteration: 9000, policy_loss: 0.111404

Background Trial: 1, reward: -357.16593196073404
Background Trial: 2, reward: -258.6995131012651
Background Trial: 3, reward: -723.4045963012434
Background Trial: 4, reward: -230.95371508003694
Background Trial: 5, reward: -670.3093381729701
Background Trial: 6, reward: -106.05619391092043
Background Trial: 7, reward: -117.57574308463647
Background Trial: 8, reward: -62.4348281781576
Background Trial: 9, reward: -70.99615119054803
Iteration: 68, average_reward: -288.62177899783467

Policy train: iteration: 500, policy_loss: 0.092670
Policy train: iteration: 1000, policy_loss: 0.091366
Policy train: iteration: 1500, policy_loss: 0.123213
Policy train: iteration: 2000, policy_loss: 0.116394
Policy train: iteration: 2500, policy_loss: 0.158770
Policy train: iteration: 3000, policy_loss: 0.148487
Policy train: iteration: 3500, policy_loss: 0.116914
Policy train: iteration: 4000, policy_loss: 0.164411
Policy train: iteration: 4500, policy_loss: 0.123812
Policy train: iteration: 5000, policy_loss: 0.145831
Policy train: iteration: 5500, policy_loss: 0.159982
Policy train: iteration: 6000, policy_loss: 0.149578
Policy train: iteration: 6500, policy_loss: 0.128258
Policy train: iteration: 7000, policy_loss: 0.152704
Policy train: iteration: 7500, policy_loss: 0.111134
Policy train: iteration: 8000, policy_loss: 0.108658
Policy train: iteration: 8500, policy_loss: 0.125311
Policy train: iteration: 9000, policy_loss: 0.100318

Background Trial: 1, reward: -47.93662694270028
Background Trial: 2, reward: -43.27500672528101
Background Trial: 3, reward: -65.47080054095812
Background Trial: 4, reward: -7.3806274896541595
Background Trial: 5, reward: -121.04781520477533
Background Trial: 6, reward: -273.1849155927551
Background Trial: 7, reward: -634.2549224293365
Background Trial: 8, reward: -42.27123135295183
Background Trial: 9, reward: -242.1662046049199
Iteration: 69, average_reward: -164.10979454259245

Policy train: iteration: 500, policy_loss: 0.117068
Policy train: iteration: 1000, policy_loss: 0.110134
Policy train: iteration: 1500, policy_loss: 0.097168
Policy train: iteration: 2000, policy_loss: 0.127210
Policy train: iteration: 2500, policy_loss: 0.131715
Policy train: iteration: 3000, policy_loss: 0.125984
Policy train: iteration: 3500, policy_loss: 0.119640
Policy train: iteration: 4000, policy_loss: 0.132545
Policy train: iteration: 4500, policy_loss: 0.139331
Policy train: iteration: 5000, policy_loss: 0.111423
Policy train: iteration: 5500, policy_loss: 0.136462
Policy train: iteration: 6000, policy_loss: 0.117131
Policy train: iteration: 6500, policy_loss: 0.116766
Policy train: iteration: 7000, policy_loss: 0.132258
Policy train: iteration: 7500, policy_loss: 0.132730
Policy train: iteration: 8000, policy_loss: 0.114224
Policy train: iteration: 8500, policy_loss: 0.111187
Policy train: iteration: 9000, policy_loss: 0.108174

Background Trial: 1, reward: -124.52822978180859
Background Trial: 2, reward: -332.68273753316356
Background Trial: 3, reward: 9.011363413040172
Background Trial: 4, reward: -32.02860063793359
Background Trial: 5, reward: -390.8550205428507
Background Trial: 6, reward: 35.05972421028335
Background Trial: 7, reward: -88.91842687211545
Background Trial: 8, reward: -1.953503792846547
Background Trial: 9, reward: -104.35775315133299
Iteration: 70, average_reward: -114.58368718763644

Policy train: iteration: 500, policy_loss: 0.146570
Policy train: iteration: 1000, policy_loss: 0.145332
Policy train: iteration: 1500, policy_loss: 0.146549
Policy train: iteration: 2000, policy_loss: 0.117745
Policy train: iteration: 2500, policy_loss: 0.149345
Policy train: iteration: 3000, policy_loss: 0.130922
Policy train: iteration: 3500, policy_loss: 0.119589
Policy train: iteration: 4000, policy_loss: 0.111113
Policy train: iteration: 4500, policy_loss: 0.121984
Policy train: iteration: 5000, policy_loss: 0.119912
Policy train: iteration: 5500, policy_loss: 0.144043
Policy train: iteration: 6000, policy_loss: 0.131654
Policy train: iteration: 6500, policy_loss: 0.116367
Policy train: iteration: 7000, policy_loss: 0.123693
Policy train: iteration: 7500, policy_loss: 0.120607
Policy train: iteration: 8000, policy_loss: 0.158880
Policy train: iteration: 8500, policy_loss: 0.110934
Policy train: iteration: 9000, policy_loss: 0.117249

Background Trial: 1, reward: -244.87951672051537
Background Trial: 2, reward: -11.713946026364795
Background Trial: 3, reward: -19.73286743366009
Background Trial: 4, reward: -5.611162864067055
Background Trial: 5, reward: -557.8799598148362
Background Trial: 6, reward: -62.96754570136187
Background Trial: 7, reward: -240.86977489674058
Background Trial: 8, reward: 237.04582191162189
Background Trial: 9, reward: -415.37212099274234
Iteration: 71, average_reward: -146.88678583762962

Policy train: iteration: 500, policy_loss: 0.132032
Policy train: iteration: 1000, policy_loss: 0.107628
Policy train: iteration: 1500, policy_loss: 0.148808
Policy train: iteration: 2000, policy_loss: 0.109941
Policy train: iteration: 2500, policy_loss: 0.121306
Policy train: iteration: 3000, policy_loss: 0.136286
Policy train: iteration: 3500, policy_loss: 0.106408
Policy train: iteration: 4000, policy_loss: 0.108480
Policy train: iteration: 4500, policy_loss: 0.116153
Policy train: iteration: 5000, policy_loss: 0.131827
Policy train: iteration: 5500, policy_loss: 0.139319
Policy train: iteration: 6000, policy_loss: 0.123348
Policy train: iteration: 6500, policy_loss: 0.148393
Policy train: iteration: 7000, policy_loss: 0.122616
Policy train: iteration: 7500, policy_loss: 0.111552
Policy train: iteration: 8000, policy_loss: 0.120113
Policy train: iteration: 8500, policy_loss: 0.123095
Policy train: iteration: 9000, policy_loss: 0.102021

Background Trial: 1, reward: 19.266991004199028
Background Trial: 2, reward: 22.895212282372157
Background Trial: 3, reward: 6.057148930191843
Background Trial: 4, reward: 302.8614004990926
Background Trial: 5, reward: -366.3405962008495
Background Trial: 6, reward: -34.70590271184061
Background Trial: 7, reward: -29.314440108550144
Background Trial: 8, reward: 10.067419420738958
Background Trial: 9, reward: -529.5365171726962
Iteration: 72, average_reward: -66.52769822859355

Policy train: iteration: 500, policy_loss: 0.100138
Policy train: iteration: 1000, policy_loss: 0.127749
Policy train: iteration: 1500, policy_loss: 0.140766
Policy train: iteration: 2000, policy_loss: 0.106434
Policy train: iteration: 2500, policy_loss: 0.158745
Policy train: iteration: 3000, policy_loss: 0.123907
Policy train: iteration: 3500, policy_loss: 0.123303
Policy train: iteration: 4000, policy_loss: 0.116407
Policy train: iteration: 4500, policy_loss: 0.121822
Policy train: iteration: 5000, policy_loss: 0.125574
Policy train: iteration: 5500, policy_loss: 0.104527
Policy train: iteration: 6000, policy_loss: 0.143627
Policy train: iteration: 6500, policy_loss: 0.112132
Policy train: iteration: 7000, policy_loss: 0.098914
Policy train: iteration: 7500, policy_loss: 0.125679
Policy train: iteration: 8000, policy_loss: 0.133520
Policy train: iteration: 8500, policy_loss: 0.134581
Policy train: iteration: 9000, policy_loss: 0.132847

Background Trial: 1, reward: -116.17630090257725
Background Trial: 2, reward: 8.618493007515056
Background Trial: 3, reward: -445.67324042381586
Background Trial: 4, reward: -239.24392808139157
Background Trial: 5, reward: -284.71842091277455
Background Trial: 6, reward: -0.11307764110412677
Background Trial: 7, reward: -19.488957031182153
Background Trial: 8, reward: -608.2167266787202
Background Trial: 9, reward: -97.77788998907977
Iteration: 73, average_reward: -200.31000540590338

Policy train: iteration: 500, policy_loss: 0.141327
Policy train: iteration: 1000, policy_loss: 0.138933
Policy train: iteration: 1500, policy_loss: 0.102727
Policy train: iteration: 2000, policy_loss: 0.170040
Policy train: iteration: 2500, policy_loss: 0.115191
Policy train: iteration: 3000, policy_loss: 0.118241
Policy train: iteration: 3500, policy_loss: 0.111062
Policy train: iteration: 4000, policy_loss: 0.122484
Policy train: iteration: 4500, policy_loss: 0.133382
Policy train: iteration: 5000, policy_loss: 0.132721
Policy train: iteration: 5500, policy_loss: 0.133532
Policy train: iteration: 6000, policy_loss: 0.092927
Policy train: iteration: 6500, policy_loss: 0.138336
Policy train: iteration: 7000, policy_loss: 0.114348
Policy train: iteration: 7500, policy_loss: 0.093523
Policy train: iteration: 8000, policy_loss: 0.106054
Policy train: iteration: 8500, policy_loss: 0.128546
Policy train: iteration: 9000, policy_loss: 0.105326

Background Trial: 1, reward: -85.97737493378115
Background Trial: 2, reward: -191.55915654985313
Background Trial: 3, reward: -87.17705752577405
Background Trial: 4, reward: -419.337520859296
Background Trial: 5, reward: -16.78092705634144
Background Trial: 6, reward: -39.835483280041665
Background Trial: 7, reward: 2.869590091977699
Background Trial: 8, reward: -51.30968433783211
Background Trial: 9, reward: 43.43019867329082
Iteration: 74, average_reward: -93.96415730862789

Policy train: iteration: 500, policy_loss: 0.120079
Policy train: iteration: 1000, policy_loss: 0.141621
Policy train: iteration: 1500, policy_loss: 0.112493
Policy train: iteration: 2000, policy_loss: 0.122428
Policy train: iteration: 2500, policy_loss: 0.117587
Policy train: iteration: 3000, policy_loss: 0.150353
Policy train: iteration: 3500, policy_loss: 0.118825
Policy train: iteration: 4000, policy_loss: 0.135114
Policy train: iteration: 4500, policy_loss: 0.122614
Policy train: iteration: 5000, policy_loss: 0.106817
Policy train: iteration: 5500, policy_loss: 0.142551
Policy train: iteration: 6000, policy_loss: 0.095479
Policy train: iteration: 6500, policy_loss: 0.112049
Policy train: iteration: 7000, policy_loss: 0.129864
Policy train: iteration: 7500, policy_loss: 0.101629
Policy train: iteration: 8000, policy_loss: 0.108526
Policy train: iteration: 8500, policy_loss: 0.134049
Policy train: iteration: 9000, policy_loss: 0.098357

Background Trial: 1, reward: -73.04928362650311
Background Trial: 2, reward: -701.8633163494078
Background Trial: 3, reward: -146.64513220458977
Background Trial: 4, reward: 12.280571810452997
Background Trial: 5, reward: 26.765193164046465
Background Trial: 6, reward: -17.43069890424384
Background Trial: 7, reward: -54.31457635251941
Background Trial: 8, reward: -622.1982768397721
Background Trial: 9, reward: -8.673222001709078
Iteration: 75, average_reward: -176.12541570047176

Policy train: iteration: 500, policy_loss: 0.117589
Policy train: iteration: 1000, policy_loss: 0.131469
Policy train: iteration: 1500, policy_loss: 0.128780
Policy train: iteration: 2000, policy_loss: 0.111325
Policy train: iteration: 2500, policy_loss: 0.151240
Policy train: iteration: 3000, policy_loss: 0.133133
Policy train: iteration: 3500, policy_loss: 0.085731
Policy train: iteration: 4000, policy_loss: 0.110500
Policy train: iteration: 4500, policy_loss: 0.119860
Policy train: iteration: 5000, policy_loss: 0.119180
Policy train: iteration: 5500, policy_loss: 0.129504
Policy train: iteration: 6000, policy_loss: 0.146110
Policy train: iteration: 6500, policy_loss: 0.165263
Policy train: iteration: 7000, policy_loss: 0.092999
Policy train: iteration: 7500, policy_loss: 0.123888
Policy train: iteration: 8000, policy_loss: 0.114632
Policy train: iteration: 8500, policy_loss: 0.121443
Policy train: iteration: 9000, policy_loss: 0.123941

Background Trial: 1, reward: -86.87132484368446
Background Trial: 2, reward: -136.0819669696597
Background Trial: 3, reward: -151.67409740279524
Background Trial: 4, reward: -31.322213340911432
Background Trial: 5, reward: 199.84872500249838
Background Trial: 6, reward: -692.6107546556801
Background Trial: 7, reward: -651.6725554109637
Background Trial: 8, reward: -54.154073405472786
Background Trial: 9, reward: -210.0184497233199
Iteration: 76, average_reward: -201.61741230555435

Policy train: iteration: 500, policy_loss: 0.127317
Policy train: iteration: 1000, policy_loss: 0.148939
Policy train: iteration: 1500, policy_loss: 0.115216
Policy train: iteration: 2000, policy_loss: 0.110120
Policy train: iteration: 2500, policy_loss: 0.128522
Policy train: iteration: 3000, policy_loss: 0.118285
Policy train: iteration: 3500, policy_loss: 0.119687
Policy train: iteration: 4000, policy_loss: 0.133609
Policy train: iteration: 4500, policy_loss: 0.132539
Policy train: iteration: 5000, policy_loss: 0.137310
Policy train: iteration: 5500, policy_loss: 0.107869
Policy train: iteration: 6000, policy_loss: 0.126047
Policy train: iteration: 6500, policy_loss: 0.120879
Policy train: iteration: 7000, policy_loss: 0.125365
Policy train: iteration: 7500, policy_loss: 0.112976
Policy train: iteration: 8000, policy_loss: 0.142035
Policy train: iteration: 8500, policy_loss: 0.116905
Policy train: iteration: 9000, policy_loss: 0.109183

Background Trial: 1, reward: 27.61920356806688
Background Trial: 2, reward: -19.507633219921388
Background Trial: 3, reward: -18.625206662878753
Background Trial: 4, reward: 38.66993035519073
Background Trial: 5, reward: 1.7932702882501559
Background Trial: 6, reward: -104.89856687477125
Background Trial: 7, reward: -579.7952772503332
Background Trial: 8, reward: -130.9854384010633
Background Trial: 9, reward: -597.4758603942278
Iteration: 77, average_reward: -153.68950873240976

Policy train: iteration: 500, policy_loss: 0.117054
Policy train: iteration: 1000, policy_loss: 0.139254
Policy train: iteration: 1500, policy_loss: 0.134845
Policy train: iteration: 2000, policy_loss: 0.098518
Policy train: iteration: 2500, policy_loss: 0.122158
Policy train: iteration: 3000, policy_loss: 0.125076
Policy train: iteration: 3500, policy_loss: 0.120637
Policy train: iteration: 4000, policy_loss: 0.127794
Policy train: iteration: 4500, policy_loss: 0.124920
Policy train: iteration: 5000, policy_loss: 0.118024
Policy train: iteration: 5500, policy_loss: 0.125816
Policy train: iteration: 6000, policy_loss: 0.113812
Policy train: iteration: 6500, policy_loss: 0.151121
Policy train: iteration: 7000, policy_loss: 0.094085
Policy train: iteration: 7500, policy_loss: 0.103641
Policy train: iteration: 8000, policy_loss: 0.117109
Policy train: iteration: 8500, policy_loss: 0.118384
Policy train: iteration: 9000, policy_loss: 0.122677

Background Trial: 1, reward: -214.005208778896
Background Trial: 2, reward: 237.35756806126355
Background Trial: 3, reward: -15.787023590075407
Background Trial: 4, reward: -727.7048346931897
Background Trial: 5, reward: -553.2763870761507
Background Trial: 6, reward: 1.2210592948226378
Background Trial: 7, reward: -183.9084302249547
Background Trial: 8, reward: -640.121770293795
Background Trial: 9, reward: -200.83522692360702
Iteration: 78, average_reward: -255.22891713606467

Policy train: iteration: 500, policy_loss: 0.185000
Policy train: iteration: 1000, policy_loss: 0.143856
Policy train: iteration: 1500, policy_loss: 0.100985
Policy train: iteration: 2000, policy_loss: 0.124052
Policy train: iteration: 2500, policy_loss: 0.121710
Policy train: iteration: 3000, policy_loss: 0.096496
Policy train: iteration: 3500, policy_loss: 0.114764
Policy train: iteration: 4000, policy_loss: 0.123636
Policy train: iteration: 4500, policy_loss: 0.131246
Policy train: iteration: 5000, policy_loss: 0.123195
Policy train: iteration: 5500, policy_loss: 0.102091
Policy train: iteration: 6000, policy_loss: 0.103175
Policy train: iteration: 6500, policy_loss: 0.124068
Policy train: iteration: 7000, policy_loss: 0.098824
Policy train: iteration: 7500, policy_loss: 0.136928
Policy train: iteration: 8000, policy_loss: 0.139087
Policy train: iteration: 8500, policy_loss: 0.116209
Policy train: iteration: 9000, policy_loss: 0.151229

Background Trial: 1, reward: -133.21504110756075
Background Trial: 2, reward: -36.78668531878067
Background Trial: 3, reward: 248.98191692871768
Background Trial: 4, reward: -86.75254658417637
Background Trial: 5, reward: -40.36360134790282
Background Trial: 6, reward: -104.38490674674418
Background Trial: 7, reward: -121.0991012673664
Background Trial: 8, reward: -8.097729691002996
Background Trial: 9, reward: -452.9000710513578
Iteration: 79, average_reward: -81.62419624290825

Policy train: iteration: 500, policy_loss: 0.121062
Policy train: iteration: 1000, policy_loss: 0.128572
Policy train: iteration: 1500, policy_loss: 0.112729
Policy train: iteration: 2000, policy_loss: 0.116878
Policy train: iteration: 2500, policy_loss: 0.121994
Policy train: iteration: 3000, policy_loss: 0.107122
Policy train: iteration: 3500, policy_loss: 0.126269
Policy train: iteration: 4000, policy_loss: 0.123296
Policy train: iteration: 4500, policy_loss: 0.100958
Policy train: iteration: 5000, policy_loss: 0.126021
Policy train: iteration: 5500, policy_loss: 0.086004
Policy train: iteration: 6000, policy_loss: 0.134124
Policy train: iteration: 6500, policy_loss: 0.134077
Policy train: iteration: 7000, policy_loss: 0.111596
Policy train: iteration: 7500, policy_loss: 0.122073
Policy train: iteration: 8000, policy_loss: 0.111036
Policy train: iteration: 8500, policy_loss: 0.124590
Policy train: iteration: 9000, policy_loss: 0.100459

Background Trial: 1, reward: -65.48970856084196
Background Trial: 2, reward: -29.412713332795732
Background Trial: 3, reward: -501.41618516908807
Background Trial: 4, reward: -104.25807494157745
Background Trial: 5, reward: 3.0128935699407577
Background Trial: 6, reward: -69.79070793334787
Background Trial: 7, reward: -686.709667873799
Background Trial: 8, reward: 6.39990243927511
Background Trial: 9, reward: 8.49132579028749
Iteration: 80, average_reward: -159.90810400132742

Policy train: iteration: 500, policy_loss: 0.085192
Policy train: iteration: 1000, policy_loss: 0.116814
Policy train: iteration: 1500, policy_loss: 0.117330
Policy train: iteration: 2000, policy_loss: 0.099007
Policy train: iteration: 2500, policy_loss: 0.105808
Policy train: iteration: 3000, policy_loss: 0.106221
Policy train: iteration: 3500, policy_loss: 0.109896
Policy train: iteration: 4000, policy_loss: 0.140321
Policy train: iteration: 4500, policy_loss: 0.107757
Policy train: iteration: 5000, policy_loss: 0.120691
Policy train: iteration: 5500, policy_loss: 0.132197
Policy train: iteration: 6000, policy_loss: 0.109005
Policy train: iteration: 6500, policy_loss: 0.120405
Policy train: iteration: 7000, policy_loss: 0.136444
Policy train: iteration: 7500, policy_loss: 0.125720
Policy train: iteration: 8000, policy_loss: 0.118613
Policy train: iteration: 8500, policy_loss: 0.114382
Policy train: iteration: 9000, policy_loss: 0.122196

Background Trial: 1, reward: -412.5116346237864
Background Trial: 2, reward: -109.17354601487027
Background Trial: 3, reward: -122.02696683953889
Background Trial: 4, reward: -477.81326953428413
Background Trial: 5, reward: -740.7366164214926
Background Trial: 6, reward: -20.13681980632184
Background Trial: 7, reward: -89.89624822524937
Background Trial: 8, reward: -336.6522633056589
Background Trial: 9, reward: -74.78027038193909
Iteration: 81, average_reward: -264.85862612812684

Policy train: iteration: 500, policy_loss: 0.092906
Policy train: iteration: 1000, policy_loss: 0.099496
Policy train: iteration: 1500, policy_loss: 0.109077
Policy train: iteration: 2000, policy_loss: 0.157683
Policy train: iteration: 2500, policy_loss: 0.141166
Policy train: iteration: 3000, policy_loss: 0.168067
Policy train: iteration: 3500, policy_loss: 0.123518
Policy train: iteration: 4000, policy_loss: 0.131793
Policy train: iteration: 4500, policy_loss: 0.131188
Policy train: iteration: 5000, policy_loss: 0.105601
Policy train: iteration: 5500, policy_loss: 0.120571
Policy train: iteration: 6000, policy_loss: 0.102658
Policy train: iteration: 6500, policy_loss: 0.129587
Policy train: iteration: 7000, policy_loss: 0.100809
Policy train: iteration: 7500, policy_loss: 0.119603
Policy train: iteration: 8000, policy_loss: 0.087846
Policy train: iteration: 8500, policy_loss: 0.129866
Policy train: iteration: 9000, policy_loss: 0.122697

Background Trial: 1, reward: -93.10690940695238
Background Trial: 2, reward: -40.88839582688682
Background Trial: 3, reward: 42.41594672334634
Background Trial: 4, reward: -106.44888586171751
Background Trial: 5, reward: -242.73207433655668
Background Trial: 6, reward: -256.43873390485146
Background Trial: 7, reward: -23.619233673686608
Background Trial: 8, reward: -707.2710142475679
Background Trial: 9, reward: -618.2001234782828
Iteration: 82, average_reward: -227.36549155701732

Policy train: iteration: 500, policy_loss: 0.128405
Policy train: iteration: 1000, policy_loss: 0.134385
Policy train: iteration: 1500, policy_loss: 0.104543
Policy train: iteration: 2000, policy_loss: 0.134354
Policy train: iteration: 2500, policy_loss: 0.098040
Policy train: iteration: 3000, policy_loss: 0.110502
Policy train: iteration: 3500, policy_loss: 0.110761
Policy train: iteration: 4000, policy_loss: 0.117143
Policy train: iteration: 4500, policy_loss: 0.099502
Policy train: iteration: 5000, policy_loss: 0.127188
Policy train: iteration: 5500, policy_loss: 0.128252
Policy train: iteration: 6000, policy_loss: 0.097442
Policy train: iteration: 6500, policy_loss: 0.153733
Policy train: iteration: 7000, policy_loss: 0.112515
Policy train: iteration: 7500, policy_loss: 0.106876
Policy train: iteration: 8000, policy_loss: 0.114619
Policy train: iteration: 8500, policy_loss: 0.099591
Policy train: iteration: 9000, policy_loss: 0.156711

Background Trial: 1, reward: -66.03552533301442
Background Trial: 2, reward: -66.90774485200483
Background Trial: 3, reward: -89.92735881960249
Background Trial: 4, reward: -95.51476618391841
Background Trial: 5, reward: 3.6694032422435754
Background Trial: 6, reward: -32.978291756468664
Background Trial: 7, reward: -101.36198021495886
Background Trial: 8, reward: -54.90911183370228
Background Trial: 9, reward: -330.3069325908548
Iteration: 83, average_reward: -92.69692314914235

Policy train: iteration: 500, policy_loss: 0.140117
Policy train: iteration: 1000, policy_loss: 0.122901
Policy train: iteration: 1500, policy_loss: 0.119299
Policy train: iteration: 2000, policy_loss: 0.147547
Policy train: iteration: 2500, policy_loss: 0.115782
Policy train: iteration: 3000, policy_loss: 0.184903
Policy train: iteration: 3500, policy_loss: 0.125092
Policy train: iteration: 4000, policy_loss: 0.126378
Policy train: iteration: 4500, policy_loss: 0.131290
Policy train: iteration: 5000, policy_loss: 0.112999
Policy train: iteration: 5500, policy_loss: 0.119561
Policy train: iteration: 6000, policy_loss: 0.114746
Policy train: iteration: 6500, policy_loss: 0.123716
Policy train: iteration: 7000, policy_loss: 0.137872
Policy train: iteration: 7500, policy_loss: 0.111363
Policy train: iteration: 8000, policy_loss: 0.119105
Policy train: iteration: 8500, policy_loss: 0.110913
Policy train: iteration: 9000, policy_loss: 0.098916

Background Trial: 1, reward: 6.500714767411793
Background Trial: 2, reward: -135.61665139139342
Background Trial: 3, reward: -154.62066022587953
Background Trial: 4, reward: -190.601228597086
Background Trial: 5, reward: -68.04894307879533
Background Trial: 6, reward: 4.979170410564649
Background Trial: 7, reward: 233.76840396875585
Background Trial: 8, reward: -11.955887553995936
Background Trial: 9, reward: -122.02092358983526
Iteration: 84, average_reward: -48.62400058780591

Policy train: iteration: 500, policy_loss: 0.129092
Policy train: iteration: 1000, policy_loss: 0.161025
Policy train: iteration: 1500, policy_loss: 0.121830
Policy train: iteration: 2000, policy_loss: 0.129982
Policy train: iteration: 2500, policy_loss: 0.115689
Policy train: iteration: 3000, policy_loss: 0.117324
Policy train: iteration: 3500, policy_loss: 0.089900
Policy train: iteration: 4000, policy_loss: 0.132370
Policy train: iteration: 4500, policy_loss: 0.127238
Policy train: iteration: 5000, policy_loss: 0.120306
Policy train: iteration: 5500, policy_loss: 0.129132
Policy train: iteration: 6000, policy_loss: 0.122343
Policy train: iteration: 6500, policy_loss: 0.101791
Policy train: iteration: 7000, policy_loss: 0.141586
Policy train: iteration: 7500, policy_loss: 0.091758
Policy train: iteration: 8000, policy_loss: 0.120378
Policy train: iteration: 8500, policy_loss: 0.120276
Policy train: iteration: 9000, policy_loss: 0.129350

Background Trial: 1, reward: -45.10408930651082
Background Trial: 2, reward: -75.88812043647013
Background Trial: 3, reward: -12.78277049542318
Background Trial: 4, reward: 26.391515343313046
Background Trial: 5, reward: -78.24471005208716
Background Trial: 6, reward: -73.46457273694386
Background Trial: 7, reward: -88.41020588945082
Background Trial: 8, reward: -73.17240642011672
Background Trial: 9, reward: -253.6887573752675
Iteration: 85, average_reward: -74.92934637432856

Policy train: iteration: 500, policy_loss: 0.128466
Policy train: iteration: 1000, policy_loss: 0.117419
Policy train: iteration: 1500, policy_loss: 0.153305
Policy train: iteration: 2000, policy_loss: 0.141641
Policy train: iteration: 2500, policy_loss: 0.092588
Policy train: iteration: 3000, policy_loss: 0.112217
Policy train: iteration: 3500, policy_loss: 0.112714
Policy train: iteration: 4000, policy_loss: 0.130943
Policy train: iteration: 4500, policy_loss: 0.100416
Policy train: iteration: 5000, policy_loss: 0.097959
Policy train: iteration: 5500, policy_loss: 0.139488
Policy train: iteration: 6000, policy_loss: 0.118422
Policy train: iteration: 6500, policy_loss: 0.123761
Policy train: iteration: 7000, policy_loss: 0.119698
Policy train: iteration: 7500, policy_loss: 0.110155
Policy train: iteration: 8000, policy_loss: 0.106571
Policy train: iteration: 8500, policy_loss: 0.105690
Policy train: iteration: 9000, policy_loss: 0.102101

Background Trial: 1, reward: -28.143481363185685
Background Trial: 2, reward: 22.772019373996358
Background Trial: 3, reward: -118.72787488575736
Background Trial: 4, reward: -66.11646808521589
Background Trial: 5, reward: -0.9757970517235606
Background Trial: 6, reward: -68.63723950773687
Background Trial: 7, reward: -101.99408209497238
Background Trial: 8, reward: -37.545792663961315
Background Trial: 9, reward: -642.6079979873772
Iteration: 86, average_reward: -115.77519047399265

Policy train: iteration: 500, policy_loss: 0.085570
Policy train: iteration: 1000, policy_loss: 0.112316
Policy train: iteration: 1500, policy_loss: 0.123934
Policy train: iteration: 2000, policy_loss: 0.142029
Policy train: iteration: 2500, policy_loss: 0.100365
Policy train: iteration: 3000, policy_loss: 0.129653
Policy train: iteration: 3500, policy_loss: 0.126622
Policy train: iteration: 4000, policy_loss: 0.105724
Policy train: iteration: 4500, policy_loss: 0.157795
Policy train: iteration: 5000, policy_loss: 0.119647
Policy train: iteration: 5500, policy_loss: 0.117089
Policy train: iteration: 6000, policy_loss: 0.111490
Policy train: iteration: 6500, policy_loss: 0.127342
Policy train: iteration: 7000, policy_loss: 0.136531
Policy train: iteration: 7500, policy_loss: 0.219556
Policy train: iteration: 8000, policy_loss: 0.113042
Policy train: iteration: 8500, policy_loss: 0.145775
Policy train: iteration: 9000, policy_loss: 0.110438

Background Trial: 1, reward: -58.8881470502085
Background Trial: 2, reward: -137.34203494698914
Background Trial: 3, reward: -75.33492173881308
Background Trial: 4, reward: -678.3615325117797
Background Trial: 5, reward: -1.0281226222430462
Background Trial: 6, reward: 9.854891545489792
Background Trial: 7, reward: -55.70356540942715
Background Trial: 8, reward: -592.7700075603891
Background Trial: 9, reward: -78.69967128965935
Iteration: 87, average_reward: -185.36367906489102

Policy train: iteration: 500, policy_loss: 0.120643
Policy train: iteration: 1000, policy_loss: 0.120612
Policy train: iteration: 1500, policy_loss: 0.122964
Policy train: iteration: 2000, policy_loss: 0.140629
Policy train: iteration: 2500, policy_loss: 0.178997
Policy train: iteration: 3000, policy_loss: 0.112411
Policy train: iteration: 3500, policy_loss: 0.133928
Policy train: iteration: 4000, policy_loss: 0.121503
Policy train: iteration: 4500, policy_loss: 0.117059
Policy train: iteration: 5000, policy_loss: 0.129423
Policy train: iteration: 5500, policy_loss: 0.110353
Policy train: iteration: 6000, policy_loss: 0.126694
Policy train: iteration: 6500, policy_loss: 0.133250
Policy train: iteration: 7000, policy_loss: 0.106311
Policy train: iteration: 7500, policy_loss: 0.105783
Policy train: iteration: 8000, policy_loss: 0.118109
Policy train: iteration: 8500, policy_loss: 0.140268
Policy train: iteration: 9000, policy_loss: 0.106281

Background Trial: 1, reward: -148.26220119276516
Background Trial: 2, reward: -82.62993818948013
Background Trial: 3, reward: -6.70747802787308
Background Trial: 4, reward: -51.400604771673834
Background Trial: 5, reward: -51.41890574702035
Background Trial: 6, reward: -285.5983340885756
Background Trial: 7, reward: -9.24312131129436
Background Trial: 8, reward: -55.96690205500333
Background Trial: 9, reward: -46.624520734165756
Iteration: 88, average_reward: -81.98355623531684

Policy train: iteration: 500, policy_loss: 0.115709
Policy train: iteration: 1000, policy_loss: 0.118022
Policy train: iteration: 1500, policy_loss: 0.121587
Policy train: iteration: 2000, policy_loss: 0.125166
Policy train: iteration: 2500, policy_loss: 0.144166
Policy train: iteration: 3000, policy_loss: 0.118017
Policy train: iteration: 3500, policy_loss: 0.117224
Policy train: iteration: 4000, policy_loss: 0.151668
Policy train: iteration: 4500, policy_loss: 0.114934
Policy train: iteration: 5000, policy_loss: 0.116980
Policy train: iteration: 5500, policy_loss: 0.130560
Policy train: iteration: 6000, policy_loss: 0.121161
Policy train: iteration: 6500, policy_loss: 0.138714
Policy train: iteration: 7000, policy_loss: 0.108650
Policy train: iteration: 7500, policy_loss: 0.122545
Policy train: iteration: 8000, policy_loss: 0.120290
Policy train: iteration: 8500, policy_loss: 0.162558
Policy train: iteration: 9000, policy_loss: 0.121935

Background Trial: 1, reward: -73.05667661012684
Background Trial: 2, reward: -700.8412387657186
Background Trial: 3, reward: -92.20757462806212
Background Trial: 4, reward: -706.5775198219156
Background Trial: 5, reward: -80.11941961136428
Background Trial: 6, reward: -79.16730806323721
Background Trial: 7, reward: -33.300361884274835
Background Trial: 8, reward: 31.229461215229946
Background Trial: 9, reward: 2.2651312967518606
Iteration: 89, average_reward: -192.4195007636353

Policy train: iteration: 500, policy_loss: 0.126646
Policy train: iteration: 1000, policy_loss: 0.121005
Policy train: iteration: 1500, policy_loss: 0.113014
Policy train: iteration: 2000, policy_loss: 0.133304
Policy train: iteration: 2500, policy_loss: 0.121049
Policy train: iteration: 3000, policy_loss: 0.099882
Policy train: iteration: 3500, policy_loss: 0.119580
Policy train: iteration: 4000, policy_loss: 0.103179
Policy train: iteration: 4500, policy_loss: 0.109150
Policy train: iteration: 5000, policy_loss: 0.108803
Policy train: iteration: 5500, policy_loss: 0.105198
Policy train: iteration: 6000, policy_loss: 0.129594
Policy train: iteration: 6500, policy_loss: 0.096125
Policy train: iteration: 7000, policy_loss: 0.119300
Policy train: iteration: 7500, policy_loss: 0.115063
Policy train: iteration: 8000, policy_loss: 0.094441
Policy train: iteration: 8500, policy_loss: 0.115006
Policy train: iteration: 9000, policy_loss: 0.132905

Background Trial: 1, reward: -454.92476589583237
Background Trial: 2, reward: -8.106491752920277
Background Trial: 3, reward: -7.292991836352314
Background Trial: 4, reward: 10.509917957654721
Background Trial: 5, reward: -23.77452750567089
Background Trial: 6, reward: -25.107954280983023
Background Trial: 7, reward: -510.8102933636702
Background Trial: 8, reward: -48.39512547036002
Background Trial: 9, reward: -177.5671059000082
Iteration: 90, average_reward: -138.3854820053492

Policy train: iteration: 500, policy_loss: 0.107628
Policy train: iteration: 1000, policy_loss: 0.106800
Policy train: iteration: 1500, policy_loss: 0.123678
Policy train: iteration: 2000, policy_loss: 0.113449
Policy train: iteration: 2500, policy_loss: 0.114256
Policy train: iteration: 3000, policy_loss: 0.107269
Policy train: iteration: 3500, policy_loss: 0.146766
Policy train: iteration: 4000, policy_loss: 0.112388
Policy train: iteration: 4500, policy_loss: 0.120379
Policy train: iteration: 5000, policy_loss: 0.084036
Policy train: iteration: 5500, policy_loss: 0.107150
Policy train: iteration: 6000, policy_loss: 0.112683
Policy train: iteration: 6500, policy_loss: 0.097425
Policy train: iteration: 7000, policy_loss: 0.138799
Policy train: iteration: 7500, policy_loss: 0.131449
Policy train: iteration: 8000, policy_loss: 0.125589
Policy train: iteration: 8500, policy_loss: 0.093849
Policy train: iteration: 9000, policy_loss: 0.118347

Background Trial: 1, reward: -333.4831784661271
Background Trial: 2, reward: -52.304085997703915
Background Trial: 3, reward: -67.64199066074968
Background Trial: 4, reward: -642.5511898336365
Background Trial: 5, reward: -624.4618877696961
Background Trial: 6, reward: -574.5879642864237
Background Trial: 7, reward: -60.3055350863652
Background Trial: 8, reward: -186.70802726666238
Background Trial: 9, reward: -251.18612972719583
Iteration: 91, average_reward: -310.35888767717336

Policy train: iteration: 500, policy_loss: 0.123446
Policy train: iteration: 1000, policy_loss: 0.112262
Policy train: iteration: 1500, policy_loss: 0.100452
Policy train: iteration: 2000, policy_loss: 0.097353
Policy train: iteration: 2500, policy_loss: 0.111843
Policy train: iteration: 3000, policy_loss: 0.096704
Policy train: iteration: 3500, policy_loss: 0.136637
Policy train: iteration: 4000, policy_loss: 0.094318
Policy train: iteration: 4500, policy_loss: 0.132104
Policy train: iteration: 5000, policy_loss: 0.114032
Policy train: iteration: 5500, policy_loss: 0.102451
Policy train: iteration: 6000, policy_loss: 0.116189
Policy train: iteration: 6500, policy_loss: 0.082880
Policy train: iteration: 7000, policy_loss: 0.120364
Policy train: iteration: 7500, policy_loss: 0.117265
Policy train: iteration: 8000, policy_loss: 0.112877
Policy train: iteration: 8500, policy_loss: 0.109173
Policy train: iteration: 9000, policy_loss: 0.116475

Background Trial: 1, reward: -358.9623950932819
Background Trial: 2, reward: -4.745549127583047
Background Trial: 3, reward: -645.4251833616089
Background Trial: 4, reward: -374.9464901173408
Background Trial: 5, reward: -281.06849153449264
Background Trial: 6, reward: -78.29022033682048
Background Trial: 7, reward: -698.22075244112
Background Trial: 8, reward: -67.12449478748886
Background Trial: 9, reward: -361.8492373858235
Iteration: 92, average_reward: -318.95920157617337

Policy train: iteration: 500, policy_loss: 0.156712
Policy train: iteration: 1000, policy_loss: 0.114087
Policy train: iteration: 1500, policy_loss: 0.106372
Policy train: iteration: 2000, policy_loss: 0.112201
Policy train: iteration: 2500, policy_loss: 0.130872
Policy train: iteration: 3000, policy_loss: 0.126550
Policy train: iteration: 3500, policy_loss: 0.144893
Policy train: iteration: 4000, policy_loss: 0.132435
Policy train: iteration: 4500, policy_loss: 0.123924
Policy train: iteration: 5000, policy_loss: 0.135574
Policy train: iteration: 5500, policy_loss: 0.109983
Policy train: iteration: 6000, policy_loss: 0.089901
Policy train: iteration: 6500, policy_loss: 0.118916
Policy train: iteration: 7000, policy_loss: 0.128291
Policy train: iteration: 7500, policy_loss: 0.122566
Policy train: iteration: 8000, policy_loss: 0.103792
Policy train: iteration: 8500, policy_loss: 0.139235
Policy train: iteration: 9000, policy_loss: 0.103321

Background Trial: 1, reward: -606.3427982498335
Background Trial: 2, reward: -40.024048156645314
Background Trial: 3, reward: -131.35676031105285
Background Trial: 4, reward: -592.0406697890846
Background Trial: 5, reward: -53.13062245617041
Background Trial: 6, reward: -632.4429212021129
Background Trial: 7, reward: 277.6534966430983
Background Trial: 8, reward: -21.210870067650802
Background Trial: 9, reward: -275.9198274653615
Iteration: 93, average_reward: -230.53500233942376

Policy train: iteration: 500, policy_loss: 0.125520
Policy train: iteration: 1000, policy_loss: 0.158064
Policy train: iteration: 1500, policy_loss: 0.126046
Policy train: iteration: 2000, policy_loss: 0.110839
Policy train: iteration: 2500, policy_loss: 0.101529
Policy train: iteration: 3000, policy_loss: 0.113716
Policy train: iteration: 3500, policy_loss: 0.120093
Policy train: iteration: 4000, policy_loss: 0.107727
Policy train: iteration: 4500, policy_loss: 0.109343
Policy train: iteration: 5000, policy_loss: 0.135506
Policy train: iteration: 5500, policy_loss: 0.108750
Policy train: iteration: 6000, policy_loss: 0.125601
Policy train: iteration: 6500, policy_loss: 0.092671
Policy train: iteration: 7000, policy_loss: 0.150232
Policy train: iteration: 7500, policy_loss: 0.119799
Policy train: iteration: 8000, policy_loss: 0.126954
Policy train: iteration: 8500, policy_loss: 0.133064
Policy train: iteration: 9000, policy_loss: 0.122140

Background Trial: 1, reward: -150.6017809184832
Background Trial: 2, reward: -89.73341915375981
Background Trial: 3, reward: -96.48955909918526
Background Trial: 4, reward: -349.8802744120763
Background Trial: 5, reward: -35.29396081002095
Background Trial: 6, reward: -583.8480491699088
Background Trial: 7, reward: -395.56872815703866
Background Trial: 8, reward: 302.53297616445343
Background Trial: 9, reward: 134.41989701478002
Iteration: 94, average_reward: -140.4958776156933

Policy train: iteration: 500, policy_loss: 0.107038
Policy train: iteration: 1000, policy_loss: 0.101679
Policy train: iteration: 1500, policy_loss: 0.103611
Policy train: iteration: 2000, policy_loss: 0.090969
Policy train: iteration: 2500, policy_loss: 0.126301
Policy train: iteration: 3000, policy_loss: 0.121496
Policy train: iteration: 3500, policy_loss: 0.112735
Policy train: iteration: 4000, policy_loss: 0.100275
Policy train: iteration: 4500, policy_loss: 0.091826
Policy train: iteration: 5000, policy_loss: 0.139424
Policy train: iteration: 5500, policy_loss: 0.100189
Policy train: iteration: 6000, policy_loss: 0.113451
Policy train: iteration: 6500, policy_loss: 0.131646
Policy train: iteration: 7000, policy_loss: 0.255279
Policy train: iteration: 7500, policy_loss: 0.118223
Policy train: iteration: 8000, policy_loss: 0.108816
Policy train: iteration: 8500, policy_loss: 0.115737
Policy train: iteration: 9000, policy_loss: 0.147976

Background Trial: 1, reward: -25.843346337000483
Background Trial: 2, reward: -92.52786495352296
Background Trial: 3, reward: 8.22690213293221
Background Trial: 4, reward: -602.0457151004105
Background Trial: 5, reward: -47.5816464146624
Background Trial: 6, reward: -218.64894781090393
Background Trial: 7, reward: -124.92532976404516
Background Trial: 8, reward: -32.91985796998124
Background Trial: 9, reward: -171.43591363740597
Iteration: 95, average_reward: -145.30019109500006

Policy train: iteration: 500, policy_loss: 0.113713
Policy train: iteration: 1000, policy_loss: 0.118052
Policy train: iteration: 1500, policy_loss: 0.082577
Policy train: iteration: 2000, policy_loss: 0.101030
Policy train: iteration: 2500, policy_loss: 0.151212
Policy train: iteration: 3000, policy_loss: 0.114454
Policy train: iteration: 3500, policy_loss: 0.080565
Policy train: iteration: 4000, policy_loss: 0.121863
Policy train: iteration: 4500, policy_loss: 0.120492
Policy train: iteration: 5000, policy_loss: 0.121808
Policy train: iteration: 5500, policy_loss: 0.096916
Policy train: iteration: 6000, policy_loss: 0.127393
Policy train: iteration: 6500, policy_loss: 0.142709
Policy train: iteration: 7000, policy_loss: 0.104568
Policy train: iteration: 7500, policy_loss: 0.126263
Policy train: iteration: 8000, policy_loss: 0.108501
Policy train: iteration: 8500, policy_loss: 0.100933
Policy train: iteration: 9000, policy_loss: 0.086052

Background Trial: 1, reward: -30.47337840127895
Background Trial: 2, reward: -594.5459064654473
Background Trial: 3, reward: -243.81909049887008
Background Trial: 4, reward: -666.1378841693314
Background Trial: 5, reward: -404.0283390154135
Background Trial: 6, reward: 40.641328852046996
Background Trial: 7, reward: -525.209764536223
Background Trial: 8, reward: -33.58889980136611
Background Trial: 9, reward: -421.0294609210344
Iteration: 96, average_reward: -319.79904388410193

Policy train: iteration: 500, policy_loss: 0.125388
Policy train: iteration: 1000, policy_loss: 0.104673
Policy train: iteration: 1500, policy_loss: 0.131161
Policy train: iteration: 2000, policy_loss: 0.144873
Policy train: iteration: 2500, policy_loss: 0.117857
Policy train: iteration: 3000, policy_loss: 0.111776
Policy train: iteration: 3500, policy_loss: 0.136460
Policy train: iteration: 4000, policy_loss: 0.108296
Policy train: iteration: 4500, policy_loss: 0.115594
Policy train: iteration: 5000, policy_loss: 0.119574
Policy train: iteration: 5500, policy_loss: 0.106882
Policy train: iteration: 6000, policy_loss: 0.112411
Policy train: iteration: 6500, policy_loss: 0.113940
Policy train: iteration: 7000, policy_loss: 0.125550
Policy train: iteration: 7500, policy_loss: 0.124564
Policy train: iteration: 8000, policy_loss: 0.095689
Policy train: iteration: 8500, policy_loss: 0.102359
Policy train: iteration: 9000, policy_loss: 0.174499

Background Trial: 1, reward: -778.6452813211689
Background Trial: 2, reward: -723.7204147618336
Background Trial: 3, reward: 1.1915506763497916
Background Trial: 4, reward: -192.7165984264135
Background Trial: 5, reward: -409.4222471309513
Background Trial: 6, reward: -63.774050199014
Background Trial: 7, reward: -18.80522002034816
Background Trial: 8, reward: -408.34439982401216
Background Trial: 9, reward: -40.07778197152213
Iteration: 97, average_reward: -292.7016047754349

Policy train: iteration: 500, policy_loss: 0.166412
Policy train: iteration: 1000, policy_loss: 0.113909
Policy train: iteration: 1500, policy_loss: 0.108864
Policy train: iteration: 2000, policy_loss: 0.101460
Policy train: iteration: 2500, policy_loss: 0.103021
Policy train: iteration: 3000, policy_loss: 0.137791
Policy train: iteration: 3500, policy_loss: 0.114666
Policy train: iteration: 4000, policy_loss: 0.137096
Policy train: iteration: 4500, policy_loss: 0.097369
Policy train: iteration: 5000, policy_loss: 0.098401
Policy train: iteration: 5500, policy_loss: 0.117376
Policy train: iteration: 6000, policy_loss: 0.123629
Policy train: iteration: 6500, policy_loss: 0.125057
Policy train: iteration: 7000, policy_loss: 0.126597
Policy train: iteration: 7500, policy_loss: 0.105651
Policy train: iteration: 8000, policy_loss: 0.121068
Policy train: iteration: 8500, policy_loss: 0.124979
Policy train: iteration: 9000, policy_loss: 0.110385

Background Trial: 1, reward: -99.96664016928924
Background Trial: 2, reward: -716.2816766551704
Background Trial: 3, reward: -68.29080281163635
Background Trial: 4, reward: 226.1294637584314
Background Trial: 5, reward: -86.63608881164899
Background Trial: 6, reward: -428.3828546633253
Background Trial: 7, reward: -2.4484219284420448
Background Trial: 8, reward: -102.31546763340799
Background Trial: 9, reward: 37.41871718235518
Iteration: 98, average_reward: -137.86375241468153

Policy train: iteration: 500, policy_loss: 0.112982
Policy train: iteration: 1000, policy_loss: 0.095777
Policy train: iteration: 1500, policy_loss: 0.143228
Policy train: iteration: 2000, policy_loss: 0.118079
Policy train: iteration: 2500, policy_loss: 0.096254
Policy train: iteration: 3000, policy_loss: 0.102652
Policy train: iteration: 3500, policy_loss: 0.102957
Policy train: iteration: 4000, policy_loss: 0.115189
Policy train: iteration: 4500, policy_loss: 0.121331
Policy train: iteration: 5000, policy_loss: 0.118029
Policy train: iteration: 5500, policy_loss: 0.113103
Policy train: iteration: 6000, policy_loss: 0.131657
Policy train: iteration: 6500, policy_loss: 0.094849
Policy train: iteration: 7000, policy_loss: 0.136400
Policy train: iteration: 7500, policy_loss: 0.123194
Policy train: iteration: 8000, policy_loss: 0.121401
Policy train: iteration: 8500, policy_loss: 0.111035
Policy train: iteration: 9000, policy_loss: 0.105698

Background Trial: 1, reward: -64.32008710265309
Background Trial: 2, reward: -53.068160067599756
Background Trial: 3, reward: -93.39255357728138
Background Trial: 4, reward: -37.20002245610826
Background Trial: 5, reward: -232.4106843078336
Background Trial: 6, reward: -242.58434459458033
Background Trial: 7, reward: -2.845926429664729
Background Trial: 8, reward: -717.1159419715501
Background Trial: 9, reward: -669.1249647041059
Iteration: 99, average_reward: -234.67363169015303

Policy train: iteration: 500, policy_loss: 0.134386
Policy train: iteration: 1000, policy_loss: 0.130649
Policy train: iteration: 1500, policy_loss: 0.140738
Policy train: iteration: 2000, policy_loss: 0.110889
Policy train: iteration: 2500, policy_loss: 0.103241
Policy train: iteration: 3000, policy_loss: 0.108135
Policy train: iteration: 3500, policy_loss: 0.132382
Policy train: iteration: 4000, policy_loss: 0.121644
Policy train: iteration: 4500, policy_loss: 0.119012
Policy train: iteration: 5000, policy_loss: 0.107064
Policy train: iteration: 5500, policy_loss: 0.107811
Policy train: iteration: 6000, policy_loss: 0.118840
Policy train: iteration: 6500, policy_loss: 0.129149
Policy train: iteration: 7000, policy_loss: 0.105675
Policy train: iteration: 7500, policy_loss: 0.123778
Policy train: iteration: 8000, policy_loss: 0.143946
Policy train: iteration: 8500, policy_loss: 0.109804
Policy train: iteration: 9000, policy_loss: 0.265251

Background Trial: 1, reward: -90.05512036297928
Background Trial: 2, reward: -405.69282914507056
Background Trial: 3, reward: -773.6659873266938
Background Trial: 4, reward: -543.1799057397031
Background Trial: 5, reward: -741.422113205764
Background Trial: 6, reward: -663.7637441091308
Background Trial: 7, reward: -205.19692354745877
Background Trial: 8, reward: 236.75127636555194
Background Trial: 9, reward: -238.1052223744153
Iteration: 100, average_reward: -380.4811743828516

Policy train: iteration: 500, policy_loss: 0.117498
Policy train: iteration: 1000, policy_loss: 0.097562
Policy train: iteration: 1500, policy_loss: 0.108699
Policy train: iteration: 2000, policy_loss: 0.100850
Policy train: iteration: 2500, policy_loss: 0.113396
Policy train: iteration: 3000, policy_loss: 0.088691
Policy train: iteration: 3500, policy_loss: 0.099951
Policy train: iteration: 4000, policy_loss: 0.100098
Policy train: iteration: 4500, policy_loss: 0.117420
Policy train: iteration: 5000, policy_loss: 0.198014
Policy train: iteration: 5500, policy_loss: 0.115202
Policy train: iteration: 6000, policy_loss: 0.149760
Policy train: iteration: 6500, policy_loss: 0.104989
Policy train: iteration: 7000, policy_loss: 0.119794
Policy train: iteration: 7500, policy_loss: 0.124048
Policy train: iteration: 8000, policy_loss: 0.122531
Policy train: iteration: 8500, policy_loss: 0.149187
Policy train: iteration: 9000, policy_loss: 0.142752

Background Trial: 1, reward: -627.0318791531321
Background Trial: 2, reward: -99.4706786608615
Background Trial: 3, reward: -77.83339539708652
Background Trial: 4, reward: -618.2466943661997
Background Trial: 5, reward: 282.94956656370164
Background Trial: 6, reward: -80.23083747168812
Background Trial: 7, reward: -665.7612857515951
Background Trial: 8, reward: -510.3586484621286
Background Trial: 9, reward: -200.30279431100087
Iteration: 101, average_reward: -288.4762941122212

Policy train: iteration: 500, policy_loss: 0.088967
Policy train: iteration: 1000, policy_loss: 0.115070
Policy train: iteration: 1500, policy_loss: 0.140450
Policy train: iteration: 2000, policy_loss: 0.081142
Policy train: iteration: 2500, policy_loss: 0.163252
Policy train: iteration: 3000, policy_loss: 0.104411
Policy train: iteration: 3500, policy_loss: 0.114927
Policy train: iteration: 4000, policy_loss: 0.107844
Policy train: iteration: 4500, policy_loss: 0.087829
Policy train: iteration: 5000, policy_loss: 0.113352
Policy train: iteration: 5500, policy_loss: 0.095303
Policy train: iteration: 6000, policy_loss: 0.136356
Policy train: iteration: 6500, policy_loss: 0.136010
Policy train: iteration: 7000, policy_loss: 0.099400
Policy train: iteration: 7500, policy_loss: 0.131810
Policy train: iteration: 8000, policy_loss: 0.113677
Policy train: iteration: 8500, policy_loss: 0.106175
Policy train: iteration: 9000, policy_loss: 0.124804

Background Trial: 1, reward: -658.3219688756562
Background Trial: 2, reward: -766.7048964715124
Background Trial: 3, reward: -596.7693344908466
Background Trial: 4, reward: -45.32422168279698
Background Trial: 5, reward: -151.38362484279628
Background Trial: 6, reward: -128.5265601607555
Background Trial: 7, reward: -672.1198410347108
Background Trial: 8, reward: 11.154918161531384
Background Trial: 9, reward: 12.741309606920197
Iteration: 102, average_reward: -332.8060244211804

Policy train: iteration: 500, policy_loss: 0.095207
Policy train: iteration: 1000, policy_loss: 0.108692
Policy train: iteration: 1500, policy_loss: 0.119289
Policy train: iteration: 2000, policy_loss: 0.118292
Policy train: iteration: 2500, policy_loss: 0.131037
Policy train: iteration: 3000, policy_loss: 0.081868
Policy train: iteration: 3500, policy_loss: 0.102536
Policy train: iteration: 4000, policy_loss: 0.120568
Policy train: iteration: 4500, policy_loss: 0.111596
Policy train: iteration: 5000, policy_loss: 0.103483
Policy train: iteration: 5500, policy_loss: 0.102383
Policy train: iteration: 6000, policy_loss: 0.100636
Policy train: iteration: 6500, policy_loss: 0.103077
Policy train: iteration: 7000, policy_loss: 0.109676
Policy train: iteration: 7500, policy_loss: 0.131102
Policy train: iteration: 8000, policy_loss: 0.095541
Policy train: iteration: 8500, policy_loss: 0.113021
Policy train: iteration: 9000, policy_loss: 0.152123

Background Trial: 1, reward: -32.9515350728537
Background Trial: 2, reward: -59.345881879536556
Background Trial: 3, reward: -296.12038231696795
Background Trial: 4, reward: -34.46812920865595
Background Trial: 5, reward: -122.43261226346112
Background Trial: 6, reward: -568.2462162651553
Background Trial: 7, reward: -48.58452327802982
Background Trial: 8, reward: -26.810151641239656
Background Trial: 9, reward: -132.61462438736416
Iteration: 103, average_reward: -146.84156181258493

Policy train: iteration: 500, policy_loss: 0.112946
Policy train: iteration: 1000, policy_loss: 0.139710
Policy train: iteration: 1500, policy_loss: 0.113505
Policy train: iteration: 2000, policy_loss: 0.097367
Policy train: iteration: 2500, policy_loss: 0.131459
Policy train: iteration: 3000, policy_loss: 0.116038
Policy train: iteration: 3500, policy_loss: 0.126697
Policy train: iteration: 4000, policy_loss: 0.093727
Policy train: iteration: 4500, policy_loss: 0.110183
Policy train: iteration: 5000, policy_loss: 0.109701
Policy train: iteration: 5500, policy_loss: 0.125833
Policy train: iteration: 6000, policy_loss: 0.132922
Policy train: iteration: 6500, policy_loss: 0.130244
Policy train: iteration: 7000, policy_loss: 0.146468
Policy train: iteration: 7500, policy_loss: 0.109582
Policy train: iteration: 8000, policy_loss: 0.130522
Policy train: iteration: 8500, policy_loss: 0.099335
Policy train: iteration: 9000, policy_loss: 0.147728

Background Trial: 1, reward: -122.54768527031439
Background Trial: 2, reward: -86.91438944064274
Background Trial: 3, reward: -122.19979726591363
Background Trial: 4, reward: -160.7026024144136
Background Trial: 5, reward: -299.23333404820914
Background Trial: 6, reward: -33.802209523614806
Background Trial: 7, reward: -2.9375795706798016
Background Trial: 8, reward: -28.60301058987818
Background Trial: 9, reward: -20.951788337731884
Iteration: 104, average_reward: -97.54359960682201

Policy train: iteration: 500, policy_loss: 0.102477
Policy train: iteration: 1000, policy_loss: 0.114188
Policy train: iteration: 1500, policy_loss: 0.099821
Policy train: iteration: 2000, policy_loss: 0.118641
Policy train: iteration: 2500, policy_loss: 0.114422
Policy train: iteration: 3000, policy_loss: 0.093435
Policy train: iteration: 3500, policy_loss: 0.100886
Policy train: iteration: 4000, policy_loss: 0.111219
Policy train: iteration: 4500, policy_loss: 0.092427
Policy train: iteration: 5000, policy_loss: 0.141429
Policy train: iteration: 5500, policy_loss: 0.127378
Policy train: iteration: 6000, policy_loss: 0.112676
Policy train: iteration: 6500, policy_loss: 0.112621
Policy train: iteration: 7000, policy_loss: 0.106040
Policy train: iteration: 7500, policy_loss: 0.092534
Policy train: iteration: 8000, policy_loss: 0.112524
Policy train: iteration: 8500, policy_loss: 0.123837
Policy train: iteration: 9000, policy_loss: 0.104661

Background Trial: 1, reward: -487.35806150474957
Background Trial: 2, reward: -85.54472642611516
Background Trial: 3, reward: -302.7757590455284
Background Trial: 4, reward: -293.9009440995715
Background Trial: 5, reward: -570.4421563170381
Background Trial: 6, reward: -66.67688183053772
Background Trial: 7, reward: -246.71383047744632
Background Trial: 8, reward: -26.469931889750654
Background Trial: 9, reward: -226.15992894650898
Iteration: 105, average_reward: -256.22691339302736

Policy train: iteration: 500, policy_loss: 0.136126
Policy train: iteration: 1000, policy_loss: 0.120545
Policy train: iteration: 1500, policy_loss: 0.130460
Policy train: iteration: 2000, policy_loss: 0.116928
Policy train: iteration: 2500, policy_loss: 0.104798
Policy train: iteration: 3000, policy_loss: 0.134660
Policy train: iteration: 3500, policy_loss: 0.127282
Policy train: iteration: 4000, policy_loss: 0.098140
Policy train: iteration: 4500, policy_loss: 0.114604
Policy train: iteration: 5000, policy_loss: 0.108971
Policy train: iteration: 5500, policy_loss: 0.113956
Policy train: iteration: 6000, policy_loss: 0.101402
Policy train: iteration: 6500, policy_loss: 0.119809
Policy train: iteration: 7000, policy_loss: 0.116882
Policy train: iteration: 7500, policy_loss: 0.118852
Policy train: iteration: 8000, policy_loss: 0.128769
Policy train: iteration: 8500, policy_loss: 0.126277
Policy train: iteration: 9000, policy_loss: 0.125334

Background Trial: 1, reward: -143.2631180020708
Background Trial: 2, reward: -588.5457822052492
Background Trial: 3, reward: -122.45130196467397
Background Trial: 4, reward: -11.029482580865576
Background Trial: 5, reward: -31.14199970539609
Background Trial: 6, reward: -109.96111923384473
Background Trial: 7, reward: -729.0773540414804
Background Trial: 8, reward: -186.56692991320767
Background Trial: 9, reward: -223.77660156197845
Iteration: 106, average_reward: -238.4237432454186

Policy train: iteration: 500, policy_loss: 0.093822
Policy train: iteration: 1000, policy_loss: 0.135631
Policy train: iteration: 1500, policy_loss: 0.100860
Policy train: iteration: 2000, policy_loss: 0.109040
Policy train: iteration: 2500, policy_loss: 0.112429
Policy train: iteration: 3000, policy_loss: 0.112371
Policy train: iteration: 3500, policy_loss: 0.112487
Policy train: iteration: 4000, policy_loss: 0.099530
Policy train: iteration: 4500, policy_loss: 0.124185
Policy train: iteration: 5000, policy_loss: 0.117253
Policy train: iteration: 5500, policy_loss: 0.096981
Policy train: iteration: 6000, policy_loss: 0.130427
Policy train: iteration: 6500, policy_loss: 0.141700
Policy train: iteration: 7000, policy_loss: 0.117546
Policy train: iteration: 7500, policy_loss: 0.100708
Policy train: iteration: 8000, policy_loss: 0.092058
Policy train: iteration: 8500, policy_loss: 0.094176
Policy train: iteration: 9000, policy_loss: 0.108562

Background Trial: 1, reward: 2.2436762537413557
Background Trial: 2, reward: -238.37586191412942
Background Trial: 3, reward: -806.2164696432856
Background Trial: 4, reward: -575.0975574634836
Background Trial: 5, reward: -627.8591489587998
Background Trial: 6, reward: 17.573401141308906
Background Trial: 7, reward: -101.84089461621917
Background Trial: 8, reward: -70.44546151646584
Background Trial: 9, reward: -44.532393702462485
Iteration: 107, average_reward: -271.6167456021995

Policy train: iteration: 500, policy_loss: 0.133422
Policy train: iteration: 1000, policy_loss: 0.113891
Policy train: iteration: 1500, policy_loss: 0.080273
Policy train: iteration: 2000, policy_loss: 0.126582
Policy train: iteration: 2500, policy_loss: 0.108642
Policy train: iteration: 3000, policy_loss: 0.110628
Policy train: iteration: 3500, policy_loss: 0.092288
Policy train: iteration: 4000, policy_loss: 0.124709
Policy train: iteration: 4500, policy_loss: 0.130516
Policy train: iteration: 5000, policy_loss: 0.094206
Policy train: iteration: 5500, policy_loss: 0.108582
Policy train: iteration: 6000, policy_loss: 0.093961
Policy train: iteration: 6500, policy_loss: 0.106993
Policy train: iteration: 7000, policy_loss: 0.116664
Policy train: iteration: 7500, policy_loss: 0.133294
Policy train: iteration: 8000, policy_loss: 0.095898
Policy train: iteration: 8500, policy_loss: 0.128183
Policy train: iteration: 9000, policy_loss: 0.119049

Background Trial: 1, reward: -223.5859626216781
Background Trial: 2, reward: -36.355363741887814
Background Trial: 3, reward: -75.94137806763942
Background Trial: 4, reward: -325.8908110284501
Background Trial: 5, reward: -188.2536121621543
Background Trial: 6, reward: -690.3067808068108
Background Trial: 7, reward: -623.1671117011628
Background Trial: 8, reward: -733.2651667651714
Background Trial: 9, reward: -116.93665880092968
Iteration: 108, average_reward: -334.8558717439871

Policy train: iteration: 500, policy_loss: 0.090025
Policy train: iteration: 1000, policy_loss: 0.126660
Policy train: iteration: 1500, policy_loss: 0.133729
Policy train: iteration: 2000, policy_loss: 0.137821
Policy train: iteration: 2500, policy_loss: 0.101492
Policy train: iteration: 3000, policy_loss: 0.124531
Policy train: iteration: 3500, policy_loss: 0.105222
Policy train: iteration: 4000, policy_loss: 0.142443
Policy train: iteration: 4500, policy_loss: 0.082119
Policy train: iteration: 5000, policy_loss: 0.112911
Policy train: iteration: 5500, policy_loss: 0.136159
Policy train: iteration: 6000, policy_loss: 0.124355
Policy train: iteration: 6500, policy_loss: 0.107663
Policy train: iteration: 7000, policy_loss: 0.113786
Policy train: iteration: 7500, policy_loss: 0.111913
Policy train: iteration: 8000, policy_loss: 0.129843
Policy train: iteration: 8500, policy_loss: 0.122880
Policy train: iteration: 9000, policy_loss: 0.097133

Background Trial: 1, reward: -115.11289240468699
Background Trial: 2, reward: 28.94939005502323
Background Trial: 3, reward: -600.4179241417872
Background Trial: 4, reward: -142.34176577391426
Background Trial: 5, reward: -54.388795314579006
Background Trial: 6, reward: -26.195257627748916
Background Trial: 7, reward: 8.28670995068019
Background Trial: 8, reward: -42.79313519245342
Background Trial: 9, reward: -78.22305060407007
Iteration: 109, average_reward: -113.58185789483738

Policy train: iteration: 500, policy_loss: 0.116451
Policy train: iteration: 1000, policy_loss: 0.118041
Policy train: iteration: 1500, policy_loss: 0.090943
Policy train: iteration: 2000, policy_loss: 0.110344
Policy train: iteration: 2500, policy_loss: 0.123477
Policy train: iteration: 3000, policy_loss: 0.118859
Policy train: iteration: 3500, policy_loss: 0.158761
Policy train: iteration: 4000, policy_loss: 0.113292
Policy train: iteration: 4500, policy_loss: 0.134118
Policy train: iteration: 5000, policy_loss: 0.091164
Policy train: iteration: 5500, policy_loss: 0.172374
Policy train: iteration: 6000, policy_loss: 0.108573
Policy train: iteration: 6500, policy_loss: 0.093963
Policy train: iteration: 7000, policy_loss: 0.121879
Policy train: iteration: 7500, policy_loss: 0.106502
Policy train: iteration: 8000, policy_loss: 0.126885
Policy train: iteration: 8500, policy_loss: 0.120753
Policy train: iteration: 9000, policy_loss: 0.133070

Background Trial: 1, reward: -327.76850606149776
Background Trial: 2, reward: 12.884813051028104
Background Trial: 3, reward: -690.7134880252285
Background Trial: 4, reward: -494.65641877451367
Background Trial: 5, reward: -663.9023655202018
Background Trial: 6, reward: -42.11535339331731
Background Trial: 7, reward: -80.21944947600701
Background Trial: 8, reward: -655.3918108180118
Background Trial: 9, reward: -317.87719164997964
Iteration: 110, average_reward: -362.19553007419216

Policy train: iteration: 500, policy_loss: 0.128565
Policy train: iteration: 1000, policy_loss: 0.106751
Policy train: iteration: 1500, policy_loss: 0.097750
Policy train: iteration: 2000, policy_loss: 0.117133
Policy train: iteration: 2500, policy_loss: 0.096286
Policy train: iteration: 3000, policy_loss: 0.100162
Policy train: iteration: 3500, policy_loss: 0.103981
Policy train: iteration: 4000, policy_loss: 0.096465
Policy train: iteration: 4500, policy_loss: 0.105900
Policy train: iteration: 5000, policy_loss: 0.117168
Policy train: iteration: 5500, policy_loss: 0.132830
Policy train: iteration: 6000, policy_loss: 0.147935
Policy train: iteration: 6500, policy_loss: 0.114804
Policy train: iteration: 7000, policy_loss: 0.139664
Policy train: iteration: 7500, policy_loss: 0.121908
Policy train: iteration: 8000, policy_loss: 0.125521
Policy train: iteration: 8500, policy_loss: 0.126033
Policy train: iteration: 9000, policy_loss: 0.124292

Background Trial: 1, reward: -77.01863430453851
Background Trial: 2, reward: -151.39971759422545
Background Trial: 3, reward: -1270.4500558518387
Background Trial: 4, reward: -6.589023807554938
Background Trial: 5, reward: -51.0160324218495
Background Trial: 6, reward: 18.649802723525397
Background Trial: 7, reward: -61.46183941727227
Background Trial: 8, reward: 231.22849320401525
Background Trial: 9, reward: -111.7602322898625
Iteration: 111, average_reward: -164.4241377510668

Policy train: iteration: 500, policy_loss: 0.159997
Policy train: iteration: 1000, policy_loss: 0.113137
Policy train: iteration: 1500, policy_loss: 0.103113
Policy train: iteration: 2000, policy_loss: 0.138813
Policy train: iteration: 2500, policy_loss: 0.111122
Policy train: iteration: 3000, policy_loss: 0.110718
Policy train: iteration: 3500, policy_loss: 0.111619
Policy train: iteration: 4000, policy_loss: 0.144792
Policy train: iteration: 4500, policy_loss: 0.108268
Policy train: iteration: 5000, policy_loss: 0.107562
Policy train: iteration: 5500, policy_loss: 0.094428
Policy train: iteration: 6000, policy_loss: 0.146620
Policy train: iteration: 6500, policy_loss: 0.145977
Policy train: iteration: 7000, policy_loss: 0.118689
Policy train: iteration: 7500, policy_loss: 0.095312
Policy train: iteration: 8000, policy_loss: 0.116894
Policy train: iteration: 8500, policy_loss: 0.098075
Policy train: iteration: 9000, policy_loss: 0.101658

Background Trial: 1, reward: -109.55787756324679
Background Trial: 2, reward: 14.589421988554037
Background Trial: 3, reward: -64.86576529266668
Background Trial: 4, reward: -21.813414462503445
Background Trial: 5, reward: -15.21588258236666
Background Trial: 6, reward: -245.2720950142043
Background Trial: 7, reward: -107.25819116121849
Background Trial: 8, reward: -122.52416602083693
Background Trial: 9, reward: -69.28053309989093
Iteration: 112, average_reward: -82.35538924537556

Policy train: iteration: 500, policy_loss: 0.161336
Policy train: iteration: 1000, policy_loss: 0.114761
Policy train: iteration: 1500, policy_loss: 0.174692
Policy train: iteration: 2000, policy_loss: 0.090466
Policy train: iteration: 2500, policy_loss: 0.095863
Policy train: iteration: 3000, policy_loss: 0.148236
Policy train: iteration: 3500, policy_loss: 0.087675
Policy train: iteration: 4000, policy_loss: 0.100024
Policy train: iteration: 4500, policy_loss: 0.113211
Policy train: iteration: 5000, policy_loss: 0.105999
Policy train: iteration: 5500, policy_loss: 0.101271
Policy train: iteration: 6000, policy_loss: 0.111401
Policy train: iteration: 6500, policy_loss: 0.107061
Policy train: iteration: 7000, policy_loss: 0.096989
Policy train: iteration: 7500, policy_loss: 0.116327
Policy train: iteration: 8000, policy_loss: 0.101076
Policy train: iteration: 8500, policy_loss: 0.101086
Policy train: iteration: 9000, policy_loss: 0.108769

Background Trial: 1, reward: -565.7707795502258
Background Trial: 2, reward: -671.7524423304903
Background Trial: 3, reward: -19.19348437243093
Background Trial: 4, reward: -29.583423079704005
Background Trial: 5, reward: -460.87304435314235
Background Trial: 6, reward: -242.81770932115592
Background Trial: 7, reward: -168.05854458165038
Background Trial: 8, reward: -373.1254821838263
Background Trial: 9, reward: -502.0363941700606
Iteration: 113, average_reward: -337.0234782158541

Policy train: iteration: 500, policy_loss: 0.110595
Policy train: iteration: 1000, policy_loss: 0.099932
Policy train: iteration: 1500, policy_loss: 0.115366
Policy train: iteration: 2000, policy_loss: 0.090061
Policy train: iteration: 2500, policy_loss: 0.120492
Policy train: iteration: 3000, policy_loss: 0.137063
Policy train: iteration: 3500, policy_loss: 0.102683
Policy train: iteration: 4000, policy_loss: 0.119805
Policy train: iteration: 4500, policy_loss: 0.097098
Policy train: iteration: 5000, policy_loss: 0.121491
Policy train: iteration: 5500, policy_loss: 0.092802
Policy train: iteration: 6000, policy_loss: 0.115789
Policy train: iteration: 6500, policy_loss: 0.120290
Policy train: iteration: 7000, policy_loss: 0.101602
Policy train: iteration: 7500, policy_loss: 0.097231
Policy train: iteration: 8000, policy_loss: 0.071684
Policy train: iteration: 8500, policy_loss: 0.101742
Policy train: iteration: 9000, policy_loss: 0.140312

Background Trial: 1, reward: -144.56126860297962
Background Trial: 2, reward: -150.86248009402746
Background Trial: 3, reward: -38.98956149529296
Background Trial: 4, reward: 24.435874626475268
Background Trial: 5, reward: -9.086016424086793
Background Trial: 6, reward: -124.72299944533364
Background Trial: 7, reward: -107.72635971346925
Background Trial: 8, reward: -616.8305579469977
Background Trial: 9, reward: -188.99823747045062
Iteration: 114, average_reward: -150.81573406290698

Policy train: iteration: 500, policy_loss: 0.099157
Policy train: iteration: 1000, policy_loss: 0.106213
Policy train: iteration: 1500, policy_loss: 0.124767
Policy train: iteration: 2000, policy_loss: 0.105753
Policy train: iteration: 2500, policy_loss: 0.130786
Policy train: iteration: 3000, policy_loss: 0.117511
Policy train: iteration: 3500, policy_loss: 0.109764
Policy train: iteration: 4000, policy_loss: 0.099411
Policy train: iteration: 4500, policy_loss: 0.084959
Policy train: iteration: 5000, policy_loss: 0.124667
Policy train: iteration: 5500, policy_loss: 0.110174
Policy train: iteration: 6000, policy_loss: 0.120284
Policy train: iteration: 6500, policy_loss: 0.090832
Policy train: iteration: 7000, policy_loss: 0.149959
Policy train: iteration: 7500, policy_loss: 0.116606
Policy train: iteration: 8000, policy_loss: 0.090958
Policy train: iteration: 8500, policy_loss: 0.111949
Policy train: iteration: 9000, policy_loss: 0.115255

Background Trial: 1, reward: -398.51141395104816
Background Trial: 2, reward: -644.723654790822
Background Trial: 3, reward: -627.1353476589929
Background Trial: 4, reward: -402.3686886135542
Background Trial: 5, reward: -340.90966895642816
Background Trial: 6, reward: -81.95976420463097
Background Trial: 7, reward: -42.72333882391127
Background Trial: 8, reward: 258.73996982045253
Background Trial: 9, reward: -872.6970064102155
Iteration: 115, average_reward: -350.2543237321279

Policy train: iteration: 500, policy_loss: 0.112416
Policy train: iteration: 1000, policy_loss: 0.114396
Policy train: iteration: 1500, policy_loss: 0.119191
Policy train: iteration: 2000, policy_loss: 0.116174
Policy train: iteration: 2500, policy_loss: 0.099636
Policy train: iteration: 3000, policy_loss: 0.137984
Policy train: iteration: 3500, policy_loss: 0.085459
Policy train: iteration: 4000, policy_loss: 0.111995
Policy train: iteration: 4500, policy_loss: 0.152262
Policy train: iteration: 5000, policy_loss: 0.122436
Policy train: iteration: 5500, policy_loss: 0.097376
Policy train: iteration: 6000, policy_loss: 0.085543
Policy train: iteration: 6500, policy_loss: 0.130389
Policy train: iteration: 7000, policy_loss: 0.129910
Policy train: iteration: 7500, policy_loss: 0.099192
Policy train: iteration: 8000, policy_loss: 0.091778
Policy train: iteration: 8500, policy_loss: 0.098768
Policy train: iteration: 9000, policy_loss: 0.158534

Background Trial: 1, reward: -84.72310445202487
Background Trial: 2, reward: -73.62043912934101
Background Trial: 3, reward: -14.923468837235461
Background Trial: 4, reward: -172.06418684328915
Background Trial: 5, reward: -122.23600337869559
Background Trial: 6, reward: -198.17365399812343
Background Trial: 7, reward: -370.4822479320642
Background Trial: 8, reward: -104.9403307853764
Background Trial: 9, reward: -479.06295054201144
Iteration: 116, average_reward: -180.0251539886846

Policy train: iteration: 500, policy_loss: 0.100211
Policy train: iteration: 1000, policy_loss: 0.107770
Policy train: iteration: 1500, policy_loss: 0.114814
Policy train: iteration: 2000, policy_loss: 0.110594
Policy train: iteration: 2500, policy_loss: 0.105356
Policy train: iteration: 3000, policy_loss: 0.101003
Policy train: iteration: 3500, policy_loss: 0.103016
Policy train: iteration: 4000, policy_loss: 0.128936
Policy train: iteration: 4500, policy_loss: 0.122820
Policy train: iteration: 5000, policy_loss: 0.093427
Policy train: iteration: 5500, policy_loss: 0.083660
Policy train: iteration: 6000, policy_loss: 0.113701
Policy train: iteration: 6500, policy_loss: 0.099885
Policy train: iteration: 7000, policy_loss: 0.122509
Policy train: iteration: 7500, policy_loss: 0.107346
Policy train: iteration: 8000, policy_loss: 0.104752
Policy train: iteration: 8500, policy_loss: 0.137368
Policy train: iteration: 9000, policy_loss: 0.117054

Background Trial: 1, reward: -58.919179263474284
Background Trial: 2, reward: -49.72310729690247
Background Trial: 3, reward: -117.2536768500025
Background Trial: 4, reward: 39.36703011878009
Background Trial: 5, reward: -30.46098153546869
Background Trial: 6, reward: -95.49632398468903
Background Trial: 7, reward: -346.51134593421693
Background Trial: 8, reward: -465.5467860535136
Background Trial: 9, reward: -8.809430667666987
Iteration: 117, average_reward: -125.92820016301715

Policy train: iteration: 500, policy_loss: 0.107970
Policy train: iteration: 1000, policy_loss: 0.116621
Policy train: iteration: 1500, policy_loss: 0.136668
Policy train: iteration: 2000, policy_loss: 0.101868
Policy train: iteration: 2500, policy_loss: 0.108528
Policy train: iteration: 3000, policy_loss: 0.083475
Policy train: iteration: 3500, policy_loss: 0.109814
Policy train: iteration: 4000, policy_loss: 0.083590
Policy train: iteration: 4500, policy_loss: 0.100747
Policy train: iteration: 5000, policy_loss: 0.082890
Policy train: iteration: 5500, policy_loss: 0.123354
Policy train: iteration: 6000, policy_loss: 0.116725
Policy train: iteration: 6500, policy_loss: 0.109996
Policy train: iteration: 7000, policy_loss: 0.114394
Policy train: iteration: 7500, policy_loss: 0.112042
Policy train: iteration: 8000, policy_loss: 0.120374
Policy train: iteration: 8500, policy_loss: 0.091628
Policy train: iteration: 9000, policy_loss: 0.177354

Background Trial: 1, reward: 9.186228425554631
Background Trial: 2, reward: -687.4425788299897
Background Trial: 3, reward: -59.85393035970932
Background Trial: 4, reward: -114.51459625281073
Background Trial: 5, reward: -245.3947134759118
Background Trial: 6, reward: -534.9233926371585
Background Trial: 7, reward: -3.9323422805280472
Background Trial: 8, reward: 20.13516817102645
Background Trial: 9, reward: -170.16718431442177
Iteration: 118, average_reward: -198.54526017266096

Policy train: iteration: 500, policy_loss: 0.098840
Policy train: iteration: 1000, policy_loss: 0.142638
Policy train: iteration: 1500, policy_loss: 0.115219
Policy train: iteration: 2000, policy_loss: 0.089127
Policy train: iteration: 2500, policy_loss: 0.095590
