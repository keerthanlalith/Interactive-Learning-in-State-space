Policy train: iteration: 500, policy_loss: 0.548444
Policy train: iteration: 1000, policy_loss: 0.386976
Policy train: iteration: 1500, policy_loss: 0.452309
Policy train: iteration: 2000, policy_loss: 0.286544
Policy train: iteration: 2500, policy_loss: 0.316244
Policy train: iteration: 3000, policy_loss: 0.208206
Policy train: iteration: 3500, policy_loss: 0.243477
Policy train: iteration: 4000, policy_loss: 0.220196

Background Trial: 1, reward: 15.0
Background Trial: 2, reward: 15.0
Background Trial: 3, reward: 15.0
Background Trial: 4, reward: 15.0
Background Trial: 5, reward: 18.0
Background Trial: 6, reward: 16.0
Background Trial: 7, reward: 15.0
Background Trial: 8, reward: 15.0
Background Trial: 9, reward: 16.0
Iteration: 1, average_reward: 15.555555555555555

Policy train: iteration: 500, policy_loss: 0.217342
Policy train: iteration: 1000, policy_loss: 0.355563
Policy train: iteration: 1500, policy_loss: 0.182755
Policy train: iteration: 2000, policy_loss: 0.354254
Policy train: iteration: 2500, policy_loss: 0.156108
Policy train: iteration: 3000, policy_loss: 0.129758
Policy train: iteration: 3500, policy_loss: 0.179870
Policy train: iteration: 4000, policy_loss: 0.101025

Background Trial: 1, reward: 16.0
Background Trial: 2, reward: 21.0
Background Trial: 3, reward: 15.0
Background Trial: 4, reward: 14.0
Background Trial: 5, reward: 16.0
Background Trial: 6, reward: 15.0
Background Trial: 7, reward: 20.0
Background Trial: 8, reward: 15.0
Background Trial: 9, reward: 18.0
Iteration: 2, average_reward: 16.666666666666668

Policy train: iteration: 500, policy_loss: 0.241433
Policy train: iteration: 1000, policy_loss: 0.094339
Policy train: iteration: 1500, policy_loss: 0.072772
Policy train: iteration: 2000, policy_loss: 0.160712
Policy train: iteration: 2500, policy_loss: 0.177918
Policy train: iteration: 3000, policy_loss: 0.266812
Policy train: iteration: 3500, policy_loss: 0.404619
Policy train: iteration: 4000, policy_loss: 0.204057

Background Trial: 1, reward: 15.0
Background Trial: 2, reward: 15.0
Background Trial: 3, reward: 15.0
Background Trial: 4, reward: 15.0
Background Trial: 5, reward: 15.0
Background Trial: 6, reward: 15.0
Background Trial: 7, reward: 15.0
Background Trial: 8, reward: 16.0
Background Trial: 9, reward: 15.0
Iteration: 3, average_reward: 15.11111111111111

Policy train: iteration: 500, policy_loss: 0.303478
Policy train: iteration: 1000, policy_loss: 0.313846
Policy train: iteration: 1500, policy_loss: 0.342586
Policy train: iteration: 2000, policy_loss: 0.431841
Policy train: iteration: 2500, policy_loss: 0.107795
Policy train: iteration: 3000, policy_loss: 0.246652
Policy train: iteration: 3500, policy_loss: 0.012335
Policy train: iteration: 4000, policy_loss: 0.290822

Background Trial: 1, reward: 15.0
Background Trial: 2, reward: 15.0
Background Trial: 3, reward: 15.0
Background Trial: 4, reward: 16.0
Background Trial: 5, reward: 19.0
Background Trial: 6, reward: 16.0
Background Trial: 7, reward: 18.0
Background Trial: 8, reward: 18.0
Background Trial: 9, reward: 15.0
Iteration: 4, average_reward: 16.333333333333332

Policy train: iteration: 500, policy_loss: 0.087276
Policy train: iteration: 1000, policy_loss: 0.074156
Policy train: iteration: 1500, policy_loss: 0.165993
Policy train: iteration: 2000, policy_loss: 0.119262
Policy train: iteration: 2500, policy_loss: 0.035857
Policy train: iteration: 3000, policy_loss: 0.082288
Policy train: iteration: 3500, policy_loss: 0.107718
Policy train: iteration: 4000, policy_loss: 0.162549

Background Trial: 1, reward: 16.0
Background Trial: 2, reward: 15.0
Background Trial: 3, reward: 15.0
Background Trial: 4, reward: 15.0
Background Trial: 5, reward: 15.0
Background Trial: 6, reward: 18.0
Background Trial: 7, reward: 18.0
Background Trial: 8, reward: 15.0
Background Trial: 9, reward: 20.0
Iteration: 5, average_reward: 16.333333333333332

Policy train: iteration: 500, policy_loss: 0.081623
Policy train: iteration: 1000, policy_loss: 0.097295
Policy train: iteration: 1500, policy_loss: 0.255782
Policy train: iteration: 2000, policy_loss: 0.066158
Policy train: iteration: 2500, policy_loss: 0.173152
Policy train: iteration: 3000, policy_loss: 0.128113
Policy train: iteration: 3500, policy_loss: 0.263079
Policy train: iteration: 4000, policy_loss: 0.221495

Background Trial: 1, reward: 15.0
Background Trial: 2, reward: 15.0
Background Trial: 3, reward: 15.0
Background Trial: 4, reward: 20.0
Background Trial: 5, reward: 16.0
Background Trial: 6, reward: 15.0
Background Trial: 7, reward: 15.0
Background Trial: 8, reward: 16.0
Background Trial: 9, reward: 22.0
Iteration: 6, average_reward: 16.555555555555557

Policy train: iteration: 500, policy_loss: 0.080780
Policy train: iteration: 1000, policy_loss: 0.189477
Policy train: iteration: 1500, policy_loss: 0.065564
Policy train: iteration: 2000, policy_loss: 0.252207
Policy train: iteration: 2500, policy_loss: 0.234071
Policy train: iteration: 3000, policy_loss: 0.173279
Policy train: iteration: 3500, policy_loss: 0.127419
Policy train: iteration: 4000, policy_loss: 0.109359

Background Trial: 1, reward: 18.0
Background Trial: 2, reward: 19.0
Background Trial: 3, reward: 15.0
Background Trial: 4, reward: 21.0
Background Trial: 5, reward: 20.0
Background Trial: 6, reward: 19.0
Background Trial: 7, reward: 15.0
Background Trial: 8, reward: 20.0
Background Trial: 9, reward: 15.0
Iteration: 7, average_reward: 18.0

Policy train: iteration: 500, policy_loss: 0.198588
Policy train: iteration: 1000, policy_loss: 0.114548
Policy train: iteration: 1500, policy_loss: 0.105554
