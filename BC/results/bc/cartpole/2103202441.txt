Policy train: iteration: 500, policy_loss: 0.173632
Policy train: iteration: 1000, policy_loss: 0.184495
Policy train: iteration: 1500, policy_loss: 0.122816
Policy train: iteration: 2000, policy_loss: 0.103171
Policy train: iteration: 2500, policy_loss: 0.115725
Policy train: iteration: 3000, policy_loss: 0.108052
Policy train: iteration: 3500, policy_loss: 0.126401
Policy train: iteration: 4000, policy_loss: 0.183458

Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 200.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 1, average_reward: 200.0

Policy train: iteration: 500, policy_loss: 0.212736
Policy train: iteration: 1000, policy_loss: 0.187430
Policy train: iteration: 1500, policy_loss: 0.244023
Policy train: iteration: 2000, policy_loss: 0.141625
Policy train: iteration: 2500, policy_loss: 0.068304
Policy train: iteration: 3000, policy_loss: 0.149837
Policy train: iteration: 3500, policy_loss: 0.126180
Policy train: iteration: 4000, policy_loss: 0.121607

Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 200.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 2, average_reward: 200.0

Policy train: iteration: 500, policy_loss: 0.117557
Policy train: iteration: 1000, policy_loss: 0.226133
Policy train: iteration: 1500, policy_loss: 0.152740
Policy train: iteration: 2000, policy_loss: 0.059023
Policy train: iteration: 2500, policy_loss: 0.144845
Policy train: iteration: 3000, policy_loss: 0.065574
Policy train: iteration: 3500, policy_loss: 0.140311
Policy train: iteration: 4000, policy_loss: 0.154435

Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 200.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 3, average_reward: 200.0

Policy train: iteration: 500, policy_loss: 0.181192
Policy train: iteration: 1000, policy_loss: 0.245385
Policy train: iteration: 1500, policy_loss: 0.193582
Policy train: iteration: 2000, policy_loss: 0.131831
Policy train: iteration: 2500, policy_loss: 0.245029
Policy train: iteration: 3000, policy_loss: 0.246858
Policy train: iteration: 3500, policy_loss: 0.081102
Policy train: iteration: 4000, policy_loss: 0.111122

Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 200.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 4, average_reward: 200.0

Policy train: iteration: 500, policy_loss: 0.108039
Policy train: iteration: 1000, policy_loss: 0.175095
Policy train: iteration: 1500, policy_loss: 0.130232
Policy train: iteration: 2000, policy_loss: 0.101099
Policy train: iteration: 2500, policy_loss: 0.149028
Policy train: iteration: 3000, policy_loss: 0.111714
Policy train: iteration: 3500, policy_loss: 0.184878
Policy train: iteration: 4000, policy_loss: 0.209477

Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 200.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 5, average_reward: 200.0

Policy train: iteration: 500, policy_loss: 0.174921
Policy train: iteration: 1000, policy_loss: 0.083389
Policy train: iteration: 1500, policy_loss: 0.111089
Policy train: iteration: 2000, policy_loss: 0.076434
Policy train: iteration: 2500, policy_loss: 0.140609
Policy train: iteration: 3000, policy_loss: 0.183655
Policy train: iteration: 3500, policy_loss: 0.195738
Policy train: iteration: 4000, policy_loss: 0.102267

Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 200.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 6, average_reward: 200.0

Policy train: iteration: 500, policy_loss: 0.100449
Policy train: iteration: 1000, policy_loss: 0.144241
Policy train: iteration: 1500, policy_loss: 0.202434
Policy train: iteration: 2000, policy_loss: 0.115530
Policy train: iteration: 2500, policy_loss: 0.177037
Policy train: iteration: 3000, policy_loss: 0.153231
Policy train: iteration: 3500, policy_loss: 0.054315
Policy train: iteration: 4000, policy_loss: 0.163845

Background Trial: 1, reward: 200.0
Background Trial: 2, reward: 200.0
Background Trial: 3, reward: 200.0
Background Trial: 4, reward: 200.0
Background Trial: 5, reward: 200.0
Background Trial: 6, reward: 200.0
Background Trial: 7, reward: 200.0
Background Trial: 8, reward: 200.0
Background Trial: 9, reward: 200.0
Iteration: 7, average_reward: 200.0

Policy train: iteration: 500, policy_loss: 0.238108
Policy train: iteration: 1000, policy_loss: 0.181056
